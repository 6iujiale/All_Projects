{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Update history Last updated: 2020.11.30 Update time Update contents 2020.3.24 Release the Cruzr open platform's V2.5.3 SDK and instructions. 2020.4.10 Release the Cruzr open platform's V2.7.0 SDK and instructions. 1.Add the following relevant interfaces and descriptions: leisure \uff0c multimedia \uff0c user management \uff0c visual 2. Setting \uff1aSetting robot language is added 3. voice-assistant \uff1aEach component of the voice assistant is displayed and hidden separately is added,and related interfaces is added 4.4. Add sensor list \uff0c diagnosis list 2020.11.30 Release the Cruzr open platform's V2.7.0 SDK and instructions. 1. voice-assistant \uff1aAdded voice assistant global display and hidden interface. 2. visual \uff1aAdded face detection, face tracking, offline picture recognition, online picture recognition, face capture, and opening and closing the visual service interface. 3. user management \uff1aNew user registration interface. 4. FAQ \uff1aAdd the common process of using google voice service.","title":"Update history"},{"location":"index.html#update-history","text":"Last updated: 2020.11.30 Update time Update contents 2020.3.24 Release the Cruzr open platform's V2.5.3 SDK and instructions. 2020.4.10 Release the Cruzr open platform's V2.7.0 SDK and instructions. 1.Add the following relevant interfaces and descriptions: leisure \uff0c multimedia \uff0c user management \uff0c visual 2. Setting \uff1aSetting robot language is added 3. voice-assistant \uff1aEach component of the voice assistant is displayed and hidden separately is added,and related interfaces is added 4.4. Add sensor list \uff0c diagnosis list 2020.11.30 Release the Cruzr open platform's V2.7.0 SDK and instructions. 1. voice-assistant \uff1aAdded voice assistant global display and hidden interface. 2. visual \uff1aAdded face detection, face tracking, offline picture recognition, online picture recognition, face capture, and opening and closing the visual service interface. 3. user management \uff1aNew user registration interface. 4. FAQ \uff1aAdd the common process of using google voice service.","title":"Update history"},{"location":"adbconnect.html","text":"How to connect the Cruzr? Like all Android devices, the Cruzr debugs through ADB connection. The only difference is that a specific adbkey needs to be configured. Preparations Configure adbkey Before connecting the Cruzr through the ADB, you must first configure the corresponding adbkey. There are two files: adbkey and adbkey.pub. Place these two files in the .android folder in the computer user directory. If there is no .android directory, create one. The computer user's directory is C:\\Users\\xxxx (xxxx represents the current user), e.g.: C:\\Users\\ubt. Create a .android folder C:\\Users\\ubt.android, and then place the above file in this directory, as shown in the figure below. After configuring the adbkey, the way to connect Cruzr is the same for mobile phones and tablets. 2.Configure the ADB environment Configure the ADB environment, download the adb.exe tool (adb-4.4.rar or use the adb tool in the Android SDK platform-tools directory), and place ADB in a directory such as D:\\adb\\adb-4.4. Configure the ADB environment variable. Connecting the ADB to the robot There are three ways for ADB to connect to the robot. Connect via USB cable Loosen the screw at the chin, and then rotate the face clockwise, as shown below. Use a USB cable to connect the robot and the computer as shown below. Open the cmd window and use ADB devices to check. If it looks like the picture below, it has been successful. Otherwise, please restart the computer and the device, and try again. 2.Connect via HDMI-to-USB cable Prepare a HDMI-to-USB cable Remove the back cover next to the \u201cemergency stop\u201d button on the robot. Connect the hdmi port with the word \"TEST\" on it with the hide end, and then connect the robot to the usbm end, as shown in the figure below. Open the cmd window and use adb devices to check. If it looks like the picture below, it has been successful. Otherwise, please restart the computer and the device, and try again. 3.Connect via LAN Make sure that the Cruzr Android system's WiFi network is connected to the computer network, and check the Cruzr Android system's WiFi network IP. How to view your IP Using 10.10.27.42 as an example, execute adb connect 10.10.27.42 in cmd, and then check through the ADB devices. If it looks like the example below, it has been successful.","title":"Preparatory work\uff08environment\uff09"},{"location":"adbconnect.html#how-to-connect-the-cruzr","text":"Like all Android devices, the Cruzr debugs through ADB connection. The only difference is that a specific adbkey needs to be configured.","title":"How to connect the Cruzr?"},{"location":"adbconnect.html#preparations","text":"Configure adbkey Before connecting the Cruzr through the ADB, you must first configure the corresponding adbkey. There are two files: adbkey and adbkey.pub. Place these two files in the .android folder in the computer user directory. If there is no .android directory, create one. The computer user's directory is C:\\Users\\xxxx (xxxx represents the current user), e.g.: C:\\Users\\ubt. Create a .android folder C:\\Users\\ubt.android, and then place the above file in this directory, as shown in the figure below. After configuring the adbkey, the way to connect Cruzr is the same for mobile phones and tablets. 2.Configure the ADB environment Configure the ADB environment, download the adb.exe tool (adb-4.4.rar or use the adb tool in the Android SDK platform-tools directory), and place ADB in a directory such as D:\\adb\\adb-4.4. Configure the ADB environment variable.","title":"Preparations"},{"location":"adbconnect.html#connecting-the-adb-to-the-robot","text":"There are three ways for ADB to connect to the robot. Connect via USB cable Loosen the screw at the chin, and then rotate the face clockwise, as shown below. Use a USB cable to connect the robot and the computer as shown below. Open the cmd window and use ADB devices to check. If it looks like the picture below, it has been successful. Otherwise, please restart the computer and the device, and try again. 2.Connect via HDMI-to-USB cable Prepare a HDMI-to-USB cable Remove the back cover next to the \u201cemergency stop\u201d button on the robot. Connect the hdmi port with the word \"TEST\" on it with the hide end, and then connect the robot to the usbm end, as shown in the figure below. Open the cmd window and use adb devices to check. If it looks like the picture below, it has been successful. Otherwise, please restart the computer and the device, and try again. 3.Connect via LAN Make sure that the Cruzr Android system's WiFi network is connected to the computer network, and check the Cruzr Android system's WiFi network IP. How to view your IP Using 10.10.27.42 as an example, execute adb connect 10.10.27.42 in cmd, and then check through the ADB devices. If it looks like the example below, it has been successful.","title":"Connecting the ADB to the robot"},{"location":"compatibility.html","text":"The SDK version number released this time is V2.8.0,and the corresponding robot version number is 3.607.3.605 . Use the method of \"reference system SDK\" to develop. Please refer to the build your first app chapter. SDK version number Robot version number Whether it is compatible 2.8.0 3.607.3.605 compatible 2.7.0 3.607.3.605 compatible 2.7.0 3.502.3.403 compatible 2.7.0 3.403.3.402 compatible 2.7.0 3.304.3.303 compatible 2.7.0 3.200.3.201 compatible 2.7.0 3.101.3.106 compatible 2.7.0 3.10.3.104 compatible 2.5.3.5 2.386.2.66 compatible 2.5.2 3.10.3.104 compatible 2.5.2 2.386.2.66 compatible 2.5.2 2.361.2.48 compatible","title":"Compatibility"},{"location":"cruzrframe.html","text":"Cruzr's technical framework Introduction As a developer, when you try to write your first program on Cruzr, you may have many questions such as \u201cWhat is the structure of the Cruzr system?\u201d, \u201cHow do you create a simple project?\u201d, \"What is the difference between a Cruzr robot and other android devices such as mobile phones, tablets, and TV boxes?\", and \"How do the modules of the robot work?\", and so on. The following is a brief introduction to the Cruzr's technical framework, which will not only answer the questions above, but also let you understand the internal implementation principles of Cruzr, so everyone can quickly access their own programs. Cruzr framework Cruzr system framework As can be seen from the figure above, Cruzr is mainly made up of four layers. From the top to the bottom are the application layer, business service layer, control layer, and hardware layer. The application layer provides the business functions of the robot. The business service layer is Cruzr's core service and provides various capabilities for the application layer. For example, the speech service provides speech recognition,natural language understanding , and text to speech capabilities. The control layer mainly controls the hardware modules of the robot. Cruzr's internal implementation principle By analyzing the Cruzr's framework, we can see that Cruzr's software structure is the application layer, service layer, and control layer from top to bottom. The application layer receives the events entered by the user and then calls the functions provided by the service layer through the API interface. The service layer sends the corresponding instruction to the control layer through the socket. The control layer controls the corresponding hardware module according to the instructions. At the same time, the data generated by the sensor is uploaded to the service layer through the control layer, and then returned to the application layer through the API interface. The application layer then sends the feedback to the user through interface display or voice. What are the differences? The difference between Cruzr and other android devices is that it has a more powerful hardware foundation and requires more service layers to manage it. However, developers only need to pay attention to our business service layer and the provided API interface. Therefore, customization and development based on Cruzr are the same as other android platforms.","title":"Technical framework"},{"location":"cruzrframe.html#cruzrs-technical-framework","text":"","title":"Cruzr's technical framework"},{"location":"cruzrframe.html#introduction","text":"As a developer, when you try to write your first program on Cruzr, you may have many questions such as \u201cWhat is the structure of the Cruzr system?\u201d, \u201cHow do you create a simple project?\u201d, \"What is the difference between a Cruzr robot and other android devices such as mobile phones, tablets, and TV boxes?\", and \"How do the modules of the robot work?\", and so on. The following is a brief introduction to the Cruzr's technical framework, which will not only answer the questions above, but also let you understand the internal implementation principles of Cruzr, so everyone can quickly access their own programs.","title":"Introduction"},{"location":"cruzrframe.html#cruzr-framework","text":"Cruzr system framework As can be seen from the figure above, Cruzr is mainly made up of four layers. From the top to the bottom are the application layer, business service layer, control layer, and hardware layer. The application layer provides the business functions of the robot. The business service layer is Cruzr's core service and provides various capabilities for the application layer. For example, the speech service provides speech recognition,natural language understanding , and text to speech capabilities. The control layer mainly controls the hardware modules of the robot.","title":"Cruzr framework"},{"location":"cruzrframe.html#cruzrs-internal-implementation-principle","text":"By analyzing the Cruzr's framework, we can see that Cruzr's software structure is the application layer, service layer, and control layer from top to bottom. The application layer receives the events entered by the user and then calls the functions provided by the service layer through the API interface. The service layer sends the corresponding instruction to the control layer through the socket. The control layer controls the corresponding hardware module according to the instructions. At the same time, the data generated by the sensor is uploaded to the service layer through the control layer, and then returned to the application layer through the API interface. The application layer then sends the feedback to the user through interface display or voice.","title":"Cruzr's internal implementation principle"},{"location":"cruzrframe.html#what-are-the-differences","text":"The difference between Cruzr and other android devices is that it has a more powerful hardware foundation and requires more service layers to manage it. However, developers only need to pay attention to our business service layer and the provided API interface. Therefore, customization and development based on Cruzr are the same as other android platforms.","title":"What are the differences?"},{"location":"faq.html","text":"FAQ 1. How do add online voice commands? Please contact the technical staff at UBTECH. 2. Interface to detect objects approaching In a real scenario, if you need to know whether an object is near or far away from the robot, you can pass the human_detect parameter to obtain it in the sensor monitoring, as follows: public class SensorActivity extends AppCompatActivity { private static final String TAG = SensorActivity . class . getSimpleName (); private SensorManager sensorManager ; private SensorListener sensorListener ; private boolean mHumanIn = false ; public static final String SENSOR_HUMAN_DETECT = \"human_detect\" ; //Human detection sensor ID public static final int HUMAN_CLOSER = 1 ; // There are objects approaching public static final int HUMAN_AWAY = 2 ; //There are objects leaving public static final float HUMAN_DETECT_DISTANT_THRESHOLD = 1.3f ; // There is people entering the distance threshold @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_sensor ); GlobalContext robotContext = Robot . globalContext (); sensorManager = robotContext . getSystemService ( SensorManager . SERVICE ); } public boolean isHumanIn () { return mHumanIn ; } @Override protected void onPause () { super . onPause (); sensorManager . unregisterListener ( sensorListener ); } @Override protected void onResume () { super . onResume (); sensorListener = new SensorListener () { @Override public void onSensorChanged ( SensorDevice sensorDevice , SensorEvent sensorEvent ) { //check the sensor change event here try { int direction = Math . round ( sensorEvent . getValues () [ 0 ] ); switch ( direction ) { case HUMAN_AWAY : 3.Demo: Replace speech service to Google service To satisfy the requirements of replacing speech services, we provide Demo code to make the replacing process easy. Please refer to the file: overriding-speech-service. This Demo code is made for replacing Google Cloud Speech services. Please get the access token json file of speech service(ASR, NLP,TTS) from https://cloud.google.com, and replace it in the corresponding line in the Demo code.","title":"FAQ"},{"location":"faq.html#faq","text":"","title":"FAQ"},{"location":"faq.html#1-how-do-add-online-voice-commands","text":"Please contact the technical staff at UBTECH.","title":"1. How do add online voice commands?"},{"location":"faq.html#2-interface-to-detect-objects-approaching","text":"In a real scenario, if you need to know whether an object is near or far away from the robot, you can pass the human_detect parameter to obtain it in the sensor monitoring, as follows: public class SensorActivity extends AppCompatActivity { private static final String TAG = SensorActivity . class . getSimpleName (); private SensorManager sensorManager ; private SensorListener sensorListener ; private boolean mHumanIn = false ; public static final String SENSOR_HUMAN_DETECT = \"human_detect\" ; //Human detection sensor ID public static final int HUMAN_CLOSER = 1 ; // There are objects approaching public static final int HUMAN_AWAY = 2 ; //There are objects leaving public static final float HUMAN_DETECT_DISTANT_THRESHOLD = 1.3f ; // There is people entering the distance threshold @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_sensor ); GlobalContext robotContext = Robot . globalContext (); sensorManager = robotContext . getSystemService ( SensorManager . SERVICE ); } public boolean isHumanIn () { return mHumanIn ; } @Override protected void onPause () { super . onPause (); sensorManager . unregisterListener ( sensorListener ); } @Override protected void onResume () { super . onResume (); sensorListener = new SensorListener () { @Override public void onSensorChanged ( SensorDevice sensorDevice , SensorEvent sensorEvent ) { //check the sensor change event here try { int direction = Math . round ( sensorEvent . getValues () [ 0 ] ); switch ( direction ) { case HUMAN_AWAY :","title":"2. Interface to detect objects approaching"},{"location":"faq.html#3demo-replace-speech-service-to-google-service","text":"To satisfy the requirements of replacing speech services, we provide Demo code to make the replacing process easy. Please refer to the file: overriding-speech-service. This Demo code is made for replacing Google Cloud Speech services. Please get the access token json file of speech service(ASR, NLP,TTS) from https://cloud.google.com, and replace it in the corresponding line in the Demo code.","title":"3.Demo: Replace speech service to Google service"},{"location":"openplatform.html","text":"Cruzr open platform Platform introduction The Cruzr open platform is based on the Cruzr robot, which provides an open system, application-level software environment and service resources for third-party partners. It helps third-party partners to efficiently develop applications and effectively solve problems encountered during development and operation. Platform capabilities The capabilities of the Cruzr open platform include the Cruzr's own SDK, cloud API, and auxiliary tools.","title":"Cruzr open platform"},{"location":"openplatform.html#cruzr-open-platform","text":"","title":"Cruzr open platform"},{"location":"openplatform.html#platform-introduction","text":"The Cruzr open platform is based on the Cruzr robot, which provides an open system, application-level software environment and service resources for third-party partners. It helps third-party partners to efficiently develop applications and effectively solve problems encountered during development and operation.","title":"Platform introduction"},{"location":"openplatform.html#platform-capabilities","text":"The capabilities of the Cruzr open platform include the Cruzr's own SDK, cloud API, and auxiliary tools.","title":"Platform capabilities"},{"location":"productintroduction.html","text":"Cruzr product introduction The Cruzr cloud platform commercial service robot is a humanoid robot independently developed by UBTECH. It is flexible and has speech and visual capabilities, as well as stereo navigation and obstacle avoidance capabilities. Cruzr can serve in a series of scenarios such as transportation, government service halls, shopping malls, banks, luxury hotels, exhibition halls, 4S stores, hospitals, and commercial real estate. It can effectively streamline work processes, reduce labor costs, unearth business value, reduce operating costs, and help businesses and service halls realize intelligent change. Top functions Top functions Hardware structure Hardware structure Product parameters Project Product parameters Product size 1210(H)630(W)537(D)mm Product net weight About 45kg Screen 11.5 inch TFT HD screen Resolution 1920x1080 pixels Camera 13MP HD camera Battery Battery type: lithium ion battery; Capacity: 25Ah; Voltage: 25.6V The maximum height it can hurdle 1cm The maximum angle of slope it can climb 6 degrees Moving speed 0.2 / 0.5 / 0.7 m/s (max speed 1m/s) Communication Wi-Fi (2.4G / 5G), supports external 4G Speaker 85dB (60cm ahead) Operating temperature 5-25 degrees Celsius Operating environment Indoor System platform Android (application layer) + ROS (control layer) Sensor Head: 6+0 microphone array * 1 Waist: Depth Camera * 1 Arm: Electronic skin 2 * 2 (two arms, 2 pieces per arm) Chassis: Ultrasonic sensor * 6, infrared sensor * 9, 9-axis gyroscope sensor * 1, lidar * 1, temperature and humidity sensor * 1","title":"Cruzr product introduction"},{"location":"productintroduction.html#cruzr-product-introduction","text":"The Cruzr cloud platform commercial service robot is a humanoid robot independently developed by UBTECH. It is flexible and has speech and visual capabilities, as well as stereo navigation and obstacle avoidance capabilities. Cruzr can serve in a series of scenarios such as transportation, government service halls, shopping malls, banks, luxury hotels, exhibition halls, 4S stores, hospitals, and commercial real estate. It can effectively streamline work processes, reduce labor costs, unearth business value, reduce operating costs, and help businesses and service halls realize intelligent change.","title":"Cruzr product introduction"},{"location":"productintroduction.html#top-functions","text":"Top functions","title":"Top functions"},{"location":"productintroduction.html#hardware-structure","text":"Hardware structure","title":"Hardware structure"},{"location":"productintroduction.html#product-parameters","text":"Project Product parameters Product size 1210(H)630(W)537(D)mm Product net weight About 45kg Screen 11.5 inch TFT HD screen Resolution 1920x1080 pixels Camera 13MP HD camera Battery Battery type: lithium ion battery; Capacity: 25Ah; Voltage: 25.6V The maximum height it can hurdle 1cm The maximum angle of slope it can climb 6 degrees Moving speed 0.2 / 0.5 / 0.7 m/s (max speed 1m/s) Communication Wi-Fi (2.4G / 5G), supports external 4G Speaker 85dB (60cm ahead) Operating temperature 5-25 degrees Celsius Operating environment Indoor System platform Android (application layer) + ROS (control layer) Sensor Head: 6+0 microphone array * 1 Waist: Depth Camera * 1 Arm: Electronic skin 2 * 2 (two arms, 2 pieces per arm) Chassis: Ultrasonic sensor * 6, infrared sensor * 9, 9-axis gyroscope sensor * 1, lidar * 1, temperature and humidity sensor * 1","title":"Product parameters"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html","text":"Cruzr External Devices Extended Services The Cruzr extended services provide a unified interfaces for calling Cruzr external devices,which can do operations such as opening and closing the external device\uff0cor listening device reported data . If the specified device is not accessed, this interface will not work. This version provides the interface of infrared temperature measuring device as well. Please refer to API documents . How to use it Import GRADLE implementation 'com.ubtrobot.cruzr.framework:extension:(insert version)' Initialization This extension package depends on Cruzr service ,Therefore, the Cruzr service needs to be initialized first . Robot . initialize ( getApplicationContext ()); // Cruzr Service initialization ExtensionManager extensionManager = new ExtensionManager ( Robot . globalContext ()); Open or close device components You can call the enableComponent() and disableComponent() methods of ExtensionManager to control the opening or closing of components. The method parameter is the key of the corresponding component. Please refer to components supported by the current version . Example\uff1a Promise < Void , ExtensionException > promise = extensionManager . enableComponent ( key ); promise . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback of device component opening success } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { // Callback of device component opening fail } }); Promise < Void , ExtensionException > promise = extensionManager . disableComponent ( key ); promise . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback of device component closing success } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { //Callback of device component closing fail } }); Get component status You can call the getComponentstatus() method of ExtensionManager to get the component status .The method parameter is the key of the corresponding component. (version 1.0.1 only supports body temperature detection) . Promise < Integer , ExtensionException > promise = extensionManager . getComponentStatus ( key ); promise . done ( new DoneCallback < Integer > () { @Override public void onDone ( Integer integer ) { // Callback for getting part status success // Integer is the current state value of the component. (the status value will be defined according to the characteristics of the component. The status value will be different for different components. ) } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { // Callback for getting part status fail } }); Register or unregister listener The registered listening interface is used to obtain the information reported by components. ComponentListener listener = new ComponentListener () { @Override public void onComponentUpdate ( ComponentInfo componentInfo ) { Log . d ( TAG , componentInfo . toString ()); } }; extensionManager . registerComponentListener ( listener ); extensionManager . unregisterComponentListener ( listener ); Registration and unregistration must be in pairs to avoid memory leakage. You can get reported component title through componentInfo.getTitle() method.(Please refer to Title Definition You can get reported component data through componentInfo.getExtension() method.(Please refer to Title Definition . introduction to Componentinfo Class Components for the current version Key Item Support Version MASK Mask detection 1.0.1 TEMPERATURE Temperature detection 1.0.1 SPRINKLER disinfection 1.0.2 Refering to the introduction of Componentkey Class Notifications for the current version Title Explanation Supported Version NOTIFY_THERMOMETRY_INFO Report the temperature in the crowd 1.0.1 NOTIFY_THERMOMETRY_ALARM Report the abnormal temperature in the crowd 1.0.1 NOTIFY_MASK_INFO Report the wearing of masks in the crowd 1.0.1 Refering to the introduction of NotifyTitle Class Report Data The current version (1.0.1) supports the following title, which submitted data in JSON format. NOTIFY_THERMOMETRY_INFO Report Data { \"temp_status\" : [ // It's an array , which may be multiple temperatures { \"temp\" : float , // Current temperature \"faceid\" : int , // Face id\uff0cwhich can detect human traffic roughly // The highest temperature of the face position coordinate area \"x\" : int , //x, y: Coordinate point in the upper left corner of the picture \"y\" : int , \"width\" : int , //width, height: Width and height of the Picture \"height\" : int , \"name\" : String // The name of the picture }, ... ] } NOTIFY_THERMOMETRY_ALARM Report Data { \"temp_status\" : [ // It's an array. There may be multiple temperatures. { \"temp\" : float , // Current temperature \"faceid\" : int , // Face id \"name\" : String // The name of the picture. }, ... ] } NOTIFY_MASK_INFO Report Data { \"mask_status\" : [ // It's an array. There may be multiple mask status. { \"mask\" : int , // Status of wearing mask 0--Wearing1,1--Not wearing,2--Not standard \"faceid\" : int , // Face id\uff0cwhich can detect human traffic roughly // Coordinate area of face position without mask \"x\" : int , //x, y: Coordinate point in the upper left corner of the picture \"y\" : int , \"width\" : int , //width, height:Width and height of the Picture \"height\" : int , \"name\" : String //The name of the picture }, ... ] }","title":"Cruzr External Devices Extended Services"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#cruzr-external-devices-extended-services","text":"The Cruzr extended services provide a unified interfaces for calling Cruzr external devices,which can do operations such as opening and closing the external device\uff0cor listening device reported data . If the specified device is not accessed, this interface will not work. This version provides the interface of infrared temperature measuring device as well. Please refer to API documents .","title":"Cruzr External Devices Extended Services"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#how-to-use-it","text":"","title":"How to use it"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#import","text":"GRADLE implementation 'com.ubtrobot.cruzr.framework:extension:(insert version)'","title":"Import"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#initialization","text":"This extension package depends on Cruzr service ,Therefore, the Cruzr service needs to be initialized first . Robot . initialize ( getApplicationContext ()); // Cruzr Service initialization ExtensionManager extensionManager = new ExtensionManager ( Robot . globalContext ());","title":"Initialization"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#open-or-close-device-components","text":"You can call the enableComponent() and disableComponent() methods of ExtensionManager to control the opening or closing of components. The method parameter is the key of the corresponding component. Please refer to components supported by the current version . Example\uff1a Promise < Void , ExtensionException > promise = extensionManager . enableComponent ( key ); promise . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback of device component opening success } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { // Callback of device component opening fail } }); Promise < Void , ExtensionException > promise = extensionManager . disableComponent ( key ); promise . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback of device component closing success } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { //Callback of device component closing fail } });","title":"Open or close device components"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#get-component-status","text":"You can call the getComponentstatus() method of ExtensionManager to get the component status .The method parameter is the key of the corresponding component. (version 1.0.1 only supports body temperature detection) . Promise < Integer , ExtensionException > promise = extensionManager . getComponentStatus ( key ); promise . done ( new DoneCallback < Integer > () { @Override public void onDone ( Integer integer ) { // Callback for getting part status success // Integer is the current state value of the component. (the status value will be defined according to the characteristics of the component. The status value will be different for different components. ) } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { // Callback for getting part status fail } });","title":"Get component status"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#register-or-unregister-listener","text":"The registered listening interface is used to obtain the information reported by components. ComponentListener listener = new ComponentListener () { @Override public void onComponentUpdate ( ComponentInfo componentInfo ) { Log . d ( TAG , componentInfo . toString ()); } }; extensionManager . registerComponentListener ( listener ); extensionManager . unregisterComponentListener ( listener ); Registration and unregistration must be in pairs to avoid memory leakage. You can get reported component title through componentInfo.getTitle() method.(Please refer to Title Definition You can get reported component data through componentInfo.getExtension() method.(Please refer to Title Definition . introduction to Componentinfo Class","title":"Register or unregister listener"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#components-for-the-current-version","text":"Key Item Support Version MASK Mask detection 1.0.1 TEMPERATURE Temperature detection 1.0.1 SPRINKLER disinfection 1.0.2 Refering to the introduction of Componentkey Class","title":"Components for the current version"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#notifications-for-the-current-version","text":"Title Explanation Supported Version NOTIFY_THERMOMETRY_INFO Report the temperature in the crowd 1.0.1 NOTIFY_THERMOMETRY_ALARM Report the abnormal temperature in the crowd 1.0.1 NOTIFY_MASK_INFO Report the wearing of masks in the crowd 1.0.1 Refering to the introduction of NotifyTitle Class","title":"Notifications for the current version"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#report-data","text":"The current version (1.0.1) supports the following title, which submitted data in JSON format.","title":"Report Data"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#notify_thermometry_info-report-data","text":"{ \"temp_status\" : [ // It's an array , which may be multiple temperatures { \"temp\" : float , // Current temperature \"faceid\" : int , // Face id\uff0cwhich can detect human traffic roughly // The highest temperature of the face position coordinate area \"x\" : int , //x, y: Coordinate point in the upper left corner of the picture \"y\" : int , \"width\" : int , //width, height: Width and height of the Picture \"height\" : int , \"name\" : String // The name of the picture }, ... ] }","title":"NOTIFY_THERMOMETRY_INFO  Report Data"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#notify_thermometry_alarm-report-data","text":"{ \"temp_status\" : [ // It's an array. There may be multiple temperatures. { \"temp\" : float , // Current temperature \"faceid\" : int , // Face id \"name\" : String // The name of the picture. }, ... ] }","title":"NOTIFY_THERMOMETRY_ALARM  Report Data"},{"location":"Cruzr extended services/Cruzr extended services - \u526f\u672c.html#notify_mask_info-report-data","text":"{ \"mask_status\" : [ // It's an array. There may be multiple mask status. { \"mask\" : int , // Status of wearing mask 0--Wearing1,1--Not wearing,2--Not standard \"faceid\" : int , // Face id\uff0cwhich can detect human traffic roughly // Coordinate area of face position without mask \"x\" : int , //x, y: Coordinate point in the upper left corner of the picture \"y\" : int , \"width\" : int , //width, height:Width and height of the Picture \"height\" : int , \"name\" : String //The name of the picture }, ... ] }","title":"NOTIFY_MASK_INFO Report Data"},{"location":"Cruzr extended services/Cruzr extended services.html","text":"Cruzr External Devices Extended Services The Cruzr extended services provide a unified interfaces for calling Cruzr external devices,which can do operations such as opening and closing the external device\uff0cor listening device reported data . If the specified device is not accessed, this interface will not work. This version provides the interface of infrared temperature measuring device as well. Please refer to API documents . How to use it Import GRADLE Download the latest version of jar and put it in the project libs directory Initialization This extension package depends on Cruzr service ,Therefore, the Cruzr service needs to be initialized first . Robot . initialize ( getApplicationContext ()); // Cruzr Service initialization ExtensionManager extensionManager = new ExtensionManager ( Robot . globalContext ()); Open or close device components You can call the enableComponent() and disableComponent() methods of ExtensionManager to control the opening or closing of components. The method parameter is the key of the corresponding component. Please refer to components supported by the current version . Example\uff1a Promise < Void , ExtensionException > promise = extensionManager . enableComponent ( key ); promise . done ( new DoneCallback < Integer > () { @Override public void onDone ( Integer integer ) { // Callback of device component opening success // Key of component, The integer returned has a specific meaning (Refer to components for the current version) } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { // Callback of device component opening fail } }); Promise < Void , ExtensionException > promise = extensionManager . disableComponent ( key ); promise . done ( new DoneCallback < Integer > () { @Override public void onDone ( Integer integer ) { // Callback of device component closing success } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { // Callback of device component closing fail } }); Register or unregister listener The registered listening interface is used to obtain the information reported by components. ComponentListener listener = new ComponentListener () { @Override public void onComponentUpdate ( ComponentInfo componentInfo ) { Log . d ( TAG , componentInfo . toString ()); } }; extensionManager . registerComponentListener ( listener ); extensionManager . unregisterComponentListener ( listener ); Registration and unregistration must be in pairs to avoid memory leakage. You can get reported component title through componentInfo.getTitle() method.(Please refer to Title Definition You can get reported component data through componentInfo.getExtension() method.(Please refer to Title Definition . introduction to Componentinfo Class Components for the current version key Item The meaning of integer when open success Support Version TEMPERATURE Temperature detection 0: China Above 1.0.1 1: Other countries other: no MASK Mask detection no Above 1.0.1 SPRINKLER disinfection no Above 1.0.2 Refering to the introduction of Componentkey Class Notifications for the current version Title Explanation Supported Version NOTIFY_THERMOMETRY_INFO Report the temperature in the crowd Above 1.0.1 NOTIFY_THERMOMETRY_ALARM Report the abnormal temperature in the crowd Above 1.0.1 NOTIFY_MASK_INFO Report the wearing of masks in the crowd Above 1.0.1 Refering to the introduction of NotifyTitle Class Report Data The current version (1.0.1) supports the following title, which submitted data in JSON format. NOTIFY_THERMOMETRY_INFO Report Data { \"temp_status\" : [ // It's an array , which may be multiple temperatures { \"temp\" : float , // Current temperature \"faceid\" : int , // Face id\uff0cwhich can detect human traffic roughly // The highest temperature of the face position coordinate area \"x\" : int , //x, y: Coordinate point in the upper left corner of the picture \"y\" : int , \"width\" : int , //width, height: Width and height of the Picture \"height\" : int , \"name\" : String // The name of the picture }, ... ] } NOTIFY_THERMOMETRY_ALARM Report Data { \"temp_status\" : [ // It's an array. There may be multiple temperatures. { \"temp\" : float , // Current temperature \"faceid\" : int , // Face id \"name\" : String // The name of the picture. }, ... ] } NOTIFY_MASK_INFO Report Data { \"mask_status\" : [ // It's an array. There may be multiple mask status. { \"mask\" : int , // Status of wearing mask 0--Wearing1,1--Not wearing,2--Not standard \"faceid\" : int , // Face id\uff0cwhich can detect human traffic roughly // Coordinate area of face position without mask \"x\" : int , //x, y: Coordinate point in the upper left corner of the picture \"y\" : int , \"width\" : int , //width, height:Width and height of the Picture \"height\" : int , \"name\" : String //The name of the picture }, ... ] }","title":"Cruzr External Devices Extended Services"},{"location":"Cruzr extended services/Cruzr extended services.html#cruzr-external-devices-extended-services","text":"The Cruzr extended services provide a unified interfaces for calling Cruzr external devices,which can do operations such as opening and closing the external device\uff0cor listening device reported data . If the specified device is not accessed, this interface will not work. This version provides the interface of infrared temperature measuring device as well. Please refer to API documents .","title":"Cruzr External Devices Extended Services"},{"location":"Cruzr extended services/Cruzr extended services.html#how-to-use-it","text":"","title":"How to use it"},{"location":"Cruzr extended services/Cruzr extended services.html#import","text":"GRADLE Download the latest version of jar and put it in the project libs directory","title":"Import"},{"location":"Cruzr extended services/Cruzr extended services.html#initialization","text":"This extension package depends on Cruzr service ,Therefore, the Cruzr service needs to be initialized first . Robot . initialize ( getApplicationContext ()); // Cruzr Service initialization ExtensionManager extensionManager = new ExtensionManager ( Robot . globalContext ());","title":"Initialization"},{"location":"Cruzr extended services/Cruzr extended services.html#open-or-close-device-components","text":"You can call the enableComponent() and disableComponent() methods of ExtensionManager to control the opening or closing of components. The method parameter is the key of the corresponding component. Please refer to components supported by the current version . Example\uff1a Promise < Void , ExtensionException > promise = extensionManager . enableComponent ( key ); promise . done ( new DoneCallback < Integer > () { @Override public void onDone ( Integer integer ) { // Callback of device component opening success // Key of component, The integer returned has a specific meaning (Refer to components for the current version) } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { // Callback of device component opening fail } }); Promise < Void , ExtensionException > promise = extensionManager . disableComponent ( key ); promise . done ( new DoneCallback < Integer > () { @Override public void onDone ( Integer integer ) { // Callback of device component closing success } }). fail ( new FailCallback < ExtensionException > () { @Override public void onFail ( ExtensionException e ) { // Callback of device component closing fail } });","title":"Open or close device components"},{"location":"Cruzr extended services/Cruzr extended services.html#register-or-unregister-listener","text":"The registered listening interface is used to obtain the information reported by components. ComponentListener listener = new ComponentListener () { @Override public void onComponentUpdate ( ComponentInfo componentInfo ) { Log . d ( TAG , componentInfo . toString ()); } }; extensionManager . registerComponentListener ( listener ); extensionManager . unregisterComponentListener ( listener ); Registration and unregistration must be in pairs to avoid memory leakage. You can get reported component title through componentInfo.getTitle() method.(Please refer to Title Definition You can get reported component data through componentInfo.getExtension() method.(Please refer to Title Definition . introduction to Componentinfo Class","title":"Register or unregister listener"},{"location":"Cruzr extended services/Cruzr extended services.html#components-for-the-current-version","text":"key Item The meaning of integer when open success Support Version TEMPERATURE Temperature detection 0: China Above 1.0.1 1: Other countries other: no MASK Mask detection no Above 1.0.1 SPRINKLER disinfection no Above 1.0.2 Refering to the introduction of Componentkey Class","title":"Components for the current version"},{"location":"Cruzr extended services/Cruzr extended services.html#notifications-for-the-current-version","text":"Title Explanation Supported Version NOTIFY_THERMOMETRY_INFO Report the temperature in the crowd Above 1.0.1 NOTIFY_THERMOMETRY_ALARM Report the abnormal temperature in the crowd Above 1.0.1 NOTIFY_MASK_INFO Report the wearing of masks in the crowd Above 1.0.1 Refering to the introduction of NotifyTitle Class","title":"Notifications for the current version"},{"location":"Cruzr extended services/Cruzr extended services.html#report-data","text":"The current version (1.0.1) supports the following title, which submitted data in JSON format.","title":"Report Data"},{"location":"Cruzr extended services/Cruzr extended services.html#notify_thermometry_info-report-data","text":"{ \"temp_status\" : [ // It's an array , which may be multiple temperatures { \"temp\" : float , // Current temperature \"faceid\" : int , // Face id\uff0cwhich can detect human traffic roughly // The highest temperature of the face position coordinate area \"x\" : int , //x, y: Coordinate point in the upper left corner of the picture \"y\" : int , \"width\" : int , //width, height: Width and height of the Picture \"height\" : int , \"name\" : String // The name of the picture }, ... ] }","title":"NOTIFY_THERMOMETRY_INFO  Report Data"},{"location":"Cruzr extended services/Cruzr extended services.html#notify_thermometry_alarm-report-data","text":"{ \"temp_status\" : [ // It's an array. There may be multiple temperatures. { \"temp\" : float , // Current temperature \"faceid\" : int , // Face id \"name\" : String // The name of the picture. }, ... ] }","title":"NOTIFY_THERMOMETRY_ALARM  Report Data"},{"location":"Cruzr extended services/Cruzr extended services.html#notify_mask_info-report-data","text":"{ \"mask_status\" : [ // It's an array. There may be multiple mask status. { \"mask\" : int , // Status of wearing mask 0--Wearing1,1--Not wearing,2--Not standard \"faceid\" : int , // Face id\uff0cwhich can detect human traffic roughly // Coordinate area of face position without mask \"x\" : int , //x, y: Coordinate point in the upper left corner of the picture \"y\" : int , \"width\" : int , //width, height:Width and height of the Picture \"height\" : int , \"name\" : String //The name of the picture }, ... ] }","title":"NOTIFY_MASK_INFO Report Data"},{"location":"android/guide/robot-context.html","text":"Context RobotContext provides an interface about Cruzr system's global information, and its implementation is provided by the Cruzr system. RobotContext` allows accessing to the service resources of the Cruzr system, while also supporting application-level operations, such as publishing subscriptions, sending instructions and so on. It describes the environment information of an application, and the Cruzr system provides a concrete implementation class for this interface. Cruzr has three types of context: RobotSkill describes the context of the skills. RobotSkill.this returns the context of the current skill. Its action scope follows the life cycle of the skill. RobotService describes the context of the services. RobotService.this returns the context of the current service. Its action scope follows the service life cycle of the service. GlobalContext describes the whole context, Robot.globalContext () returns the whole context, and its action scope follows the life cycle of the process. Obtain system services /** * Get the robot system service by service name. * like SpeechManager, SkillManager, MotionManager and so on. * * @param serviceName the name of manager * @return the Manager which match the service name */ < T > T getSystemService ( String serviceName ); SpeechManager manager = robotContext . getSystemService ( SpeechManager . SERVICE ); Note: The corresponding system service can be obtained through the SERVICE constant Subscription events /** * subscribe the action publish by MasterService * * @param receiver the event receiver * @param parameterClass the class of parameter * @param topic the topic of event */ < T > void subscribe ( EventReceiver < T > receiver , Class < T > parameterClass , String topic ); /** * cancel the subscribe * * @param receiver the event receiver */ void unsubscribe ( EventReceiver <?> receiver ); private class BatteryEventReceiver implements EventReceiver < BatteryProperties > { @Override public void onReceive ( RobotContext robotContext , String action , final BatteryProperties batteryProperties ) { // get BatteryProperties } } mReceiver = new BatteryEventReceiver (); robotContext . subscribe ( mReceiver , BatteryProperties . class , \"event.action.power.BATTERY_CHANGE\" ); robotContext . unsubscribe ( mReceiver ); Note: subscribe can receive published events through EventReceiver <T> , where parameterclass is the parameter type of the event, and Cruzr will get the final parameters according to automatically deserialize the parameter type of the published event. Please refer to serialization and deserialization . Distribution of instructions /** * Dispatch a inProcess directive to skill with a param. * * @param skillAction the action of directive * @param paramObj the param to dispatch to the directive * @return the promise of the result */ Promise < Void , DispatchException > dispatchDirective ( String skillAction , Object paramObj ); /** * Dispatch a inProcess directive to skill with no param. * * @param skillAction he action of directive * @return the promise of the result */ Promise < Void , DispatchException > dispatchDirective ( String skillAction ); RobotContext can be used to implement inter-process instruction distribution. For inter-process instruction distribution, please refer to Skill Management . In-process nonparametric instruction distribution robotContext . dispatchDirective ( \"music.PLAY\" ) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // If the distribution is successful, it will run here } }) . fail ( new FailCallback < DispatchException > () { @Override public void onFail ( DispatchException e ) { // Here will be executed if the distribution failed } }); In-process parameter instruction distribution java Music music = new Music(); robotContext.dispatchDirective(\"music.PLAY\", music); > Note: In-process parameter distribution doesn't need to consider serialization","title":"Context"},{"location":"android/guide/robot-context.html#context","text":"RobotContext provides an interface about Cruzr system's global information, and its implementation is provided by the Cruzr system. RobotContext` allows accessing to the service resources of the Cruzr system, while also supporting application-level operations, such as publishing subscriptions, sending instructions and so on. It describes the environment information of an application, and the Cruzr system provides a concrete implementation class for this interface. Cruzr has three types of context: RobotSkill describes the context of the skills. RobotSkill.this returns the context of the current skill. Its action scope follows the life cycle of the skill. RobotService describes the context of the services. RobotService.this returns the context of the current service. Its action scope follows the service life cycle of the service. GlobalContext describes the whole context, Robot.globalContext () returns the whole context, and its action scope follows the life cycle of the process.","title":"Context"},{"location":"android/guide/robot-context.html#obtain-system-services","text":"/** * Get the robot system service by service name. * like SpeechManager, SkillManager, MotionManager and so on. * * @param serviceName the name of manager * @return the Manager which match the service name */ < T > T getSystemService ( String serviceName ); SpeechManager manager = robotContext . getSystemService ( SpeechManager . SERVICE ); Note: The corresponding system service can be obtained through the SERVICE constant","title":"Obtain system services"},{"location":"android/guide/robot-context.html#subscription-events","text":"/** * subscribe the action publish by MasterService * * @param receiver the event receiver * @param parameterClass the class of parameter * @param topic the topic of event */ < T > void subscribe ( EventReceiver < T > receiver , Class < T > parameterClass , String topic ); /** * cancel the subscribe * * @param receiver the event receiver */ void unsubscribe ( EventReceiver <?> receiver ); private class BatteryEventReceiver implements EventReceiver < BatteryProperties > { @Override public void onReceive ( RobotContext robotContext , String action , final BatteryProperties batteryProperties ) { // get BatteryProperties } } mReceiver = new BatteryEventReceiver (); robotContext . subscribe ( mReceiver , BatteryProperties . class , \"event.action.power.BATTERY_CHANGE\" ); robotContext . unsubscribe ( mReceiver ); Note: subscribe can receive published events through EventReceiver <T> , where parameterclass is the parameter type of the event, and Cruzr will get the final parameters according to automatically deserialize the parameter type of the published event. Please refer to serialization and deserialization .","title":"Subscription events"},{"location":"android/guide/robot-context.html#distribution-of-instructions","text":"/** * Dispatch a inProcess directive to skill with a param. * * @param skillAction the action of directive * @param paramObj the param to dispatch to the directive * @return the promise of the result */ Promise < Void , DispatchException > dispatchDirective ( String skillAction , Object paramObj ); /** * Dispatch a inProcess directive to skill with no param. * * @param skillAction he action of directive * @return the promise of the result */ Promise < Void , DispatchException > dispatchDirective ( String skillAction ); RobotContext can be used to implement inter-process instruction distribution. For inter-process instruction distribution, please refer to Skill Management . In-process nonparametric instruction distribution robotContext . dispatchDirective ( \"music.PLAY\" ) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // If the distribution is successful, it will run here } }) . fail ( new FailCallback < DispatchException > () { @Override public void onFail ( DispatchException e ) { // Here will be executed if the distribution failed } }); In-process parameter instruction distribution java Music music = new Music(); robotContext.dispatchDirective(\"music.PLAY\", music); > Note: In-process parameter distribution doesn't need to consider serialization","title":"Distribution of instructions"},{"location":"android/guide/skill.html","text":"Skill Skill is an application component of Cruzr, which is used to let the robot receive user instructions and perform actions. It is also used to accept the system's control on actions. A skill contains a set of related instructions, which represents a set of robot actions. The robot accepts the related instructions and executes the related actions accordingly. As an integral control unit of Cruzr, the skill also receives control notifications from the system such as start, pause, resume, and stop. Upon receiving control notifications, the skill controls the start, pause, resume, and stop actions of the robot accordingly. The action performed by one skill may conflict with the action performed by other skills or the state of the system. The role of the control notification is to assist the system in handling these action conflicts. After the Cruzr system detects action conflicts, it makes decisions and issues control notifications to the skills involved in the action conflicts. The skills will control their actions, so that the conflicts can finally be resolved. Skills include common skill and background skill . The difference between these two is that the background skills don't have pause or recover state, and can stay in the background. Cruzr has a set of built-in skill interruption logic, which stipulates the rules of interrupting for the skills in the system. The rules of interrupting include: 1.Refuse to enter new skills. For example, when the robot is currently in an emergency stop, navigation, voice and other skills cannot be entered. 2.The newly entered skills co-exist with the current skills. For example, when recharging, touching the electronic skin will not interrupt the process. 3.The newly entered skills interrupt the current skills. For example, when the robot is playing music and its battery is low, the robot will enter the recharging skill and interrupt the music skill. 4.Undefined skills, if they are allowed to start, will not change other skills. Create and configure skills A complete skill must contain the corresponding Java class and configuration file. The Java class is used to receive related instructions and perform related behaviors. The configuration file is used to declare the existence of the skill to the system and inform the system of the set of instructions received by the skill. Create skills Create a Java class to inherit RobotSkill (common skill) or RobotBackgroundSkill (skills running in the background). The following code indicates the creation of a Java class named FooSkill . In actual development, the class name should reflect the general behavior of the skill. Common skill package robot.example ; import com.ubtrobot.skill.RobotSkill ; public class FooSkill extends RobotSkill { } background skill ~~~java package robot.example; import com.ubtrobot.skill.RobotBackgroundSkill; public class FooSkill extends RobotBackgroundSkill { } ~~~ Configure the skill In the app/src/main/res/xml directory, create a robot_manifest.xml file and add an xml file declaration and a <manifest> root tag. In the <manifest> tag, add the corresponding skill configuration for the FooSkill.java class created in the previous step. Each skill component needs to be configured with a <skill> tag. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"robot.example.FooSkill\" name= \"foo\" > </skill> </manifest> The tags and their characteristics involved in the configuration above are the most necessary configurations required by the skills. The complete configuration is as follows: <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"[string]\" name= \"[string]\" background = \"boolean\" description= \"[string_resource]\" > <directive action= \"[string]\" context= \"[string]\" corpus= \"[corpus_baz1]\" triggerPause= \"boolean\" triggerStop= \"boolean\" /> </skill> </manifest> The corpus voice instructions in the configuration above support multiple languages, as follows: <!-- values/arrays.xml --> <resources> <string-array name= \"corpus_baz1\" > <item> Hello </item> <item> Hi </item> </string-array> </resources> <!-- values-zh-rCN/arrays.xml --> <resources> <string-array name= \"corpus_baz1\" > <item> Hello </item> <item> Hi </item> </string-array> </resources> The description of the configuration file is as follows Tag Characteristics Descriptions Necessity skill class The skill Java class needs to include a standardized name Yes skill name The skill name is the unique identifier of the skill in an application, which can briefly describe the role of the skill Yes skill background By default false No skill description A detailed description of the skill to introduce its functions No skill.directive action The unique identifier of the instruction. No duplicates allowed. Generally, the format is skillName/intent. Yes skill.directive context The instruction context required to receive this instruction. It is used in normal skills only. No skill.directive corpus The voice instruction corpus supports multiple languages. No duplicates allowed. It is only used in voice instructions. No skill.directive triggerPause The current instruction will automatically trigger the skill to enter the pause state, which is only used in common skills No skill.directive triggerStop The current instruction will automatically trigger the skill to enter the stop state No Note: for the background skill, context and triggerPause is invalid. In addition, triggerPause and triggerStop cannot coexist. instructions An instruction can be understood as a kind of order. The role of an instruction is to control skills. By giving specific instructions, you can control the corresponding skills to perform certain actions. Objects of instructions All the attributes of Directive include: Constants Descriptions Directive.SOURCE_IN_PROCESS The instruction source comes from in-process calling Directive.SOURCE_INTER_PROCESS The instruction source comes from inter-process calling Directive.SOURCE_INTER_PROCESS_SPEECH The instruction source comes from inter-process voice calling Directive.SOURCE_INTER_PROCESS_TIMER The instruction source comes from inter-process timer calling Directive.SOURCE_INTER_PROCESS_REMOTE_DEVICE The instruction source comes from inter-process remote device calling Attribute getter Descriptions Directive.action The unique identifier of the instruction is generally understood as the intent of the instruction. The intent distributes the key information of the instruction. Directive.source The source of the instruction indicates where the instruction was distributed from. See Directive.SOURCE_ * constants Directive.sourceExtra Additional description of the source of the instruction, which is used to add user-defined parameters. Directive.paramObj Parameter object attached to the instruction Directive.contentType The type of parameter attached to the instruction during the serialization process, by default: ContentTypes.PARCELABLE , see Serialization Directive.paramBytes Serialize byte array of the parameter attached to the instruction Directive.corpus The corpus of the voice instructions means the voice text can trigger the instruction. The voice distributes the key information of the instruction. Configure instructions After the skills are created and configured, the Cruzr system can already recognize the newly created skills. Next, you need to give declaration instructions to the skills. Add all supported instructions inside the <skill> tag in robot_manifest.xml . The following code is declared with FooSkill , which is assumed to support two instructions. The following only displays the most basic configuration. Please refer to the configuration file for the complete configuration. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"robot.example.FooSkill\" name= \"foo\" > <directive action= \"foo.BAZ1\" /> <directive action= \"foo.BAZ2\" /> </skill> </manifest> Receive instructions The action instruction declared in the robot_manifest.xml of the skill must correspond one-to-one with the method modified by the OnDirective annotation in the Java class in order to receive the instruction normally. Otherwise, RuntimeException will be thrown out when the application fails to pass the checkout, and cause a crash when it is started. The source of the action instruction can be specified by a third-party server or defined by the user, as long as it can identify the unique instruction. The action of the instruction can be a string of any style, but the recommended style is \"noun[.noun].VERB\", such as music.PLAY. Instruction without parameters Adding receiving methods of the instructions in the skill's Java classes is a public method decorated with OnDirective annotations. The following code is described in FooSkill as stated above. public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive () { } @OnDirective ( action = \"foo.BAZ2\" ) public void onBar2Directive () { } } Instruction with parameters For an action instruction, it either doesn't carry any parameter or it carries parameters with the same data type and structure. In the case of carrying parameters, the parameter object can be obtained in two ways. Here is a case where the parameter object is Bar. Method one: Obtain the parameter object directly from the parameters of the instruction receiving method public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , Bar bar ) { // bar is the parameter object. } } Method two: Obtain the parameter object from the instruction object public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive ) { Bar bar = directive . getParamObj ( Bar . class ); //[1] // bar is the parameter object. if ( bar == null ) { // The directive was not dispatched correctly. } } } For instructions that come from inter-process, the instruction parameters will undergo serialization and deserialization in the process of distribution and reception. In the above two methods for obtaining parameters, the instruction parameters are automatically deserialized (converted from a byte array into a Java object). By default, Cruzr only supports ContentTypes.PARCELABLE . In the two methods above, the default instruction parameters are used for automatic deserialization. If the built-in instruction parameter is not used for automatic deserialization, the byte array of the instruction parameter can be used for manual deserialization. public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive ) { byte [] parameterBytes = directive . getParamBytes (); // Use the bytes to deserialize to the Bar object. } } Note: The first parameter of the instruction method with parameters must be Directive A disadvantage of directly configuring the instruction parameters in the parameter list of the receiving function is that all the parameters of the instruction must be Bar, otherwise, the instruction parameter serialization exception will be returned. Voice instructions The instructions that the voice system service recognized and understood from the user's words to the robot are called voice instructions. The parameter objects of the voice instructions are unified as the UnderstandingResult class. public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , UnderstandingResult result ) { // Use the result object getting the info of speech directive } } The distribution of common instructions is achieved by comparing the unique identification action . For voice instructions, it can be achieved by comparing to the corpus (words spoken to the robot). When configuring the robot_manifest.xml file for the skill, add corpus for the corresponding instructions, as shown below. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"robot.example.FooSkill\" name= \"foo\" > <directive action= \"foo.BAZ1\" corpus= \"@array/corpus_baz1\" /> </skill> </manifest> Processing instructions Context of instructions Some instructions are only open for receiving when in a certain context or when they meet a certain condition. Take playing music as an example. After receiving the \"play\" instruction, the music starts to play, and then the \"pause\" instruction is opened for receiving. When the music is not playing, the pause command is invalid. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"robot.example.FooSkill\" name= \"foo\" > <directive action= \"foo.BAZ1\" /> <directive action= \"foo.BAZ2\" context= \"foo_context\" /> </skill> </manifest> The following code illustrates how to enter and exit the context: public class FooSkill extends RobotSkill { private static final String CTX_FOO = \"foo_context\" ; @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive () { enterContext ( CTX_FOO ); //enter \"foo_context\" context // Perform some behavior asynchronously performSomeBehavior ( new Callback () { @Override public void onDone () { // After finishing performing the behavior, exit context exitContext ( CTX_FOO ); } }); } @OnDirective ( action = \"foo.BAZ2\" ) public void onBar2Directive () { // Be here // only after enterContext(CTX_FOO) and before exitContext(CTX_FOO); } } Note: The context variable value of the instruction used in the code must be consistent with that in the configuration file to be valid. Reply instructions After all the instructions are processed, the instruction sender must be replied to. By default, the Cruzr system will automatically reply the processing result of the instruction (success or failure) to the caller. If you need to reply with the results autonomously, you can add a parameter Replier of the instruction receiving function. Reply success or failure public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , Replier replier ) { boolean success = performSomeBehavior (); if ( success ) { replier . replySuccess (); } else { replier . replyFailure ( errorCode , \"error message\" ); } } } Reply data result public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , Replier replier ) { Result result = performSomeBehavior (); replier . replySuccess ( result ); } } Reply viscosity data results public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , Replier replier ) { StickilyResult stickilyResult = performStickilyBehavior (); replier . replyStickily ( stickilyResult ); Result result = performResultBehavior (); replier . replySuccess ( result ); } } Note: Replier must be the last in the parameter list of the instruction receiving function Distribute instructions Instructions can be distributed to skills through [ RobotContext[ or [ SkillManager[ . RobotContext is used to distribute instructions within a process. SkillManager is often used to distribute inter-process instructions. But it is not recommended to distribute the instructions with the process. Since the Java class of the skill is inherited from RobotContext , the skill class itself can distribute instructions to other skills within the application. public class Example { public void dispatchDirective ( RobotContext robotContext ) { // Dispatch a directive without a parameter object. Promise < Void , DispatchException > promise = robotContext . dispatchDirective ( action ); // Dispatch a directive with a parameter object. promise = robotContext . dispatchDirective ( action , arbitraryObject ); } \uff5d Note: For detailed usage of instruction distribution, please refer to Context and Skill Management Skill status Skills include four states: start, pause, resume, and stop. When the state changes, there will be corresponding notifications. There are two reasons for the change of skill state, namely, system control and manual control. The relationship between the states is shown in the following figure: Status notification When the status of the skill changes, the system will send corresponding notifications, which selectively override the corresponding status and constrain the action of the skill. For example, the skill of music playback is in the running state while playing music. When receiving the system notification of onPause , the playback should be paused. public class FooSkill extends RobotSkill { @Override protected void onStart () { // Ready to receive directives } @Override protected void onResume ( boolean directiveWillFollow ) { // Resume the paused behavior } @Override protected void onPause ( SkillPauseCause cause ) { // Pause the performing behaviors } @Override protected void onStop ( SkillStopCause cause ) { // Stop the performing behaviors } } The onResume (boolean directiveWillFollow) parameter indicates whether an instruction will be received immediately after the skill is restored, which means the restoration of the current skill is triggered by a new instruction. The parameters onPause (SkillPauseCause cause) and onStop (SkillStopCause cause) describe the reason for the status change. The specific meanings is as follows: Name Descriptions BY_SYSTEM_DECISION The current result is triggered by system decision. BY_SKILL_MANAGEMENT The current result is triggered by the skill management class. BY_PAUSING_SELF The pause status is actively triggered. BY_STOPPING_SELF The stop status is actively triggered. BY_INTERNAL_ERROR The stop status is trigged by an internal error. Name Descriptions cause For the reasons for status trigger, please refer to the above constants condition The cause of the trigger is only valid when cause == BY_SYSTEM_DECISION. Control status After receiving the instruction, the skill begins to perform the action. When the action is completed, stopSelf () should be called to actively stop the current skill. If you don't actively stop the skill, the skills that didn't execute any actions make the other skills which conflict with it unable to execute any instruction. Here are ways to stop the skill internally and externally, respectively. For example: The navigation skill starts navigating when a navigation instruction is received. When the navigation task ends, it should actively call stopSelf () to stop the skill. Otherwise, the navigation skill is still in the running state, which will cause other skills to fail to be executed. Here are ways to stop the skill internally and externally, respectively. Stop the skill internally public class FooSkill extends RobotSkill { private void onBehaviorEnd () { stopSelf (); } } Stop the skill externally public class Example { public void stopSkill () { Robot . globalContext (). stopSkill ( skillName ); } } In addition, in some cases, it is necessary to actively pause the skill. For example, when the user tells the robot to pause the music, the skill of playing music should be paused. Here are ways to pause the skill internally and externally, respectively. - Pause the skill internally public class FooSkill extends RobotSkill { private void onPauseNeeded () { pauseSelf (); } } Pause the skill externally public class Example { public void pauseSkill () { Robot . globalContext (). pauseSkill ( skillName ); } } Note: because background skill don\u2018t have pause or recover state, there is no situations to pause backgroud skills","title":"Skill"},{"location":"android/guide/skill.html#skill","text":"Skill is an application component of Cruzr, which is used to let the robot receive user instructions and perform actions. It is also used to accept the system's control on actions. A skill contains a set of related instructions, which represents a set of robot actions. The robot accepts the related instructions and executes the related actions accordingly. As an integral control unit of Cruzr, the skill also receives control notifications from the system such as start, pause, resume, and stop. Upon receiving control notifications, the skill controls the start, pause, resume, and stop actions of the robot accordingly. The action performed by one skill may conflict with the action performed by other skills or the state of the system. The role of the control notification is to assist the system in handling these action conflicts. After the Cruzr system detects action conflicts, it makes decisions and issues control notifications to the skills involved in the action conflicts. The skills will control their actions, so that the conflicts can finally be resolved. Skills include common skill and background skill . The difference between these two is that the background skills don't have pause or recover state, and can stay in the background. Cruzr has a set of built-in skill interruption logic, which stipulates the rules of interrupting for the skills in the system. The rules of interrupting include: 1.Refuse to enter new skills. For example, when the robot is currently in an emergency stop, navigation, voice and other skills cannot be entered. 2.The newly entered skills co-exist with the current skills. For example, when recharging, touching the electronic skin will not interrupt the process. 3.The newly entered skills interrupt the current skills. For example, when the robot is playing music and its battery is low, the robot will enter the recharging skill and interrupt the music skill. 4.Undefined skills, if they are allowed to start, will not change other skills.","title":"Skill"},{"location":"android/guide/skill.html#create-and-configure-skills","text":"A complete skill must contain the corresponding Java class and configuration file. The Java class is used to receive related instructions and perform related behaviors. The configuration file is used to declare the existence of the skill to the system and inform the system of the set of instructions received by the skill.","title":"Create and configure skills"},{"location":"android/guide/skill.html#create-skills","text":"Create a Java class to inherit RobotSkill (common skill) or RobotBackgroundSkill (skills running in the background). The following code indicates the creation of a Java class named FooSkill . In actual development, the class name should reflect the general behavior of the skill. Common skill package robot.example ; import com.ubtrobot.skill.RobotSkill ; public class FooSkill extends RobotSkill { } background skill ~~~java package robot.example; import com.ubtrobot.skill.RobotBackgroundSkill; public class FooSkill extends RobotBackgroundSkill { } ~~~","title":"Create skills"},{"location":"android/guide/skill.html#configure-the-skill","text":"In the app/src/main/res/xml directory, create a robot_manifest.xml file and add an xml file declaration and a <manifest> root tag. In the <manifest> tag, add the corresponding skill configuration for the FooSkill.java class created in the previous step. Each skill component needs to be configured with a <skill> tag. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"robot.example.FooSkill\" name= \"foo\" > </skill> </manifest> The tags and their characteristics involved in the configuration above are the most necessary configurations required by the skills. The complete configuration is as follows: <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"[string]\" name= \"[string]\" background = \"boolean\" description= \"[string_resource]\" > <directive action= \"[string]\" context= \"[string]\" corpus= \"[corpus_baz1]\" triggerPause= \"boolean\" triggerStop= \"boolean\" /> </skill> </manifest> The corpus voice instructions in the configuration above support multiple languages, as follows: <!-- values/arrays.xml --> <resources> <string-array name= \"corpus_baz1\" > <item> Hello </item> <item> Hi </item> </string-array> </resources> <!-- values-zh-rCN/arrays.xml --> <resources> <string-array name= \"corpus_baz1\" > <item> Hello </item> <item> Hi </item> </string-array> </resources> The description of the configuration file is as follows Tag Characteristics Descriptions Necessity skill class The skill Java class needs to include a standardized name Yes skill name The skill name is the unique identifier of the skill in an application, which can briefly describe the role of the skill Yes skill background By default false No skill description A detailed description of the skill to introduce its functions No skill.directive action The unique identifier of the instruction. No duplicates allowed. Generally, the format is skillName/intent. Yes skill.directive context The instruction context required to receive this instruction. It is used in normal skills only. No skill.directive corpus The voice instruction corpus supports multiple languages. No duplicates allowed. It is only used in voice instructions. No skill.directive triggerPause The current instruction will automatically trigger the skill to enter the pause state, which is only used in common skills No skill.directive triggerStop The current instruction will automatically trigger the skill to enter the stop state No Note: for the background skill, context and triggerPause is invalid. In addition, triggerPause and triggerStop cannot coexist.","title":"Configure the skill"},{"location":"android/guide/skill.html#instructions","text":"An instruction can be understood as a kind of order. The role of an instruction is to control skills. By giving specific instructions, you can control the corresponding skills to perform certain actions.","title":"instructions"},{"location":"android/guide/skill.html#objects-of-instructions","text":"All the attributes of Directive include: Constants Descriptions Directive.SOURCE_IN_PROCESS The instruction source comes from in-process calling Directive.SOURCE_INTER_PROCESS The instruction source comes from inter-process calling Directive.SOURCE_INTER_PROCESS_SPEECH The instruction source comes from inter-process voice calling Directive.SOURCE_INTER_PROCESS_TIMER The instruction source comes from inter-process timer calling Directive.SOURCE_INTER_PROCESS_REMOTE_DEVICE The instruction source comes from inter-process remote device calling Attribute getter Descriptions Directive.action The unique identifier of the instruction is generally understood as the intent of the instruction. The intent distributes the key information of the instruction. Directive.source The source of the instruction indicates where the instruction was distributed from. See Directive.SOURCE_ * constants Directive.sourceExtra Additional description of the source of the instruction, which is used to add user-defined parameters. Directive.paramObj Parameter object attached to the instruction Directive.contentType The type of parameter attached to the instruction during the serialization process, by default: ContentTypes.PARCELABLE , see Serialization Directive.paramBytes Serialize byte array of the parameter attached to the instruction Directive.corpus The corpus of the voice instructions means the voice text can trigger the instruction. The voice distributes the key information of the instruction.","title":"Objects of instructions"},{"location":"android/guide/skill.html#configure-instructions","text":"After the skills are created and configured, the Cruzr system can already recognize the newly created skills. Next, you need to give declaration instructions to the skills. Add all supported instructions inside the <skill> tag in robot_manifest.xml . The following code is declared with FooSkill , which is assumed to support two instructions. The following only displays the most basic configuration. Please refer to the configuration file for the complete configuration. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"robot.example.FooSkill\" name= \"foo\" > <directive action= \"foo.BAZ1\" /> <directive action= \"foo.BAZ2\" /> </skill> </manifest>","title":"Configure instructions"},{"location":"android/guide/skill.html#receive-instructions","text":"The action instruction declared in the robot_manifest.xml of the skill must correspond one-to-one with the method modified by the OnDirective annotation in the Java class in order to receive the instruction normally. Otherwise, RuntimeException will be thrown out when the application fails to pass the checkout, and cause a crash when it is started. The source of the action instruction can be specified by a third-party server or defined by the user, as long as it can identify the unique instruction. The action of the instruction can be a string of any style, but the recommended style is \"noun[.noun].VERB\", such as music.PLAY.","title":"Receive instructions"},{"location":"android/guide/skill.html#instruction-without-parameters","text":"Adding receiving methods of the instructions in the skill's Java classes is a public method decorated with OnDirective annotations. The following code is described in FooSkill as stated above. public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive () { } @OnDirective ( action = \"foo.BAZ2\" ) public void onBar2Directive () { } }","title":"Instruction without parameters"},{"location":"android/guide/skill.html#instruction-with-parameters","text":"For an action instruction, it either doesn't carry any parameter or it carries parameters with the same data type and structure. In the case of carrying parameters, the parameter object can be obtained in two ways. Here is a case where the parameter object is Bar. Method one: Obtain the parameter object directly from the parameters of the instruction receiving method public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , Bar bar ) { // bar is the parameter object. } } Method two: Obtain the parameter object from the instruction object public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive ) { Bar bar = directive . getParamObj ( Bar . class ); //[1] // bar is the parameter object. if ( bar == null ) { // The directive was not dispatched correctly. } } } For instructions that come from inter-process, the instruction parameters will undergo serialization and deserialization in the process of distribution and reception. In the above two methods for obtaining parameters, the instruction parameters are automatically deserialized (converted from a byte array into a Java object). By default, Cruzr only supports ContentTypes.PARCELABLE . In the two methods above, the default instruction parameters are used for automatic deserialization. If the built-in instruction parameter is not used for automatic deserialization, the byte array of the instruction parameter can be used for manual deserialization. public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive ) { byte [] parameterBytes = directive . getParamBytes (); // Use the bytes to deserialize to the Bar object. } } Note: The first parameter of the instruction method with parameters must be Directive A disadvantage of directly configuring the instruction parameters in the parameter list of the receiving function is that all the parameters of the instruction must be Bar, otherwise, the instruction parameter serialization exception will be returned.","title":"Instruction with parameters"},{"location":"android/guide/skill.html#voice-instructions","text":"The instructions that the voice system service recognized and understood from the user's words to the robot are called voice instructions. The parameter objects of the voice instructions are unified as the UnderstandingResult class. public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , UnderstandingResult result ) { // Use the result object getting the info of speech directive } } The distribution of common instructions is achieved by comparing the unique identification action . For voice instructions, it can be achieved by comparing to the corpus (words spoken to the robot). When configuring the robot_manifest.xml file for the skill, add corpus for the corresponding instructions, as shown below. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"robot.example.FooSkill\" name= \"foo\" > <directive action= \"foo.BAZ1\" corpus= \"@array/corpus_baz1\" /> </skill> </manifest>","title":"Voice instructions"},{"location":"android/guide/skill.html#processing-instructions","text":"Context of instructions Some instructions are only open for receiving when in a certain context or when they meet a certain condition. Take playing music as an example. After receiving the \"play\" instruction, the music starts to play, and then the \"pause\" instruction is opened for receiving. When the music is not playing, the pause command is invalid. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <skill class= \"robot.example.FooSkill\" name= \"foo\" > <directive action= \"foo.BAZ1\" /> <directive action= \"foo.BAZ2\" context= \"foo_context\" /> </skill> </manifest> The following code illustrates how to enter and exit the context: public class FooSkill extends RobotSkill { private static final String CTX_FOO = \"foo_context\" ; @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive () { enterContext ( CTX_FOO ); //enter \"foo_context\" context // Perform some behavior asynchronously performSomeBehavior ( new Callback () { @Override public void onDone () { // After finishing performing the behavior, exit context exitContext ( CTX_FOO ); } }); } @OnDirective ( action = \"foo.BAZ2\" ) public void onBar2Directive () { // Be here // only after enterContext(CTX_FOO) and before exitContext(CTX_FOO); } } Note: The context variable value of the instruction used in the code must be consistent with that in the configuration file to be valid.","title":"Processing instructions"},{"location":"android/guide/skill.html#reply-instructions","text":"After all the instructions are processed, the instruction sender must be replied to. By default, the Cruzr system will automatically reply the processing result of the instruction (success or failure) to the caller. If you need to reply with the results autonomously, you can add a parameter Replier of the instruction receiving function. Reply success or failure public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , Replier replier ) { boolean success = performSomeBehavior (); if ( success ) { replier . replySuccess (); } else { replier . replyFailure ( errorCode , \"error message\" ); } } } Reply data result public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , Replier replier ) { Result result = performSomeBehavior (); replier . replySuccess ( result ); } } Reply viscosity data results public class FooSkill extends RobotSkill { @OnDirective ( action = \"foo.BAZ1\" ) public void onBar1Directive ( Directive directive , Replier replier ) { StickilyResult stickilyResult = performStickilyBehavior (); replier . replyStickily ( stickilyResult ); Result result = performResultBehavior (); replier . replySuccess ( result ); } } Note: Replier must be the last in the parameter list of the instruction receiving function","title":"Reply instructions"},{"location":"android/guide/skill.html#distribute-instructions","text":"Instructions can be distributed to skills through [ RobotContext[ or [ SkillManager[ . RobotContext is used to distribute instructions within a process. SkillManager is often used to distribute inter-process instructions. But it is not recommended to distribute the instructions with the process. Since the Java class of the skill is inherited from RobotContext , the skill class itself can distribute instructions to other skills within the application. public class Example { public void dispatchDirective ( RobotContext robotContext ) { // Dispatch a directive without a parameter object. Promise < Void , DispatchException > promise = robotContext . dispatchDirective ( action ); // Dispatch a directive with a parameter object. promise = robotContext . dispatchDirective ( action , arbitraryObject ); } \uff5d Note: For detailed usage of instruction distribution, please refer to Context and Skill Management","title":"Distribute instructions"},{"location":"android/guide/skill.html#skill-status","text":"Skills include four states: start, pause, resume, and stop. When the state changes, there will be corresponding notifications. There are two reasons for the change of skill state, namely, system control and manual control. The relationship between the states is shown in the following figure:","title":"Skill status"},{"location":"android/guide/skill.html#status-notification","text":"When the status of the skill changes, the system will send corresponding notifications, which selectively override the corresponding status and constrain the action of the skill. For example, the skill of music playback is in the running state while playing music. When receiving the system notification of onPause , the playback should be paused. public class FooSkill extends RobotSkill { @Override protected void onStart () { // Ready to receive directives } @Override protected void onResume ( boolean directiveWillFollow ) { // Resume the paused behavior } @Override protected void onPause ( SkillPauseCause cause ) { // Pause the performing behaviors } @Override protected void onStop ( SkillStopCause cause ) { // Stop the performing behaviors } } The onResume (boolean directiveWillFollow) parameter indicates whether an instruction will be received immediately after the skill is restored, which means the restoration of the current skill is triggered by a new instruction. The parameters onPause (SkillPauseCause cause) and onStop (SkillStopCause cause) describe the reason for the status change. The specific meanings is as follows: Name Descriptions BY_SYSTEM_DECISION The current result is triggered by system decision. BY_SKILL_MANAGEMENT The current result is triggered by the skill management class. BY_PAUSING_SELF The pause status is actively triggered. BY_STOPPING_SELF The stop status is actively triggered. BY_INTERNAL_ERROR The stop status is trigged by an internal error. Name Descriptions cause For the reasons for status trigger, please refer to the above constants condition The cause of the trigger is only valid when cause == BY_SYSTEM_DECISION.","title":"Status notification"},{"location":"android/guide/skill.html#control-status","text":"After receiving the instruction, the skill begins to perform the action. When the action is completed, stopSelf () should be called to actively stop the current skill. If you don't actively stop the skill, the skills that didn't execute any actions make the other skills which conflict with it unable to execute any instruction. Here are ways to stop the skill internally and externally, respectively. For example: The navigation skill starts navigating when a navigation instruction is received. When the navigation task ends, it should actively call stopSelf () to stop the skill. Otherwise, the navigation skill is still in the running state, which will cause other skills to fail to be executed. Here are ways to stop the skill internally and externally, respectively. Stop the skill internally public class FooSkill extends RobotSkill { private void onBehaviorEnd () { stopSelf (); } } Stop the skill externally public class Example { public void stopSkill () { Robot . globalContext (). stopSkill ( skillName ); } } In addition, in some cases, it is necessary to actively pause the skill. For example, when the user tells the robot to pause the music, the skill of playing music should be paused. Here are ways to pause the skill internally and externally, respectively. - Pause the skill internally public class FooSkill extends RobotSkill { private void onPauseNeeded () { pauseSelf (); } } Pause the skill externally public class Example { public void pauseSkill () { Robot . globalContext (). pauseSkill ( skillName ); } } Note: because background skill don\u2018t have pause or recover state, there is no situations to pause backgroud skills","title":"Control status"},{"location":"android/guide/other/async.html","text":"How to use promise Most of the interfaces of Cruzr provide synchronous and asynchronous calling methods. Developers can flexibly choose the calling method of the interface according to different scenarios. Promise Promise is a Class which provide synchronous and asynchronous interfaces. The listening is set through Promise to implement the asynchronous calling of the interface. The get method can be used to implement synchronous calls to the interface. All the calling methods of Promise are shown as follows: Methods Descriptions Promise.isPending() If it is in the status of waiting for results Promise.isResolved() If it is in completed state Promise.isCanceled() If it is currently in cancel request state Promise.isRejected() If Promise's request failed. Promise.cancel() Cancel the request Promise.cancelled(CancelledCallback callback) Promise requests to cancel the callback Promise.done(DoneCallback<? super D> callback) Promise requests to complete the callback Promise.fail(FailCallback<? super F> callback) Promise requests to callback the result of the failure Promise.always(AlwaysCallback<? super D, ? super F> callback) Promise requests to finish the callback, no matter if it is successful, fails, or is cancelled. Promise.get() Get the request result synchronously. This method will block the execution. Promise.getInterruptibly(long timeout, TimeUnit unit) Get the request result synchronously. This method supports Promise to wait. Promise.getFail() Get the reason for the failure. This method will not block execution, a specific value will only be given after it fails. For some interfaces with progress feedback, ProgressivePromise \uff0ca subclass of Promise \uff0ccan be used to set the listening progress callback. Methods Descriptions ProgressivePromise.progress(ProgressCallback<? super P> callback) Promise requests to callback the progress. Note: All callbacks of Promise are on the main thread. Asynchronous call This means non-blocking call. Asynchronous calls complete the callback listening through the Promise object returned by the interface. Single callback Promise < UnderstandingResult , UnderstandingException > promise = speechManager . understand ( \"what's the weather.\" ); promise . done ( new DoneCallback < UnderstandingResult > () { @Override public void onDone ( UnderstandingResult understandingResult ) { // The result of the callback will be returned here } }); Multiple callbacks Some interfaces have progress feedback, and progress notifications can be received through ProgressivePromise.progress () . ProgressivePromise < RecognitionResult , RecognitionException , RecognitionProgress > progressivePromise = speechManager . recognize (); progressivePromise . progress ( new ProgressCallback < RecognitionProgress > () { @Override public void onProgress ( RecognitionProgress recognitionProgress ) { // The progress of the callback will be returned here } }). done ( new DoneCallback < RecognitionResult > () { @Override public void onDone ( RecognitionResult recognitionResult ) { // The result of the callback will be returned here } }) ; Cancel the execution To cancel the request, just follow the cancel method of promise. The notification will be sent to CancelledCallback. promise . cancel (); Exceptional handling promise . fail ( new FailCallback < UnderstandingException > () { @Override public void onFail ( UnderstandingException e ) { // Any exceptions in the callback will be notified here } }); Synchronous call Unlike asynchronous calls, operations such as failure and cancellation of synchronous calls will be notified in the form of exceptions. We still use the call of speech recognition as an example. The method is as follows: try { ProgressivePromise < RecognitionResult , RecognitionException , RecognitionProgress > progressivePromise = speechManager . recognize (); RecognitionResult result = progressivePromise . get (); // This will block the return of the result or an exception } catch ( RecognitionException e ) { e . printStackTrace (); // [1] } catch ( CancelledException e ) { e . printStackTrace (); // [2] } [1] If the request failed, the corresponding exception will be returned. [2] If the request is canceled, CancelledException will be returned. Note: Synchronous call is a blocking method. Please avoid using it in the main thread, and you cannot get notifications of the progress.","title":"Promise Applications"},{"location":"android/guide/other/async.html#how-to-use-promise","text":"Most of the interfaces of Cruzr provide synchronous and asynchronous calling methods. Developers can flexibly choose the calling method of the interface according to different scenarios.","title":"How to use promise"},{"location":"android/guide/other/async.html#promise","text":"Promise is a Class which provide synchronous and asynchronous interfaces. The listening is set through Promise to implement the asynchronous calling of the interface. The get method can be used to implement synchronous calls to the interface. All the calling methods of Promise are shown as follows: Methods Descriptions Promise.isPending() If it is in the status of waiting for results Promise.isResolved() If it is in completed state Promise.isCanceled() If it is currently in cancel request state Promise.isRejected() If Promise's request failed. Promise.cancel() Cancel the request Promise.cancelled(CancelledCallback callback) Promise requests to cancel the callback Promise.done(DoneCallback<? super D> callback) Promise requests to complete the callback Promise.fail(FailCallback<? super F> callback) Promise requests to callback the result of the failure Promise.always(AlwaysCallback<? super D, ? super F> callback) Promise requests to finish the callback, no matter if it is successful, fails, or is cancelled. Promise.get() Get the request result synchronously. This method will block the execution. Promise.getInterruptibly(long timeout, TimeUnit unit) Get the request result synchronously. This method supports Promise to wait. Promise.getFail() Get the reason for the failure. This method will not block execution, a specific value will only be given after it fails. For some interfaces with progress feedback, ProgressivePromise \uff0ca subclass of Promise \uff0ccan be used to set the listening progress callback. Methods Descriptions ProgressivePromise.progress(ProgressCallback<? super P> callback) Promise requests to callback the progress. Note: All callbacks of Promise are on the main thread.","title":"Promise"},{"location":"android/guide/other/async.html#asynchronous-call","text":"This means non-blocking call. Asynchronous calls complete the callback listening through the Promise object returned by the interface.","title":"Asynchronous call"},{"location":"android/guide/other/async.html#single-callback","text":"Promise < UnderstandingResult , UnderstandingException > promise = speechManager . understand ( \"what's the weather.\" ); promise . done ( new DoneCallback < UnderstandingResult > () { @Override public void onDone ( UnderstandingResult understandingResult ) { // The result of the callback will be returned here } });","title":"Single callback"},{"location":"android/guide/other/async.html#multiple-callbacks","text":"Some interfaces have progress feedback, and progress notifications can be received through ProgressivePromise.progress () . ProgressivePromise < RecognitionResult , RecognitionException , RecognitionProgress > progressivePromise = speechManager . recognize (); progressivePromise . progress ( new ProgressCallback < RecognitionProgress > () { @Override public void onProgress ( RecognitionProgress recognitionProgress ) { // The progress of the callback will be returned here } }). done ( new DoneCallback < RecognitionResult > () { @Override public void onDone ( RecognitionResult recognitionResult ) { // The result of the callback will be returned here } }) ;","title":"Multiple callbacks"},{"location":"android/guide/other/async.html#cancel-the-execution","text":"To cancel the request, just follow the cancel method of promise. The notification will be sent to CancelledCallback. promise . cancel ();","title":"Cancel the execution"},{"location":"android/guide/other/async.html#exceptional-handling","text":"promise . fail ( new FailCallback < UnderstandingException > () { @Override public void onFail ( UnderstandingException e ) { // Any exceptions in the callback will be notified here } });","title":"Exceptional handling"},{"location":"android/guide/other/async.html#synchronous-call","text":"Unlike asynchronous calls, operations such as failure and cancellation of synchronous calls will be notified in the form of exceptions. We still use the call of speech recognition as an example. The method is as follows: try { ProgressivePromise < RecognitionResult , RecognitionException , RecognitionProgress > progressivePromise = speechManager . recognize (); RecognitionResult result = progressivePromise . get (); // This will block the return of the result or an exception } catch ( RecognitionException e ) { e . printStackTrace (); // [1] } catch ( CancelledException e ) { e . printStackTrace (); // [2] } [1] If the request failed, the corresponding exception will be returned. [2] If the request is canceled, CancelledException will be returned. Note: Synchronous call is a blocking method. Please avoid using it in the main thread, and you cannot get notifications of the progress.","title":"Synchronous call"},{"location":"android/guide/other/serialize.html","text":"Serialization and deserialization The Cruzr system will process a large number of inter-process communications during system operation, followed by frequent data serialization and deserialization. In order to simplify data serialization for developers, the Cruzr system provides Marshaller interface to process data serialization and deserialization. Abstract interface Marshaller Serializing raw data into byte [] is completed by the marshall function, and the operation of deserializing byte [] into raw data is completed by the unmarshall function, as follows: public interface Marshaller { < T > T unmarshall ( byte [] value , Type type ) throws IOException ; byte [] marshall ( Object value ) throws IOException ; } MarshallerFactory public interface MarshallerFactory { Marshaller create (); String contentType (); } Note: MarshallerFactory is used to create the Marshaller of specified types. The Cruzr system provides the commonly used Marshaller implementation by default. Developers can also create customized Marshaller as required. Data type The Cruzr system provides Marshaller implementations of three common data types: PARCELABLE ParcelableMarshaller is used to handle serialization and deserialization operations of the Android Parcelable data type. It is created using the ParcelableMarshallerFactory . public void testMarshaller () throws IOException { Marshaller marshaller = new ParcelableMarshallerFactory (). create (); Bundle parcelable = createYourValue (); byte [] bytes = marshaller . marshall ( parcelable ); parcelable = marshaller . unmarshall ( bytes , Bundle . class ); } PROTOBUF ProtobufMarshaller is used to handle the serialization and deserialization of the Protobuf data type. It is created using the ProtobufMarshallerFactory . public void testMarshaller () throws IOException { Marshaller marshaller = new ProtobufMarshallerFactory (). create (); ProtoClass protobuf = createYourValue (); byte [] bytes = marshaller . marshall ( protobuf ); protobuf = marshaller . unmarshall ( bytes , ProtoClass . class ); } JSON GsonMarshaller is used to handle the serialization and deserialization of common types. It is created using the GsonMarshallerFactory . public void testMarshaller () throws IOException { Marshaller marshaller = new GsonMarshallerFactory (). create (); NormalClass normal = createYourValue (); byte [] bytes = marshaller . marshall ( normal ); normal = marshaller . unmarshall ( bytes , NormalClass . class ); } Note: The original data type xxx.class of the above deserialization operation must be consistent with the original type of the serialized data. The definitions of the above types of constant parameters are located at com.ubtrobot.marshall.ContentTypes How to use The Cruzr system comes with the serialization and deserialization implementations of the PARCELABLE type. Other types or user-defined types can be added to the Cruzr system in the following ways: // add JSON marshaller Master . get (). addMarshallerFactory ( new GsonMarshallerFactory ()); // add custom marshaller Master . get (). addMarshallerFactory ( new CustomMarshallerFactory ()); Using serialization The use of serialization operations in the Cruzr system focuses on sending instructions and publishing events. The following is an example of publishing an event: // publish interface void publish ( String topic , Object parameterObj , String contentType ); //demo code robotContext . publish ( \"event.action.power.BATTERY_CHANGE\" , batteryProperties , ContentTypes . PARCELABLE ); Note: The size of the serialization parameters cannot exceed 1 MB. The serialization operation of publishing events only needs to fill in the original data of serialization parameterObj and specify the serialization type parameter contentType . The serialization operation of sending instructions is the same. Using deserialization The deserialization in the Cruzr system has been completed for developers. As long as the Marshaller of the corresponding type is added to the Cruzr system, the system will automatically complete it when deserialization is needed.","title":"Serialization"},{"location":"android/guide/other/serialize.html#serialization-and-deserialization","text":"The Cruzr system will process a large number of inter-process communications during system operation, followed by frequent data serialization and deserialization. In order to simplify data serialization for developers, the Cruzr system provides Marshaller interface to process data serialization and deserialization.","title":"Serialization and deserialization"},{"location":"android/guide/other/serialize.html#abstract-interface","text":"Marshaller Serializing raw data into byte [] is completed by the marshall function, and the operation of deserializing byte [] into raw data is completed by the unmarshall function, as follows: public interface Marshaller { < T > T unmarshall ( byte [] value , Type type ) throws IOException ; byte [] marshall ( Object value ) throws IOException ; } MarshallerFactory public interface MarshallerFactory { Marshaller create (); String contentType (); } Note: MarshallerFactory is used to create the Marshaller of specified types. The Cruzr system provides the commonly used Marshaller implementation by default. Developers can also create customized Marshaller as required.","title":"Abstract interface"},{"location":"android/guide/other/serialize.html#data-type","text":"The Cruzr system provides Marshaller implementations of three common data types: PARCELABLE ParcelableMarshaller is used to handle serialization and deserialization operations of the Android Parcelable data type. It is created using the ParcelableMarshallerFactory . public void testMarshaller () throws IOException { Marshaller marshaller = new ParcelableMarshallerFactory (). create (); Bundle parcelable = createYourValue (); byte [] bytes = marshaller . marshall ( parcelable ); parcelable = marshaller . unmarshall ( bytes , Bundle . class ); } PROTOBUF ProtobufMarshaller is used to handle the serialization and deserialization of the Protobuf data type. It is created using the ProtobufMarshallerFactory . public void testMarshaller () throws IOException { Marshaller marshaller = new ProtobufMarshallerFactory (). create (); ProtoClass protobuf = createYourValue (); byte [] bytes = marshaller . marshall ( protobuf ); protobuf = marshaller . unmarshall ( bytes , ProtoClass . class ); } JSON GsonMarshaller is used to handle the serialization and deserialization of common types. It is created using the GsonMarshallerFactory . public void testMarshaller () throws IOException { Marshaller marshaller = new GsonMarshallerFactory (). create (); NormalClass normal = createYourValue (); byte [] bytes = marshaller . marshall ( normal ); normal = marshaller . unmarshall ( bytes , NormalClass . class ); } Note: The original data type xxx.class of the above deserialization operation must be consistent with the original type of the serialized data. The definitions of the above types of constant parameters are located at com.ubtrobot.marshall.ContentTypes","title":"Data type"},{"location":"android/guide/other/serialize.html#how-to-use","text":"The Cruzr system comes with the serialization and deserialization implementations of the PARCELABLE type. Other types or user-defined types can be added to the Cruzr system in the following ways: // add JSON marshaller Master . get (). addMarshallerFactory ( new GsonMarshallerFactory ()); // add custom marshaller Master . get (). addMarshallerFactory ( new CustomMarshallerFactory ()); Using serialization The use of serialization operations in the Cruzr system focuses on sending instructions and publishing events. The following is an example of publishing an event: // publish interface void publish ( String topic , Object parameterObj , String contentType ); //demo code robotContext . publish ( \"event.action.power.BATTERY_CHANGE\" , batteryProperties , ContentTypes . PARCELABLE ); Note: The size of the serialization parameters cannot exceed 1 MB. The serialization operation of publishing events only needs to fill in the original data of serialization parameterObj and specify the serialization type parameter contentType . The serialization operation of sending instructions is the same. Using deserialization The deserialization in the Cruzr system has been completed for developers. As long as the Marshaller of the corresponding type is added to the Cruzr system, the system will automatically complete it when deserialization is needed.","title":"How to use"},{"location":"android/guide/system-service/diagnosis.html","text":"Diagnosis service The diagnosis service provides calling diagnosis related functions and monitoring fault information of each part. DiagnosisManager provides the main API of the diagnosis service, can be obtained through the RobotContext object. DiagnosisManager diagnosisManager = aRobotContext . getSystemService ( DiagnosisManager . SERVICE ); Diagnosis of parts Cruzr is composed of several parts. The diagnosis service can diagnose one or more parts at the same time. Each item will be described below. Diagnosis of a single component . Diagnose the head part with the following code: promise /* [1] */ = diagnosisManager . diagnose ( \"HeadPitch\" ) . progress ( new ProgressCallback < DiagnosisProgress > () { @Override public void onProgress ( DiagnosisProgress /* [2] */ diagnosisProgress ) { // Callback of diagnostic process } }) . done ( new DoneCallback < Diagnosis > () { @Override public void onDone ( Diagnosis /* [3] */ diagnosis ) { // Callback of diagnostic execution completion } }) . fail ( new FailCallback < DiagnosisException > () { @Override public void onFail ( DiagnosisException e ) { // Callback of diagnostic execution failed } }); [1] The asynchronous object returned by the diagnostic operation, through which the result is obtained and the processing is cancelled. Refer to Promise for specific usage. [2] The DiagnosisProgress object of the asynchronous callback describes the progress information of the diagnosis, including: Constants Descriptions DiagnosisProgress.PROGRESS_BEGAN Start of diagnosis DiagnosisProgress.PROGRESS_ENDED End of diagnosis [3] The Diagnosis object of the asynchronous callback describes the diagnosis result information, including: Attribute getter Descriptions Diagnosis.partId Part ID Diagnosis.faulty If it is faulty Diagnosis.info Diagnosis information Diagnosis.fault Fault information Diagnosis.cause Reason for fault Diagnose multiple parts ArrayList partIds = new ArrayList < String > (); partIds . add ( \"LShoulderYaw\" ); partIds . add ( \"RShoulderYaw\" ); promise = diagnosisManager . diagnose ( partIds ); Listen fault information Register listener DiagnosisListener diagnosisListener = new DiagnosisListener () { @Override public void onDiagnosed ( Diagnosis /* [1] */ diagnosis ) { // Callback of diagnostic information change } }; diagnosisManager . registerListener ( diagnosisListener ); /* [2] */ diagnosisManager . registerListener ( diagnosisListener , partId ); /* [3] */ diagnosisManager . registerListener ( diagnosisListener , partIds ); /* [4] */ [1] The Diagnosis object of the asynchronous callback describes the diagnosis result information. Please refer to Diagnosis [2] Register listener to listen the diagnosis information of all components [3] Register listener to listen the diagnosis information of a specific component [4] Register listener to listen the diagnosis information of multiple components Unregister listener diagnosisManager . unregisterListener ( diagnosisListener ); Repair the part If the device has the ability of self-healing, it can call its self-healing function through the diagnostic service when a part is abnormal. The following code is implemented: calling the self-healing function of the head joint promise /* [1] */ = diagnosisManager . repair ( \"HeadPitch\" ) . progress ( new ProgressCallback < RepairProgress > () { @Override public void onProgress ( RepairProgress /* [2] */ repairProgress ) { // Callback of repair process } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback of repair execution completion } }) . fail ( new FailCallback < RepairException > () { @Override public void onFail ( RepairException e ) { // Callback of repair execution failed } }); [1] The asynchronous object returned by the repair operation, through which the result is obtained and the processing is cancelled. Refer to Promise for specific usage. [2] The RepairProgress object of the asynchronous callback describes the progress information of the diagnosis, including: Constants Descriptions RepairProgress.PROGRESS_BEGAN Start repairs RepairProgress.PROGRESS_ENDED End repairs Diagnostic information code System diagnostic information Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_SYS_COMM 10000 System communication Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 system_comm_ok Normal system communication 10000 1 system_comm_anormaly Abnormal system communication 10001 Plug and unplug the network cable or restart by bottom switch Chassis motor diagnostic information Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_MOTOR1 20100 Motor No. 1 DIAG_MOTOR2 20200 Motor No. 2 DIAG_MOTOR3 20300 Motor No. 3 (unique to Cruzr1) Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 motor_ok The motor works normally 20100 1 motor_overcurrent Motor overcurrent 20101 Restart the robot or move to a safe area to wait 2 motor_can_error Abnormal motor CAN 20102 Restart or contact after sales 3 motor_overtension Motor overtension 20103 Restart or contact after sales 4 motor_uvp Motor voltage too low 20104 Restart or contact after sales 5 motor_overtemperature Motor temperature too high 20105 Restart or contact after sales 6 motor_vel_anormaly Abnormal motor speed 20106 Restart or contact after sales 7 motor_anormaly Abnormal sensor and current 20107 Restart or contact after sales Servo diagnostic information Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_SERVO1 30100 \"LShoulderPitch\" DIAG_SERVO2 30200 \"LShoulderRoll\" DIAG_SERVO3 30300 \"LShoulderYaw\" DIAG_SERVO4 30400 \"LElbowRoll\" DIAG_SERVO5 30500 \"LElbowYaw\" DIAG_SERVO7 30700 \"RShoulderPitch\" DIAG_SERVO8 30800 \"RShoulderRoll\" DIAG_SERVO9 30900 \"RShoulderYaw\" DIAG_SERVO10 31000 \"RElbowRoll\" DIAG_SERVO11 31100 \"RElbowYaw\" DIAG_SERVO13 31300 \"HeadYaw\" (unique to Cruzr1\uff09 DIAG_SERVO14 31400 \"HeadPitch\" DIAG_SERVO15 31500 \"LHand\" DIAG_SERVO16 31600 \"RHand\" Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 servo_ok Servo works normally 30100 1 servo_fail Servo does not respond to function operation or response timeout 30101 Restart or contact after sales 2 servo_paramerr Servo response data error 30102 Restart or contact after sales 3 servo_libiniterr Servo group structure read / write function not initialized or failed 30103 Restart or contact after sales 4 servo_tempraturelowerr Low temperature protection of Servo 30104 Restart or contact after sales 5 servo_tempraturelowwarn Low temperature alarm of Servo 30105 Restart or contact after sales 6 servo_tempraturehigherr Over temperature protection of Servo 30106 Restart or contact after sales 7 servo_tempraturehighwarn Over temperature alarm of Servo 30107 Restart or contact after sales 8 servo_voltagelowerr Low pressure protection of Servo 30108 Restart or contact after sales 9 servo_voltagelowwarn Low pressure alarm of Servo 30109 Restart or contact after sales 10 servo_voltagehigherr Over voltage protection of Servo 30110 Restart or contact after sales 11 servo_voltagehighwarn Over voltage alarm of Servo 30111 Restart or contact after sales 12 servo_currentovererr Over current protection of Servo 30112 Restart or contact after sales 13 servo_currentoverwarn Over current alarm of Servo 30113 Restart or contact after sales 14 servo_torqueovererr Moment protection of Servo 30114 Restart or contact after sales 15 servo_torqueoverwarn Torque warning of Servo 30115 Restart or contact after sales 16 servo_fuseerr Servo fuse dislocation protection 30116 Restart or contact after sales 17 servo_fusewarn Servo fuse dislocation alarm 30117 Restart or contact after sales 18 servo_pwmerr Locked rotor protection of Servo 30118 Restart or contact after sales 19 servo_pwmwarn Locked rotor alarm of Servo 30119 Restart or contact after sales 20 servo_speederr Servo speed deviation fault 30120 Restart or contact after sales 21 servo_speedwarn Servo speed deviation alarm 30121 Restart or contact after sales 22 servo_commuerr Communication disconnection protection of Servo 30122 Restart or contact after sales 23 servo_commuwarn Servo communication disconnection alarm 30123 Restart or contact after sales 24 servo_brokenerr Damage of Servo 30124 Restart or contact after sales 25 servo_brokenwarn Servo damage alarm 30125 Restart or contact after sales 26 servo_ambtempratureerr Ambient temperature fault of Servo 30126 Restart or contact after sales 27 servo_ambtempraturewarn Ambient temperature warning for Servo 30127 Restart or contact after sales 28 servo_poslimupwarn Servo position over upper limit alarm 30128 Restart or contact after sales 29 servo_poslimdownwarn Servo position over lower limit alarm 30129 Restart or contact after sales 30 servo_spdlimwarn Servo speed overrun alarm 30130 Restart or contact after sales 31 servo_torqlimwarn Servo current over limit alarm 30131 Restart Sensor diagnostic information LIDAR Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_LIDAR 40100 40200 (unique to Cruzr1s) 40300 (unique to Cruzr1s) 40400 (unique to Cruzr1s) Laser radar LIDAR Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 lidar_ok Lidar works normally 40100 1 lidar_no_device No lidar detected 40101 1\u3001Restart the device to check (turn off the bottom power supply and then turn it back on after power failure) 2\u3001Check the radar connection after restart 2 lidar_fq_error Abnormal frequency of lidar data 40102 1\u3001Restart the devic 2\u3001After restart, if the abnormality still exists, replace the radar 3 lidar_initial_error Radar node start failed 40103 After restart, if the abnormality still exists,Check hardware connection and radar 4 lidar_multi_device Multiple radar equipment 40104 After restart, if the abnormality still exists,Whether there are both the radar and the adapter board connected to the x86 5 lidar_no_data No data for radar topic 40105 Restart radar 6 lidar_zero_data Radar data is 0 40106 Ultrasonic Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_ULTRASONIC1 41100 Bottom No.1 ultrasonic DIAG_SENSOR_ULTRASONIC2 41200 Bottom No.2 ultrasonic DIAG_SENSOR_ULTRASONIC3 41300 Bottom No.3 ultrasonic DIAG_SENSOR_ULTRASONIC4 41400 Bottom No.4 ultrasonic DIAG_SENSOR_ULTRASONIC5 41500 Bottom No.5 ultrasonic DIAG_SENSOR_ULTRASONIC6 41600 Middle No.1 ultrasonic DIAG_SENSOR_ULTRASONIC7 41700 Middle No.2 ultrasonic DIAG_SENSOR_ULTRASONIC8 41800 Middle No.3 ultrasonic Ultrasonic Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_ultrasonic_ok Ultrasound normal 41100 1 sensor_ultrasonic_no_data No ultrasonic data 41101 contact after sales Infra-red Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_PSD_IR1 43100 Infra-red No.1 DIAG_SENSOR_PSD_IR2 43200 Infra-red No.2 DIAG_SENSOR_PSD_IR3 43300 Infra-red No.3 DIAG_SENSOR_PSD_IR4 43400 Infra-red No.4 DIAG_SENSOR_PSD_IR5 43500 Infra-red No.5 Infra-red Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_psd_ir_ok Infrared normal 43100 1 sensor_psd_ir_no_data No data in infrared 43101 contact after sales RGBD Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_RGBD 44000 Depth camera RGBDIdentification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_rgbd_ok Rgbd works normally 44000 1 sensor_rgbd_no_depthdata Rgbd has no depth data 44001 contact after sales 2 sensor_rgbd_no_rgbdata Rgbd has no color data 44002 contact after sales 3 sensor_rgbd_no_data rgbd has no data 44003 contact after sales TOF IR Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_TOF 45000 TOF IR TOF Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_tof_ok TOF works normally 45000 1 sensor_tof_no_data TOF has no data 45001 contact after sales 2 sensor_tof_zero_data TOF data is 0 45002 contact after sales 3 sensor_tof_triggerred Trigger TOF 45003 Geomagnetism Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_GEOMAGNETIC 46000 Geomagnetism Geomagnetism Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_geomagnetic_ok Geomagnetic normal 46000 1 sensor_geomagnetic_no_data Geomagnetic has no data 46001 contact after sales Navigation diagnostic information Navigation system Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_NAV_SYS_STATUS 60000 Navigation system Navigation system Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 nav_system_ok Navigation system OK (connected) 60000 1 nav_system_error Abnormal navigation system (not connected) 60001 Restart Navigation and location Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_NAV_SYS_LOCALIZATION 60100 Navigation and location Navigation and location Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 nav_localization_ok Relocation OK 60100 1 nav_localization_failed Relocation failed 60101 Relocation 2 nav_localization_timeout Relocation timeout 60102 Relocation Navigation behavior Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_NAV_SYS_NAVIGATION 60200 Navigation behavior Navigation behavior Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 nav_navigation_sucess Successfully navigated to the target point 60200 1 nav_navigation_unreachable Target point unreachable 60201 Check target location 2 nav_navigation_path_error Path not reachable 60202 Check the path for obstructions 3 nav_navigation_open_area The scene is empty, resulting in location loss 60203 Move the robot to the area with obvious environmental characteristics and relocate it 4 nav_navigation_crowed_passengers Multiple containment, resulting in location loss 60204 Relocation 5 nav_navigation_environment_changed Environment changes, resulting in location loss 60205 Relocate. If the exception still exists, it is recommended to update the map 6 nav_navigation_lidardata_empty No radar data, resulting in location loss 60206 Check lidar diagnostic solutions 7 nav_navigation_tof_triggerred Trigger TOF 60207 Check for TOF false alarm or cliff 8 nav_navigation_geomagnetic_triggerred Triggered geomagnetism 60208 Move the robot to a non geomagnetic area and start it up 98 nav_navigation_localization_lost Loss of location,resulting in location loss 60298 Relocation 99 nav_navigation_failed_unknown Unknown failure reason 60299 contact after sales Navigation map Diagnostic items Diagnostic items Diagnostic code Descriptions DIAG_NAV_SYS_LOAD_MAP 60300 Navigation map Navigation map Identification code Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 nav_load_map_sucess Map loaded successfully 60300 1 nav_load_map_failed Failed to load map 60301 1\u3001Check whether the map is the corresponding navigation scheme 2\u3001Check whether the map file is valid 3\u3001Re export the map 2 nav_load_map_timeout Map timeout 60302 Prompt map is too large, setting timeout","title":"Diagnosis service"},{"location":"android/guide/system-service/diagnosis.html#diagnosis-service","text":"The diagnosis service provides calling diagnosis related functions and monitoring fault information of each part. DiagnosisManager provides the main API of the diagnosis service, can be obtained through the RobotContext object. DiagnosisManager diagnosisManager = aRobotContext . getSystemService ( DiagnosisManager . SERVICE );","title":"Diagnosis service"},{"location":"android/guide/system-service/diagnosis.html#diagnosis-of-parts","text":"Cruzr is composed of several parts. The diagnosis service can diagnose one or more parts at the same time. Each item will be described below. Diagnosis of a single component . Diagnose the head part with the following code: promise /* [1] */ = diagnosisManager . diagnose ( \"HeadPitch\" ) . progress ( new ProgressCallback < DiagnosisProgress > () { @Override public void onProgress ( DiagnosisProgress /* [2] */ diagnosisProgress ) { // Callback of diagnostic process } }) . done ( new DoneCallback < Diagnosis > () { @Override public void onDone ( Diagnosis /* [3] */ diagnosis ) { // Callback of diagnostic execution completion } }) . fail ( new FailCallback < DiagnosisException > () { @Override public void onFail ( DiagnosisException e ) { // Callback of diagnostic execution failed } }); [1] The asynchronous object returned by the diagnostic operation, through which the result is obtained and the processing is cancelled. Refer to Promise for specific usage. [2] The DiagnosisProgress object of the asynchronous callback describes the progress information of the diagnosis, including: Constants Descriptions DiagnosisProgress.PROGRESS_BEGAN Start of diagnosis DiagnosisProgress.PROGRESS_ENDED End of diagnosis [3] The Diagnosis object of the asynchronous callback describes the diagnosis result information, including: Attribute getter Descriptions Diagnosis.partId Part ID Diagnosis.faulty If it is faulty Diagnosis.info Diagnosis information Diagnosis.fault Fault information Diagnosis.cause Reason for fault Diagnose multiple parts ArrayList partIds = new ArrayList < String > (); partIds . add ( \"LShoulderYaw\" ); partIds . add ( \"RShoulderYaw\" ); promise = diagnosisManager . diagnose ( partIds );","title":"Diagnosis of parts"},{"location":"android/guide/system-service/diagnosis.html#listen-fault-information","text":"Register listener DiagnosisListener diagnosisListener = new DiagnosisListener () { @Override public void onDiagnosed ( Diagnosis /* [1] */ diagnosis ) { // Callback of diagnostic information change } }; diagnosisManager . registerListener ( diagnosisListener ); /* [2] */ diagnosisManager . registerListener ( diagnosisListener , partId ); /* [3] */ diagnosisManager . registerListener ( diagnosisListener , partIds ); /* [4] */ [1] The Diagnosis object of the asynchronous callback describes the diagnosis result information. Please refer to Diagnosis [2] Register listener to listen the diagnosis information of all components [3] Register listener to listen the diagnosis information of a specific component [4] Register listener to listen the diagnosis information of multiple components Unregister listener diagnosisManager . unregisterListener ( diagnosisListener );","title":"Listen fault information"},{"location":"android/guide/system-service/diagnosis.html#repair-the-part","text":"If the device has the ability of self-healing, it can call its self-healing function through the diagnostic service when a part is abnormal. The following code is implemented: calling the self-healing function of the head joint promise /* [1] */ = diagnosisManager . repair ( \"HeadPitch\" ) . progress ( new ProgressCallback < RepairProgress > () { @Override public void onProgress ( RepairProgress /* [2] */ repairProgress ) { // Callback of repair process } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback of repair execution completion } }) . fail ( new FailCallback < RepairException > () { @Override public void onFail ( RepairException e ) { // Callback of repair execution failed } }); [1] The asynchronous object returned by the repair operation, through which the result is obtained and the processing is cancelled. Refer to Promise for specific usage. [2] The RepairProgress object of the asynchronous callback describes the progress information of the diagnosis, including: Constants Descriptions RepairProgress.PROGRESS_BEGAN Start repairs RepairProgress.PROGRESS_ENDED End repairs","title":"Repair the part"},{"location":"android/guide/system-service/diagnosis.html#diagnostic-information-code","text":"","title":"Diagnostic information code"},{"location":"android/guide/system-service/diagnosis.html#system-diagnostic-information","text":"","title":"System diagnostic information"},{"location":"android/guide/system-service/diagnosis.html#diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_SYS_COMM 10000 System communication","title":"Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 system_comm_ok Normal system communication 10000 1 system_comm_anormaly Abnormal system communication 10001 Plug and unplug the network cable or restart by bottom switch","title":"Identification code"},{"location":"android/guide/system-service/diagnosis.html#chassis-motor-diagnostic-information","text":"","title":"Chassis motor diagnostic information"},{"location":"android/guide/system-service/diagnosis.html#diagnostic-items_1","text":"Diagnostic items Diagnostic code Descriptions DIAG_MOTOR1 20100 Motor No. 1 DIAG_MOTOR2 20200 Motor No. 2 DIAG_MOTOR3 20300 Motor No. 3 (unique to Cruzr1)","title":"Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#identification-code_1","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 motor_ok The motor works normally 20100 1 motor_overcurrent Motor overcurrent 20101 Restart the robot or move to a safe area to wait 2 motor_can_error Abnormal motor CAN 20102 Restart or contact after sales 3 motor_overtension Motor overtension 20103 Restart or contact after sales 4 motor_uvp Motor voltage too low 20104 Restart or contact after sales 5 motor_overtemperature Motor temperature too high 20105 Restart or contact after sales 6 motor_vel_anormaly Abnormal motor speed 20106 Restart or contact after sales 7 motor_anormaly Abnormal sensor and current 20107 Restart or contact after sales","title":"Identification code"},{"location":"android/guide/system-service/diagnosis.html#servo-diagnostic-information","text":"","title":"Servo diagnostic information"},{"location":"android/guide/system-service/diagnosis.html#diagnostic-items_2","text":"Diagnostic items Diagnostic code Descriptions DIAG_SERVO1 30100 \"LShoulderPitch\" DIAG_SERVO2 30200 \"LShoulderRoll\" DIAG_SERVO3 30300 \"LShoulderYaw\" DIAG_SERVO4 30400 \"LElbowRoll\" DIAG_SERVO5 30500 \"LElbowYaw\" DIAG_SERVO7 30700 \"RShoulderPitch\" DIAG_SERVO8 30800 \"RShoulderRoll\" DIAG_SERVO9 30900 \"RShoulderYaw\" DIAG_SERVO10 31000 \"RElbowRoll\" DIAG_SERVO11 31100 \"RElbowYaw\" DIAG_SERVO13 31300 \"HeadYaw\" (unique to Cruzr1\uff09 DIAG_SERVO14 31400 \"HeadPitch\" DIAG_SERVO15 31500 \"LHand\" DIAG_SERVO16 31600 \"RHand\"","title":"Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#identification-code_2","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 servo_ok Servo works normally 30100 1 servo_fail Servo does not respond to function operation or response timeout 30101 Restart or contact after sales 2 servo_paramerr Servo response data error 30102 Restart or contact after sales 3 servo_libiniterr Servo group structure read / write function not initialized or failed 30103 Restart or contact after sales 4 servo_tempraturelowerr Low temperature protection of Servo 30104 Restart or contact after sales 5 servo_tempraturelowwarn Low temperature alarm of Servo 30105 Restart or contact after sales 6 servo_tempraturehigherr Over temperature protection of Servo 30106 Restart or contact after sales 7 servo_tempraturehighwarn Over temperature alarm of Servo 30107 Restart or contact after sales 8 servo_voltagelowerr Low pressure protection of Servo 30108 Restart or contact after sales 9 servo_voltagelowwarn Low pressure alarm of Servo 30109 Restart or contact after sales 10 servo_voltagehigherr Over voltage protection of Servo 30110 Restart or contact after sales 11 servo_voltagehighwarn Over voltage alarm of Servo 30111 Restart or contact after sales 12 servo_currentovererr Over current protection of Servo 30112 Restart or contact after sales 13 servo_currentoverwarn Over current alarm of Servo 30113 Restart or contact after sales 14 servo_torqueovererr Moment protection of Servo 30114 Restart or contact after sales 15 servo_torqueoverwarn Torque warning of Servo 30115 Restart or contact after sales 16 servo_fuseerr Servo fuse dislocation protection 30116 Restart or contact after sales 17 servo_fusewarn Servo fuse dislocation alarm 30117 Restart or contact after sales 18 servo_pwmerr Locked rotor protection of Servo 30118 Restart or contact after sales 19 servo_pwmwarn Locked rotor alarm of Servo 30119 Restart or contact after sales 20 servo_speederr Servo speed deviation fault 30120 Restart or contact after sales 21 servo_speedwarn Servo speed deviation alarm 30121 Restart or contact after sales 22 servo_commuerr Communication disconnection protection of Servo 30122 Restart or contact after sales 23 servo_commuwarn Servo communication disconnection alarm 30123 Restart or contact after sales 24 servo_brokenerr Damage of Servo 30124 Restart or contact after sales 25 servo_brokenwarn Servo damage alarm 30125 Restart or contact after sales 26 servo_ambtempratureerr Ambient temperature fault of Servo 30126 Restart or contact after sales 27 servo_ambtempraturewarn Ambient temperature warning for Servo 30127 Restart or contact after sales 28 servo_poslimupwarn Servo position over upper limit alarm 30128 Restart or contact after sales 29 servo_poslimdownwarn Servo position over lower limit alarm 30129 Restart or contact after sales 30 servo_spdlimwarn Servo speed overrun alarm 30130 Restart or contact after sales 31 servo_torqlimwarn Servo current over limit alarm 30131 Restart","title":"Identification code"},{"location":"android/guide/system-service/diagnosis.html#sensor-diagnostic-information","text":"","title":"Sensor diagnostic information"},{"location":"android/guide/system-service/diagnosis.html#lidar-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_LIDAR 40100 40200 (unique to Cruzr1s) 40300 (unique to Cruzr1s) 40400 (unique to Cruzr1s) Laser radar","title":"LIDAR Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#lidar-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 lidar_ok Lidar works normally 40100 1 lidar_no_device No lidar detected 40101 1\u3001Restart the device to check (turn off the bottom power supply and then turn it back on after power failure) 2\u3001Check the radar connection after restart 2 lidar_fq_error Abnormal frequency of lidar data 40102 1\u3001Restart the devic 2\u3001After restart, if the abnormality still exists, replace the radar 3 lidar_initial_error Radar node start failed 40103 After restart, if the abnormality still exists,Check hardware connection and radar 4 lidar_multi_device Multiple radar equipment 40104 After restart, if the abnormality still exists,Whether there are both the radar and the adapter board connected to the x86 5 lidar_no_data No data for radar topic 40105 Restart radar 6 lidar_zero_data Radar data is 0 40106","title":"LIDAR Identification code"},{"location":"android/guide/system-service/diagnosis.html#ultrasonic-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_ULTRASONIC1 41100 Bottom No.1 ultrasonic DIAG_SENSOR_ULTRASONIC2 41200 Bottom No.2 ultrasonic DIAG_SENSOR_ULTRASONIC3 41300 Bottom No.3 ultrasonic DIAG_SENSOR_ULTRASONIC4 41400 Bottom No.4 ultrasonic DIAG_SENSOR_ULTRASONIC5 41500 Bottom No.5 ultrasonic DIAG_SENSOR_ULTRASONIC6 41600 Middle No.1 ultrasonic DIAG_SENSOR_ULTRASONIC7 41700 Middle No.2 ultrasonic DIAG_SENSOR_ULTRASONIC8 41800 Middle No.3 ultrasonic","title":"Ultrasonic Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#ultrasonic-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_ultrasonic_ok Ultrasound normal 41100 1 sensor_ultrasonic_no_data No ultrasonic data 41101 contact after sales","title":"Ultrasonic Identification code"},{"location":"android/guide/system-service/diagnosis.html#infra-red-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_PSD_IR1 43100 Infra-red No.1 DIAG_SENSOR_PSD_IR2 43200 Infra-red No.2 DIAG_SENSOR_PSD_IR3 43300 Infra-red No.3 DIAG_SENSOR_PSD_IR4 43400 Infra-red No.4 DIAG_SENSOR_PSD_IR5 43500 Infra-red No.5","title":"Infra-red Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#infra-red-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_psd_ir_ok Infrared normal 43100 1 sensor_psd_ir_no_data No data in infrared 43101 contact after sales","title":"Infra-red Identification code"},{"location":"android/guide/system-service/diagnosis.html#rgbd-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_RGBD 44000 Depth camera","title":"RGBD Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#rgbdidentification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_rgbd_ok Rgbd works normally 44000 1 sensor_rgbd_no_depthdata Rgbd has no depth data 44001 contact after sales 2 sensor_rgbd_no_rgbdata Rgbd has no color data 44002 contact after sales 3 sensor_rgbd_no_data rgbd has no data 44003 contact after sales","title":"RGBDIdentification code"},{"location":"android/guide/system-service/diagnosis.html#tof-ir-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_TOF 45000 TOF IR","title":"TOF IR Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#tof-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_tof_ok TOF works normally 45000 1 sensor_tof_no_data TOF has no data 45001 contact after sales 2 sensor_tof_zero_data TOF data is 0 45002 contact after sales 3 sensor_tof_triggerred Trigger TOF 45003","title":"TOF Identification code"},{"location":"android/guide/system-service/diagnosis.html#geomagnetism-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_SENSOR_GEOMAGNETIC 46000 Geomagnetism","title":"Geomagnetism Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#geomagnetism-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 sensor_geomagnetic_ok Geomagnetic normal 46000 1 sensor_geomagnetic_no_data Geomagnetic has no data 46001 contact after sales","title":"Geomagnetism Identification code"},{"location":"android/guide/system-service/diagnosis.html#navigation-diagnostic-information","text":"","title":"Navigation diagnostic information"},{"location":"android/guide/system-service/diagnosis.html#navigation-system-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_NAV_SYS_STATUS 60000 Navigation system","title":"Navigation system Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#navigation-system-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 nav_system_ok Navigation system OK (connected) 60000 1 nav_system_error Abnormal navigation system (not connected) 60001 Restart","title":"Navigation system Identification code"},{"location":"android/guide/system-service/diagnosis.html#navigation-and-location-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_NAV_SYS_LOCALIZATION 60100 Navigation and location","title":"Navigation and location Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#navigation-and-location-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 nav_localization_ok Relocation OK 60100 1 nav_localization_failed Relocation failed 60101 Relocation 2 nav_localization_timeout Relocation timeout 60102 Relocation","title":"Navigation and location Identification code"},{"location":"android/guide/system-service/diagnosis.html#navigation-behavior-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_NAV_SYS_NAVIGATION 60200 Navigation behavior","title":"Navigation behavior Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#navigation-behavior-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 nav_navigation_sucess Successfully navigated to the target point 60200 1 nav_navigation_unreachable Target point unreachable 60201 Check target location 2 nav_navigation_path_error Path not reachable 60202 Check the path for obstructions 3 nav_navigation_open_area The scene is empty, resulting in location loss 60203 Move the robot to the area with obvious environmental characteristics and relocate it 4 nav_navigation_crowed_passengers Multiple containment, resulting in location loss 60204 Relocation 5 nav_navigation_environment_changed Environment changes, resulting in location loss 60205 Relocate. If the exception still exists, it is recommended to update the map 6 nav_navigation_lidardata_empty No radar data, resulting in location loss 60206 Check lidar diagnostic solutions 7 nav_navigation_tof_triggerred Trigger TOF 60207 Check for TOF false alarm or cliff 8 nav_navigation_geomagnetic_triggerred Triggered geomagnetism 60208 Move the robot to a non geomagnetic area and start it up 98 nav_navigation_localization_lost Loss of location,resulting in location loss 60298 Relocation 99 nav_navigation_failed_unknown Unknown failure reason 60299 contact after sales","title":"Navigation behavior Identification code"},{"location":"android/guide/system-service/diagnosis.html#navigation-map-diagnostic-items","text":"Diagnostic items Diagnostic code Descriptions DIAG_NAV_SYS_LOAD_MAP 60300 Navigation map","title":"Navigation map Diagnostic items"},{"location":"android/guide/system-service/diagnosis.html#navigation-map-identification-code","text":"Information code Diagnostic information Descriptions Identification code (Diagnostic code+Information code) Suggested treatment measures 0 nav_load_map_sucess Map loaded successfully 60300 1 nav_load_map_failed Failed to load map 60301 1\u3001Check whether the map is the corresponding navigation scheme 2\u3001Check whether the map file is valid 3\u3001Re export the map 2 nav_load_map_timeout Map timeout 60302 Prompt map is too large, setting timeout","title":"Navigation map Identification code"},{"location":"android/guide/system-service/emotion.html","text":"Emotion service The emotion service provides the API's ability to call the device's \"emotions\". As an emotion service access agent, EmotionManager provides the main API of the emotion service, which can be obtained through the RobotContext object. EmotionManager emotionManager = aRobotContext . getSystemService ( EmotionManager . SERVICE ); Display emotions When the device is required to display emotions such as \"happy\", \"angry\", \"sad\", and \"laugh\", like humans do, it can be achieved by displaying the emotions. promise /* [2] */ = emotionManager . express ( Uri . parse ( \"emotion://va/happy\" ) /* [1] */ ) . progress ( new ProgressCallback < ExpressingProgress > () { @Override public void onProgress ( ExpressingProgress expressingProgress /* [3] */ ) { // Callback the display process of the emotions } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback the emotions finish displaying } }) . fail ( new FailCallback < EmotionException > () { @Override public void onFail ( EmotionException e ) { // Callback the emotion display failed } }); [1]The unique identification of the emotion to be displayed. [2]Return the asynchronous object that is waiting to display progress and results. Through the object, you can wait or monitor the progress and results, or cancel the display process. See Promise for specific usage. [3]The ExpressingProgress object of the asynchronous callback describes the progress information of the emotion display, including: Attribute getter Descriptions ExpressingProgress.progress Progress information began : start of display ended : end of display By specifying options, you can adjust the action of the emotion display. ExpressingOption /* [1] */ option = new ExpressingOption . Builder ( Uri . parse ( \"emotion://va/happy\" )). build (); promise = emotionManager . express ( option ); [1] The ExpressingOption object is constructed using ExpressingOption.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor(uri) The URI of the emotion to be displayed must be imported during construction Builder.setSticky(sticky) Whether the display process is allowed to be interrupted true : cannot be interrupted false Builder.setLoops(loops) Number of loop display 0 : unlimited 0 Builder.setExtension(extension) Customized parameters JsonObjectString.EMPTY_OBJECT If you need the device to cry and laugh like a human, you can specify multiple options. ExpressingOption option1 = new ExpressingOption . Builder ( Uri . parse ( \"emotion://va/cry\" )). build (); ExpressingOption option2 = new ExpressingOption . Builder ( Uri . parse ( \"emotion://va/happy\" )). build (); promise = emotionManager . expressSerially ( option1 , option2 /* [1] */ ); [1] The set of emotions to be displayed can also be written as follows: List < ExpressingOption > optionList = new ArrayList <> (); optionList . add ( option1 ); optionList . add ( option2 ); promise = emotionManager . expressSerially ( optionList ); If you want to know the emotion that is currently displayed, you can use the following code. emotionManager . getExpressing (); The unique identifier of the emotion is returned. If you want to know if there are any emotions on display, you can use the following code. emotionManager . isExpressing (); If true is returned, it means that there are emotions on display. Close the emotion If you want the device to return to the normal interactive interface and no longer display emotions, use the following code. emotionManager . dismiss (); Emotion list Science style name id daze techface_daze smile techface_smile happy techface_happy love techface_love shy techface_shy music techface_music speak techface_speak upset techface_upset amazing techface_amazing naughty techface_naughty grin techface_grin wronged techface_wronged proud techface_proud default techface_default data techface_data monitor techface_monitor standby techface_standby doubt techface_doubt Lovely style name id daze face_daze happy face_happy love face_love shy face_shy music face_music speak face_speak upset face_upset amazing face_amazing naughty face_naughty grin face_grin wronged face_wronged proud face_proud smile face_smile default face_default Little star style name id proud littlestar_proud daze littlestar_daze sleepy littlestar_sleepy awkward littlestar_embarrassed shy littlestar_shy amazing littlestar_amazing happy littlestar_happy naughty littlestar_naughty smile littlestar_smile like littlestar_love Emotion picture","title":"Emotion service"},{"location":"android/guide/system-service/emotion.html#emotion-service","text":"The emotion service provides the API's ability to call the device's \"emotions\". As an emotion service access agent, EmotionManager provides the main API of the emotion service, which can be obtained through the RobotContext object. EmotionManager emotionManager = aRobotContext . getSystemService ( EmotionManager . SERVICE );","title":"Emotion service"},{"location":"android/guide/system-service/emotion.html#display-emotions","text":"When the device is required to display emotions such as \"happy\", \"angry\", \"sad\", and \"laugh\", like humans do, it can be achieved by displaying the emotions. promise /* [2] */ = emotionManager . express ( Uri . parse ( \"emotion://va/happy\" ) /* [1] */ ) . progress ( new ProgressCallback < ExpressingProgress > () { @Override public void onProgress ( ExpressingProgress expressingProgress /* [3] */ ) { // Callback the display process of the emotions } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback the emotions finish displaying } }) . fail ( new FailCallback < EmotionException > () { @Override public void onFail ( EmotionException e ) { // Callback the emotion display failed } }); [1]The unique identification of the emotion to be displayed. [2]Return the asynchronous object that is waiting to display progress and results. Through the object, you can wait or monitor the progress and results, or cancel the display process. See Promise for specific usage. [3]The ExpressingProgress object of the asynchronous callback describes the progress information of the emotion display, including: Attribute getter Descriptions ExpressingProgress.progress Progress information began : start of display ended : end of display By specifying options, you can adjust the action of the emotion display. ExpressingOption /* [1] */ option = new ExpressingOption . Builder ( Uri . parse ( \"emotion://va/happy\" )). build (); promise = emotionManager . express ( option ); [1] The ExpressingOption object is constructed using ExpressingOption.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor(uri) The URI of the emotion to be displayed must be imported during construction Builder.setSticky(sticky) Whether the display process is allowed to be interrupted true : cannot be interrupted false Builder.setLoops(loops) Number of loop display 0 : unlimited 0 Builder.setExtension(extension) Customized parameters JsonObjectString.EMPTY_OBJECT If you need the device to cry and laugh like a human, you can specify multiple options. ExpressingOption option1 = new ExpressingOption . Builder ( Uri . parse ( \"emotion://va/cry\" )). build (); ExpressingOption option2 = new ExpressingOption . Builder ( Uri . parse ( \"emotion://va/happy\" )). build (); promise = emotionManager . expressSerially ( option1 , option2 /* [1] */ ); [1] The set of emotions to be displayed can also be written as follows: List < ExpressingOption > optionList = new ArrayList <> (); optionList . add ( option1 ); optionList . add ( option2 ); promise = emotionManager . expressSerially ( optionList ); If you want to know the emotion that is currently displayed, you can use the following code. emotionManager . getExpressing (); The unique identifier of the emotion is returned. If you want to know if there are any emotions on display, you can use the following code. emotionManager . isExpressing (); If true is returned, it means that there are emotions on display.","title":"Display emotions"},{"location":"android/guide/system-service/emotion.html#close-the-emotion","text":"If you want the device to return to the normal interactive interface and no longer display emotions, use the following code. emotionManager . dismiss ();","title":"Close the emotion"},{"location":"android/guide/system-service/emotion.html#emotion-list","text":"Science style name id daze techface_daze smile techface_smile happy techface_happy love techface_love shy techface_shy music techface_music speak techface_speak upset techface_upset amazing techface_amazing naughty techface_naughty grin techface_grin wronged techface_wronged proud techface_proud default techface_default data techface_data monitor techface_monitor standby techface_standby doubt techface_doubt Lovely style name id daze face_daze happy face_happy love face_love shy face_shy music face_music speak face_speak upset face_upset amazing face_amazing naughty face_naughty grin face_grin wronged face_wronged proud face_proud smile face_smile default face_default Little star style name id proud littlestar_proud daze littlestar_daze sleepy littlestar_sleepy awkward littlestar_embarrassed shy littlestar_shy amazing littlestar_amazing happy littlestar_happy naughty littlestar_naughty smile littlestar_smile like littlestar_love","title":"Emotion list"},{"location":"android/guide/system-service/emotion.html#emotion-picture","text":"","title":"Emotion picture"},{"location":"android/guide/system-service/light.html","text":"Lighting services Lighting services provide the ability to API call control device lights. As the lighting service access agent, LightManager object provides the main API of the lighting service, which can be obtained through the RobotContext object. LightManager lightManager = aRobotContext . getSystemService ( LightManager . SERVICE ); Get the list of lights Get the number of lights on the device and the details of the lights using the following code: List < LightDevice > /* [1] */ devices = lightManager . getDeviceList (); [1]Light collection, LightDevice is the information of the light, including: Attribute getter Descriptions LightDevice.id The unique identification of the light LightDevice.name The name of the light LightDevice.description The description of the light If you want to find the details of the light by its id, you can use the following code: try { LightDevice lightDevice = lightManager . getDevice ( \"lightId\" ); } catch ( LightNotFoundException e ) { // Callbback when the detailed information on the light is not found } Turn the light on and off If you want to turn the light on, you can use the following code: Map < String , Integer > /* [1] */ devices = new HashMap <> (); devices . put ( \"lightId\" , Color . BLUE ); lightManager . turnOn ( devices ); [1] The collection of lights you want to turn on, key is the id of the light, and value is the color displayed after the light is turned on. If you only want to turn on one light, you can use the following code: lightManager . turnOn ( \"lightId\" , Color . BLUE ); If you want to turn the light off, you can use the following code: List < String > devices = new ArrayList <> (); devices . add ( \"lightId\" ); lightManager . turnOff ( devices ); If you want to turn off multiple lights, but don't want to create a collection, you can use the following code: lightManager . turnOff ( \"lightId001\" , \"lightId002\" ); If you only want to turn off one light, you can use the following code: lightManager . turnOff ( \"lightId\" ); If you want to know whether the light is on or off, you can use the following code: boolean isTurnedOn = lightManager . isTurnedOn ( \"lightId\" ); If the value returned isTurnedOn is true , it means that the light is on. Change the light color If you want to change the light color, you can use the following code: Map < String , Integer > /* [1] */ devices = new HashMap <> (); devices . put ( \"lightId\" , Color . BLUE ); lightManager . changeColors ( devices ); [1]The collection of lights whose color you want to change, key is the id of the light, and value is the color of the light. If you only want to change the color of one light, you can use the following code: lightManager . changeColor ( \"lightId\" , Color . BLUE ); If you want to get the color of the light, you can use the following code: int color = lightManager . getColor ( \"lightId\" ); Change the lighting effect If you want the lights to show some dynamic effects, you can use the following code: promise /* [2] */ = lightManager . displayEffect ( \"lightId\" , Uri . parse ( \"light://effect/1\" ) /* [1] */ ) . progress ( new ProgressCallback < DisplayProgress > () { @Override public void onProgress ( DisplayProgress displayProgress /* [3] */ ) { // Callback the display process of the lighting effect } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback the lighting effect is displayed } }) . fail ( new FailCallback < LightException > () { @Override public void onFail ( LightException e ) { // Callback the lighting effect display goes wrong } }); [1]The unique identification of the lighting effect to be displayed. [2]Return the asynchronous object that is waiting to display progress and results. Through the object, you can wait or monitor the progress and results, or cancel the display process. See Promise for specific usage. [3] The DisplayProgress object of the asynchronous callback describes the progress information of the light effect display, including: Attribute getter Descriptions DisplayProgress.progress Progress information began : start of display ended : end of display Through specific options, you can adjust the action of the light display. DisplayOption /* [1] */ option = new DisplayOption . Builder ( \"lightId\" , Uri . parse ( \"light://effect/1\" )). build (); promise = lightManager . displayEffect ( option ); [1] The DisplayOption object is built through DisplayOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(lightId, effectUri) The id of the light and the unique identifier of the light effect must be imported during construction. Builder.setSticky(sticky) Whether the display process is allowed to be interrupted by other threads true : if it is not in operation, it will be interrupted false Builder.setLoops(loops) Number of loop display 0: unlimited 0 Builder.setExtension(extension) Customized parameters JsonObjectString.EMPTY_OBJECT If you want to display multiple lighting effects, you can specify multiple options, which can be achieved in the following two ways: DisplayOption option1 = new DisplayOption . Builder ( \"lightId001\" , Uri . parse ( \"light://effect/1\" )). setLoops ( 1 ). build (); DisplayOption option2 = new DisplayOption . Builder ( \"lightId002\" , Uri . parse ( \"light://effect/2\" )). setLoops ( 1 ). build (); promise = lightManager . displayEffectSerially ( option1 , option2 ); List < DisplayOption > options = new ArrayList <> (); options . add ( option1 ); options . add ( option2 ); promise = lightManager . displayEffectSerially ( options ); List of lights and light effects Lights and light effects can be set by constants Light color int red RED white WHITE blue BLUE green GREEN orange ORANGE yellow CYAN purple PURPLE Lighting effect int breathe BREATHE warning WARNING Flash once FLASH_ONE Flash twice FLASH_TWO Bright for 3 seconds BRIGHT_3_SECOND Bright infinitely BRIGHT_INFINITY","title":"Lighting services"},{"location":"android/guide/system-service/light.html#lighting-services","text":"Lighting services provide the ability to API call control device lights. As the lighting service access agent, LightManager object provides the main API of the lighting service, which can be obtained through the RobotContext object. LightManager lightManager = aRobotContext . getSystemService ( LightManager . SERVICE );","title":"Lighting services"},{"location":"android/guide/system-service/light.html#get-the-list-of-lights","text":"Get the number of lights on the device and the details of the lights using the following code: List < LightDevice > /* [1] */ devices = lightManager . getDeviceList (); [1]Light collection, LightDevice is the information of the light, including: Attribute getter Descriptions LightDevice.id The unique identification of the light LightDevice.name The name of the light LightDevice.description The description of the light If you want to find the details of the light by its id, you can use the following code: try { LightDevice lightDevice = lightManager . getDevice ( \"lightId\" ); } catch ( LightNotFoundException e ) { // Callbback when the detailed information on the light is not found }","title":"Get the list of lights"},{"location":"android/guide/system-service/light.html#turn-the-light-on-and-off","text":"If you want to turn the light on, you can use the following code: Map < String , Integer > /* [1] */ devices = new HashMap <> (); devices . put ( \"lightId\" , Color . BLUE ); lightManager . turnOn ( devices ); [1] The collection of lights you want to turn on, key is the id of the light, and value is the color displayed after the light is turned on. If you only want to turn on one light, you can use the following code: lightManager . turnOn ( \"lightId\" , Color . BLUE ); If you want to turn the light off, you can use the following code: List < String > devices = new ArrayList <> (); devices . add ( \"lightId\" ); lightManager . turnOff ( devices ); If you want to turn off multiple lights, but don't want to create a collection, you can use the following code: lightManager . turnOff ( \"lightId001\" , \"lightId002\" ); If you only want to turn off one light, you can use the following code: lightManager . turnOff ( \"lightId\" ); If you want to know whether the light is on or off, you can use the following code: boolean isTurnedOn = lightManager . isTurnedOn ( \"lightId\" ); If the value returned isTurnedOn is true , it means that the light is on.","title":"Turn the light on and off"},{"location":"android/guide/system-service/light.html#change-the-light-color","text":"If you want to change the light color, you can use the following code: Map < String , Integer > /* [1] */ devices = new HashMap <> (); devices . put ( \"lightId\" , Color . BLUE ); lightManager . changeColors ( devices ); [1]The collection of lights whose color you want to change, key is the id of the light, and value is the color of the light. If you only want to change the color of one light, you can use the following code: lightManager . changeColor ( \"lightId\" , Color . BLUE ); If you want to get the color of the light, you can use the following code: int color = lightManager . getColor ( \"lightId\" );","title":"Change the light color"},{"location":"android/guide/system-service/light.html#change-the-lighting-effect","text":"If you want the lights to show some dynamic effects, you can use the following code: promise /* [2] */ = lightManager . displayEffect ( \"lightId\" , Uri . parse ( \"light://effect/1\" ) /* [1] */ ) . progress ( new ProgressCallback < DisplayProgress > () { @Override public void onProgress ( DisplayProgress displayProgress /* [3] */ ) { // Callback the display process of the lighting effect } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback the lighting effect is displayed } }) . fail ( new FailCallback < LightException > () { @Override public void onFail ( LightException e ) { // Callback the lighting effect display goes wrong } }); [1]The unique identification of the lighting effect to be displayed. [2]Return the asynchronous object that is waiting to display progress and results. Through the object, you can wait or monitor the progress and results, or cancel the display process. See Promise for specific usage. [3] The DisplayProgress object of the asynchronous callback describes the progress information of the light effect display, including: Attribute getter Descriptions DisplayProgress.progress Progress information began : start of display ended : end of display Through specific options, you can adjust the action of the light display. DisplayOption /* [1] */ option = new DisplayOption . Builder ( \"lightId\" , Uri . parse ( \"light://effect/1\" )). build (); promise = lightManager . displayEffect ( option ); [1] The DisplayOption object is built through DisplayOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(lightId, effectUri) The id of the light and the unique identifier of the light effect must be imported during construction. Builder.setSticky(sticky) Whether the display process is allowed to be interrupted by other threads true : if it is not in operation, it will be interrupted false Builder.setLoops(loops) Number of loop display 0: unlimited 0 Builder.setExtension(extension) Customized parameters JsonObjectString.EMPTY_OBJECT If you want to display multiple lighting effects, you can specify multiple options, which can be achieved in the following two ways: DisplayOption option1 = new DisplayOption . Builder ( \"lightId001\" , Uri . parse ( \"light://effect/1\" )). setLoops ( 1 ). build (); DisplayOption option2 = new DisplayOption . Builder ( \"lightId002\" , Uri . parse ( \"light://effect/2\" )). setLoops ( 1 ). build (); promise = lightManager . displayEffectSerially ( option1 , option2 ); List < DisplayOption > options = new ArrayList <> (); options . add ( option1 ); options . add ( option2 ); promise = lightManager . displayEffectSerially ( options );","title":"Change the lighting effect"},{"location":"android/guide/system-service/light.html#list-of-lights-and-light-effects","text":"Lights and light effects can be set by constants Light color int red RED white WHITE blue BLUE green GREEN orange ORANGE yellow CYAN purple PURPLE Lighting effect int breathe BREATHE warning WARNING Flash once FLASH_ONE Flash twice FLASH_TWO Bright for 3 seconds BRIGHT_3_SECOND Bright infinitely BRIGHT_INFINITY","title":"List of lights and light effects"},{"location":"android/guide/system-service/locomotion.html","text":"Locomotion control service The locomotion control service provides functions of API calling to control the locomotion of devices. As a locomotion control service agent, LocomotionManager provides the main API of the locomotion control service, which can be obtained through the RobotContext object. LocomotionManager locomotionManager = aRobotContext . getSystemService ( LocomotionManager . SERVICE ); Cruzr's locomotion control service only provides an API interface for calling the service functions. Due to the differences between different devices, the parameters in the interface itself have no specific units. Cruzr is used as a reference for the speed unit and distance unit in the sample code and interface descriptions in this guide. Get the list of devices try { LocomotionDevice /* [1] */ locomotionDevice = locomotionManager . getDevice (); } catch ( LocomotionDeviceNotFoundException e ) { e . printStackTrace (); } [1] LocomotionDevice is the detailed configuration parameter of the mobile device, including: Attribute getter Descriptions LocomotionDevice.id Device ID LocomotionDevice.name Device name LocomotionDevice.description Device description LocomotionDevice.minTurningSpeed Minimum turning speed, 1 degree per second LocomotionDevice.maxTurningSpeed Maximum turning speed, 115 degrees per second LocomotionDevice.defaultTurningSpeed Default turning speed, 50 degrees per second LocomotionDevice.minMovingSpeed Minimum moving speed, 0.1 meters per second LocomotionDevice.maxMovingSpeed Maximum moving speed, 0.8 meters per second LocomotionDevice.defaultMovingSpeed Default moving speed, 0.4 meters per second Control the movement of the device Configure different parameters to control the device to complete the specified movement or rotation behavior. Locomotion control can be a single task or a series of tasks composed of multiple controls, which will be introduced one by one below. Control the device to move in a specified direction. To end the movement, the user has to stop it themself. Use the following code to make it move forward at the default speed. promise /* [1] */ = locomotionManager . moveStraight ( 0 ) . progress ( new ProgressCallback < LocomotionProgress > () { @Override public void onProgress ( LocomotionProgress /* [2] */ locomotionProgress ) { // There will be multiple callbacks during the movement process } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Movement completed } }) . fail ( new FailCallback < LocomotionException > () { @Override public void onFail ( LocomotionException e ) { // Movement failed } }); [1] Return the asynchronous object that performs the movement operation, fetches the result and cancels the processing through this object. See Promise for specific usage. [2] The <span id=\"LocomotionProgress\">LocomotionProgress</span> object of asynchronous callback describes the progress information of the device\u2019s movement, including: Constants Descriptions LocomotionProgress.PROGRESS_BEGAN Begin progress LocomotionProgress.PROGRESS_ENDED End progress Control the device to move at a specified speed in a specified direction. To end the movement, the user has to stop it themself. Use the following code to make it move forward at a speed of 0.5 meters per second promise = locomotionManager . moveStraight ( 0 , 0.5f ); Control the device to move a specified distance in a specified direction. Use the following code to make it move forward five meters at the default speed promise = locomotionManager . moveStraightBy ( 0 , 5 ); Control the device to move a specified distance in a specified direction at a specified speed. Use the following code to make it move forward five meters at a speed of 0.5 meters per second. promise = locomotionManager . moveStraightBy ( 0 , 5 , 0.5f ); Control the device to move a specified distance in a specified direction. Use the following code to make it move forward five meters in three seconds. promise = locomotionManager . moveStraightBy ( 0 , 5 , 3000 ); Control the device to rotate at a specified speed. To end the rotation, the user has to stop it themself. Use the following code to make it rotate at a speed of 30 degrees per second. promise = locomotionManager . turn ( 30 ); Control the device to rotate in the specified direction. To end the rotation, the user has to stop it themself. Use the following code to make it rotate counterclockwise at the default speed. promise = locomotionManager . turn ( false ); Control the device to rotate a specified angle. Use the following code to make it rotate 15 degrees at the default speed. promise = locomotionManager . turnBy ( 15 ); Control the device to rotate a specified angle at a specified speed. Use the following code to make it rotate 15 degrees at a speed of 30 degrees per second. promise = locomotionManager . turnBy ( 15 , 30 f ); Control the device to rotate a specified angle within a specified time. Use the following code to let it rotate 15 degrees in one second. promise = locomotionManager . turnBy ( 15 , 1000 ); Perform the specified action by configuring locomotion parameters. Use the following code to make it move forward five meters in five seconds and rotate 15 degrees at the default speed LocomotionOption /* [1] */ option = new LocomotionOption . Builder () . setDuration ( 5000 ) . setMovingAngle ( 0 ) . setMovingDistance ( 5 ) . setTurningAngle ( 15 ) . build (); locomotionManager . locomote ( option ); [1] The LocomotionOption object is constructed through LocomotionOption.Builder . The construction parameters are described as follows: Methods Type Descriptions Default value Builder.constructor() Construction without parameters Builder.setMovingAngle(moveAngle) float Moving angle 0 Builder.setMovingSpeed(moveSpeed) float Moving speed 0 Builder.setMovingDistance(distance) float Moving distance 0 Builder.setTurningAngle(turnAngle) float Turning angle 0 Builder.setTurningSpeed(turnSpeed) float Turning speed 0 Builder.setTurningAxis(axis) int Turning axis 0 Builder.setDuration(duration) long Execution duration (unit: milliseconds) 0 Builder.setEmergency(emergency) boolean Whether it supports emergency stop true Perform a serial locomotion control task. Use the following code to make it move five meters at a speed of 0.5 meters per second toward 15 degrees, and then rotate 20 degrees at a speed of 30 degrees per second. LocomotionOption option1 = new LocomotionOption . Builder () . setMovingSpeed ( 0.5f ) . setMovingAngle ( 15 ) . setMovingDistance ( 5 ) . build (); LocomotionOption option2 = new LocomotionOption . Builder () . setTurningSpeed ( 30 ) . setTurningAngle ( 20 ) . build (); ArrayList < LocomotionOption > optionSeries = new ArrayList <> (); optionSeries . add ( option1 ); optionSeries . add ( option2 ); locomotionManager . locomoteSerially ( optionSeries ); Query the movement status of the steering engine. If it returns true, it means that the device is in locomotion. locomotionManager . isLocomoting (); Regarding the method of locomotionManager.moveStraightBy and the parameter value of the angle, Cruzr 1S currently only supports 0 or 180, which means forward and backward respectively. Do not use other values. Monitor device movement Monitor the changes of the device's movement status LocomotionListener locomotionListener = new LocomotionListener () { @Override public void onLocomotionChanged ( LocomotionProgress /* [1] */ locomotionProgress ) { // When the locomotion status of the device changes, it will run here. } }; locomotionManager . registerListener ( locomotionListener ); [1]For the device's locomotion progress information, please refer to LocomotionProgress Cancel monitoring the changes of the device's movement status. locomotionManager . unregisterListener ( locomotionListener );","title":"Locomotion control service"},{"location":"android/guide/system-service/locomotion.html#locomotion-control-service","text":"The locomotion control service provides functions of API calling to control the locomotion of devices. As a locomotion control service agent, LocomotionManager provides the main API of the locomotion control service, which can be obtained through the RobotContext object. LocomotionManager locomotionManager = aRobotContext . getSystemService ( LocomotionManager . SERVICE ); Cruzr's locomotion control service only provides an API interface for calling the service functions. Due to the differences between different devices, the parameters in the interface itself have no specific units. Cruzr is used as a reference for the speed unit and distance unit in the sample code and interface descriptions in this guide.","title":"Locomotion control service"},{"location":"android/guide/system-service/locomotion.html#get-the-list-of-devices","text":"try { LocomotionDevice /* [1] */ locomotionDevice = locomotionManager . getDevice (); } catch ( LocomotionDeviceNotFoundException e ) { e . printStackTrace (); } [1] LocomotionDevice is the detailed configuration parameter of the mobile device, including: Attribute getter Descriptions LocomotionDevice.id Device ID LocomotionDevice.name Device name LocomotionDevice.description Device description LocomotionDevice.minTurningSpeed Minimum turning speed, 1 degree per second LocomotionDevice.maxTurningSpeed Maximum turning speed, 115 degrees per second LocomotionDevice.defaultTurningSpeed Default turning speed, 50 degrees per second LocomotionDevice.minMovingSpeed Minimum moving speed, 0.1 meters per second LocomotionDevice.maxMovingSpeed Maximum moving speed, 0.8 meters per second LocomotionDevice.defaultMovingSpeed Default moving speed, 0.4 meters per second","title":"Get the list of devices"},{"location":"android/guide/system-service/locomotion.html#control-the-movement-of-the-device","text":"Configure different parameters to control the device to complete the specified movement or rotation behavior. Locomotion control can be a single task or a series of tasks composed of multiple controls, which will be introduced one by one below. Control the device to move in a specified direction. To end the movement, the user has to stop it themself. Use the following code to make it move forward at the default speed. promise /* [1] */ = locomotionManager . moveStraight ( 0 ) . progress ( new ProgressCallback < LocomotionProgress > () { @Override public void onProgress ( LocomotionProgress /* [2] */ locomotionProgress ) { // There will be multiple callbacks during the movement process } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Movement completed } }) . fail ( new FailCallback < LocomotionException > () { @Override public void onFail ( LocomotionException e ) { // Movement failed } }); [1] Return the asynchronous object that performs the movement operation, fetches the result and cancels the processing through this object. See Promise for specific usage. [2] The <span id=\"LocomotionProgress\">LocomotionProgress</span> object of asynchronous callback describes the progress information of the device\u2019s movement, including: Constants Descriptions LocomotionProgress.PROGRESS_BEGAN Begin progress LocomotionProgress.PROGRESS_ENDED End progress Control the device to move at a specified speed in a specified direction. To end the movement, the user has to stop it themself. Use the following code to make it move forward at a speed of 0.5 meters per second promise = locomotionManager . moveStraight ( 0 , 0.5f ); Control the device to move a specified distance in a specified direction. Use the following code to make it move forward five meters at the default speed promise = locomotionManager . moveStraightBy ( 0 , 5 ); Control the device to move a specified distance in a specified direction at a specified speed. Use the following code to make it move forward five meters at a speed of 0.5 meters per second. promise = locomotionManager . moveStraightBy ( 0 , 5 , 0.5f ); Control the device to move a specified distance in a specified direction. Use the following code to make it move forward five meters in three seconds. promise = locomotionManager . moveStraightBy ( 0 , 5 , 3000 ); Control the device to rotate at a specified speed. To end the rotation, the user has to stop it themself. Use the following code to make it rotate at a speed of 30 degrees per second. promise = locomotionManager . turn ( 30 ); Control the device to rotate in the specified direction. To end the rotation, the user has to stop it themself. Use the following code to make it rotate counterclockwise at the default speed. promise = locomotionManager . turn ( false ); Control the device to rotate a specified angle. Use the following code to make it rotate 15 degrees at the default speed. promise = locomotionManager . turnBy ( 15 ); Control the device to rotate a specified angle at a specified speed. Use the following code to make it rotate 15 degrees at a speed of 30 degrees per second. promise = locomotionManager . turnBy ( 15 , 30 f ); Control the device to rotate a specified angle within a specified time. Use the following code to let it rotate 15 degrees in one second. promise = locomotionManager . turnBy ( 15 , 1000 ); Perform the specified action by configuring locomotion parameters. Use the following code to make it move forward five meters in five seconds and rotate 15 degrees at the default speed LocomotionOption /* [1] */ option = new LocomotionOption . Builder () . setDuration ( 5000 ) . setMovingAngle ( 0 ) . setMovingDistance ( 5 ) . setTurningAngle ( 15 ) . build (); locomotionManager . locomote ( option ); [1] The LocomotionOption object is constructed through LocomotionOption.Builder . The construction parameters are described as follows: Methods Type Descriptions Default value Builder.constructor() Construction without parameters Builder.setMovingAngle(moveAngle) float Moving angle 0 Builder.setMovingSpeed(moveSpeed) float Moving speed 0 Builder.setMovingDistance(distance) float Moving distance 0 Builder.setTurningAngle(turnAngle) float Turning angle 0 Builder.setTurningSpeed(turnSpeed) float Turning speed 0 Builder.setTurningAxis(axis) int Turning axis 0 Builder.setDuration(duration) long Execution duration (unit: milliseconds) 0 Builder.setEmergency(emergency) boolean Whether it supports emergency stop true Perform a serial locomotion control task. Use the following code to make it move five meters at a speed of 0.5 meters per second toward 15 degrees, and then rotate 20 degrees at a speed of 30 degrees per second. LocomotionOption option1 = new LocomotionOption . Builder () . setMovingSpeed ( 0.5f ) . setMovingAngle ( 15 ) . setMovingDistance ( 5 ) . build (); LocomotionOption option2 = new LocomotionOption . Builder () . setTurningSpeed ( 30 ) . setTurningAngle ( 20 ) . build (); ArrayList < LocomotionOption > optionSeries = new ArrayList <> (); optionSeries . add ( option1 ); optionSeries . add ( option2 ); locomotionManager . locomoteSerially ( optionSeries ); Query the movement status of the steering engine. If it returns true, it means that the device is in locomotion. locomotionManager . isLocomoting (); Regarding the method of locomotionManager.moveStraightBy and the parameter value of the angle, Cruzr 1S currently only supports 0 or 180, which means forward and backward respectively. Do not use other values.","title":"Control the movement of the device"},{"location":"android/guide/system-service/locomotion.html#monitor-device-movement","text":"Monitor the changes of the device's movement status LocomotionListener locomotionListener = new LocomotionListener () { @Override public void onLocomotionChanged ( LocomotionProgress /* [1] */ locomotionProgress ) { // When the locomotion status of the device changes, it will run here. } }; locomotionManager . registerListener ( locomotionListener ); [1]For the device's locomotion progress information, please refer to LocomotionProgress Cancel monitoring the changes of the device's movement status. locomotionManager . unregisterListener ( locomotionListener );","title":"Monitor device movement"},{"location":"android/guide/system-service/motion.html","text":"Motion control The motion control service provides functions of the API's calling to control the device to perform various motions. As the motion control service agent, MotionManager provides the main API of the motion control service, which can be obtained through the RobotContext object. MotionManager motionManager = aRobotContext . getSystemService ( MotionManager . SERVICE ); Get the status of current motion. Normally, the device only has two statuses, one is the reset status (RESET) and the other is the performing status (PERFORMING_ACTION) Posture /* [1] */ posture = motionManager . getPosture (); [1] Posture parameter description Attribute getter Descriptions Posture.name Motion name Posture.description Motion description The following constants are only used to compare the current status of the device. Constants Descriptions Posture.RESET Reset status Posture.PERFORMING_ACTION Perform motion status Control device's motion Under normal circumstances, some devices will create a series of preset actions tailored for themselves, such as applause, handshake, salute, etc. Through motion control services, users can directly call these preset motions. When using it you can choose to perform one or more motions serially. The usage methods will be introduced one by one below. Perform a specific motion using Uri. Use the following code to make the device perform a \u201chug\u201d motion. Uri actionUri = Uri . parse ( \"action://ubtrobot/hug\" ); /* [2] */ promise /* [1] */ = motionManager . performAction ( actionUri ) . progress ( new ProgressCallback < PerformingProgress > () { @Override public void onProgress ( PerformingProgress /* [3] */ performingProgress ) { // There will be multiple callbacks during the performance of the motion } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Motion performance completed } }) . fail ( new FailCallback < PerformingException > () { @Override public void onFail ( PerformingException e ) { // Motion performance failed } }); [1]Return the asynchronous object that performs the movement operation, fetches the result and cancels the processing through this object. See Promise for specific usage. [2]Uri of the motion [3] The PerformingProgress object of theasynchronous callback describes the progress information of the device movement, including: Constants Descriptions PerformingProgress.PROGRESS_BEGAN Start of motion execution PerformingProgress.PROGRESS_ENDED End of motion execution Perform a specific motion in accordance with the parameters. Use the following code to make it execute the \u201cgoodbye\u201d motion three times at an interval of three seconds. Uri uri = Uri . parse ( \"action://ubtrobot/goodbye\" ); PerformingOption /* [1] */ option = new PerformingOption . Builder ( uri ) . setLoops ( 3 ) . setOffsetTime ( 3000 ) . build (); promise = motionManager . performAction ( option ); [1] The PerformingOption object is constructed by PerformingOption.Builder , the construction parameters are described as follows: Methods Type Descriptions Default value Builder.constructor(Uri) Uri Uri of the motion NULL Builder.setLoops(loop) int Number of repetitions 1 Builder.setOffsetTime(offsetTime) float Interval of execution 0 Builder.setSpeed(speed) float Speed of execution 0 Perform multiple motions serially Use the following two methods to perform this action. First perform a \"hug\" motion, and then take three seconds as the interval. Uri hugUri = Uri . parse ( \"action://ubtrobot/goodbye\" ); PerformingOption option1 = new PerformingOption . Builder ( hugUri ) . setLoops ( 1 ) . build (); Uri goodbyeUri = Uri . parse ( \"action://ubtrobot/goodbye\" ); PerformingOption option2 = new PerformingOption . Builder ( goodbyeUri ) . setLoops ( 3 ) . setOffsetTime ( 3000 ) . build (); // Method one promise = motionManager . performActionSerially ( option1 , option2 ); // Method two ArrayList < PerformingOption > optionList = new ArrayList <> (); optionList . add ( option1 ); optionList . add ( option2 ); promise = motionManager . performActionSerially ( optionList ); Determine if the device is currently performing the motion motionManager . isPerformingAction (); List of motions Motion name ID applause applause celebrate celebrate goodbye goodbye nod nod hug hug shake hand shankhand Go right guideright Go left guideleft Swing arm swingarm Search around searching Gaze tiaowang surprise surprise shy shy Grow tall zhanggao daze fadai Chit-chat Talk1-talk24 talk1 talk2 talk3 talk4 talk5 talk6 talk7 talk8 talk9 talk10 talk11 talk12 talk13 talk14 talk15 talk16 talk17 talk18 talk19 talk20 talk21 talk22 talk23 talk24","title":"Motion control"},{"location":"android/guide/system-service/motion.html#motion-control","text":"The motion control service provides functions of the API's calling to control the device to perform various motions. As the motion control service agent, MotionManager provides the main API of the motion control service, which can be obtained through the RobotContext object. MotionManager motionManager = aRobotContext . getSystemService ( MotionManager . SERVICE );","title":"Motion control"},{"location":"android/guide/system-service/motion.html#get-the-status-of-current-motion","text":"Normally, the device only has two statuses, one is the reset status (RESET) and the other is the performing status (PERFORMING_ACTION) Posture /* [1] */ posture = motionManager . getPosture (); [1] Posture parameter description Attribute getter Descriptions Posture.name Motion name Posture.description Motion description The following constants are only used to compare the current status of the device. Constants Descriptions Posture.RESET Reset status Posture.PERFORMING_ACTION Perform motion status","title":"Get the status of current motion."},{"location":"android/guide/system-service/motion.html#control-devices-motion","text":"Under normal circumstances, some devices will create a series of preset actions tailored for themselves, such as applause, handshake, salute, etc. Through motion control services, users can directly call these preset motions. When using it you can choose to perform one or more motions serially. The usage methods will be introduced one by one below. Perform a specific motion using Uri. Use the following code to make the device perform a \u201chug\u201d motion. Uri actionUri = Uri . parse ( \"action://ubtrobot/hug\" ); /* [2] */ promise /* [1] */ = motionManager . performAction ( actionUri ) . progress ( new ProgressCallback < PerformingProgress > () { @Override public void onProgress ( PerformingProgress /* [3] */ performingProgress ) { // There will be multiple callbacks during the performance of the motion } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Motion performance completed } }) . fail ( new FailCallback < PerformingException > () { @Override public void onFail ( PerformingException e ) { // Motion performance failed } }); [1]Return the asynchronous object that performs the movement operation, fetches the result and cancels the processing through this object. See Promise for specific usage. [2]Uri of the motion [3] The PerformingProgress object of theasynchronous callback describes the progress information of the device movement, including: Constants Descriptions PerformingProgress.PROGRESS_BEGAN Start of motion execution PerformingProgress.PROGRESS_ENDED End of motion execution Perform a specific motion in accordance with the parameters. Use the following code to make it execute the \u201cgoodbye\u201d motion three times at an interval of three seconds. Uri uri = Uri . parse ( \"action://ubtrobot/goodbye\" ); PerformingOption /* [1] */ option = new PerformingOption . Builder ( uri ) . setLoops ( 3 ) . setOffsetTime ( 3000 ) . build (); promise = motionManager . performAction ( option ); [1] The PerformingOption object is constructed by PerformingOption.Builder , the construction parameters are described as follows: Methods Type Descriptions Default value Builder.constructor(Uri) Uri Uri of the motion NULL Builder.setLoops(loop) int Number of repetitions 1 Builder.setOffsetTime(offsetTime) float Interval of execution 0 Builder.setSpeed(speed) float Speed of execution 0 Perform multiple motions serially Use the following two methods to perform this action. First perform a \"hug\" motion, and then take three seconds as the interval. Uri hugUri = Uri . parse ( \"action://ubtrobot/goodbye\" ); PerformingOption option1 = new PerformingOption . Builder ( hugUri ) . setLoops ( 1 ) . build (); Uri goodbyeUri = Uri . parse ( \"action://ubtrobot/goodbye\" ); PerformingOption option2 = new PerformingOption . Builder ( goodbyeUri ) . setLoops ( 3 ) . setOffsetTime ( 3000 ) . build (); // Method one promise = motionManager . performActionSerially ( option1 , option2 ); // Method two ArrayList < PerformingOption > optionList = new ArrayList <> (); optionList . add ( option1 ); optionList . add ( option2 ); promise = motionManager . performActionSerially ( optionList ); Determine if the device is currently performing the motion motionManager . isPerformingAction ();","title":"Control device's motion"},{"location":"android/guide/system-service/motion.html#list-of-motions","text":"Motion name ID applause applause celebrate celebrate goodbye goodbye nod nod hug hug shake hand shankhand Go right guideright Go left guideleft Swing arm swingarm Search around searching Gaze tiaowang surprise surprise shy shy Grow tall zhanggao daze fadai Chit-chat Talk1-talk24 talk1 talk2 talk3 talk4 talk5 talk6 talk7 talk8 talk9 talk10 talk11 talk12 talk13 talk14 talk15 talk16 talk17 talk18 talk19 talk20 talk21 talk22 talk23 talk24","title":"List of motions"},{"location":"android/guide/system-service/navigation.html","text":"Navigation service The navigation service provides the robot with the ability to move from its current position to its destination. NavigationManager provides corresponding APIs for map management, positioning, and movement. NavigationManager can be obtained through RobotContext . NavigationManager navigationManager = robotContext . getSystemService ( NavigationManager . SERVICE ); Map library The map data file is generated by a specific map scanning machine, which specifies the map's initial direction, map origin, distance unit, marker point, and tour route, etc. (This data is set when the map is generated and cannot be modified). After the map data file is obtained, the data above needs to be imported into the map library. Create a map The map consists of marker points, overlays, trajectories, and map data, which need to be added when creating a map. //Create a marker point Marker marker /* [1] */ = new Marker . Builder ( point /* [2] */ ). build (); //Create a overlay GroundOverlay groundOverlay /* [3] */ = new GroundOverlay . Builder ( width , height ). build (); //Create a route Polyline polyline /* [4] */ = new Polyline . Builder ( id ). build (); //Create a map NavMap navMap /* [5] */ = new NavMap . Builder ( scale ). build (); [1] Marker is used to describe each coordinate point, and is constructed by Marker.Builder . The instructions are as follows: Methods Descriptions Builder.constructor(point) The construction method is to set the coordinates of the marker point. Builder.setId(id) Set ID of the coordinate, which is the unique sign. Builder.setTitle(title) Set a title Builder.addTag(tag) Add a tag Builder.addTag(index,tag) Add a tag at the specified location Builder.addTagList(taglist) Add a tag list Builder.removeTag(index) Remove the tag at a specified location Builder.setDescription(description) Set description Builder.setExtension(extension) Set extended data, the content can be customized [2] Point, the coordinate is the position relative to the map origin (the unit of distance is the same as the map), and the origin is the starting point of the scan. Type Attributes Descriptions float x x-axis coordinates float y y-axis coordinates [3] GroundOverlay is overlaid on the map when displayed, it is built by GroundOverlay.Builder . The instructions are as follows: Methods Descriptions Builder.constructor(width, height) Construction method, width and height Builder.setName(name) Set the name of the overlay Builder.setType(type) Set the type of overlay Builder.setOriginInImage(originInImage) Set the origin coordinates of the overlay Builder.setImage(image) Set the URI of the local image Builder.setImageUrl(imageUrl) Set the URL of the online image [4] Polyline is composed of multiple [Location] (# Location) components and is built by Polyline.Builder . The instructions are as follows: Methods Descriptions Builder.constructor(id) Construction method, add a route id, which is the only sign Builder.setName(name) Set the name Builder.setDescription(description) Set description Builder.addLocation(location) Add a list of coordinate points included in the route Builder.addLocation(index,location) Add coordinates at the specified position Builder.addLocationList(locationList) Add a list of coordinates Builder.removeLocation(index) Remove the coordinates of the specified position Builder.setExtension(extension) Set extended data, the content can be customized [5] NavMap is built by NavMap.Builder , the instructions are as follows: Methods Descriptions Builder.constructor(scale) You can zoom on the map when building it. Builder.setId(id) Set map ID Builder.setName(name) Set map name Builder.setNavFile(navFileUri) Set the path of the map data file Builder.addGroundOverlay(groundOverlay) Add an overlay Builder.addGroundOverlay(index, groundOverlay) Add an overlay at a specified position in the list Builder.addGroundOverlayList(groundOverlayList) Add an overlay list Builder.removeGroundOverlay(index) Remove the overlay at the specified position in the list Builder.addMarker(marker) Add a marker Builder.addMarker(index, marker) Mark at the specified position in the list Builder.addMarkerList(markerList) Add a marker list Builder.removeMarker(index) Remove the marker at the specified position of the list Builder.addPolyline(polyline) Add navigation route Builder.addPolyline(index, polyline) Add a navigation route at a specified position in the list Builder.addPolylineList(polylineList) Add navigation route list Builder.removePolyline(index) Remove the navigation route at the specified position in the list Add a map Promise < NavMap , NavMapException > addNavMapPromise /* [1] */ = navigationManager . addNavMap ( navMap ); /* [2] */ [1]The method that the returned result is Promise is an asynchronous method. You can obtain the execution result by registering a callback, or you can execute the get () method to convert it to a synchronous method. For details, see async . [2]Adding a map may take a long time, you need to be careful when using the synchronization method. Get the map Promise < NavMap , NavMapException > getNavMapPromise = navigationManager . getNavMap ( navMapId ); Promise < List < NavMap > , NavMapException > getNavMapListPromise = navigationManager . getNavMapList (); Edit the map Promise < NavMap , NavMapException > modifyNavMapPromise = navigationManager . modifyNavMap ( navMap ); Remove the map Promise < NavMap , NavMapException > removeNavMapPromise = navigationManager . removeNavMap ( navMapId ); Current map You need to set a map before using position and navigation. Get the current map Promise < NavMap , NavMapException > currentNavMapPromise = navigationManager . getCurrentNavMap (); Set the current map Promise < NavMap , NavMapException > setCurrentNavMapPromise = navigationManager . setCurrentNavMap ( navMapId ); Remove the current map Promise < NavMap , NavMapException > unsetCurrentNavMapPromise = navigationManager . unsetCurrentNavMap (); Positioning The robot needs to determine its position on the map before navigation. Query the positioning status //If the positioning is correct boolean locatingSelf = navigationManager . isLocatingSelf (); //If the positioning is successful boolean selfLocated = navigationManager . isSelfLocated (); Positioning //Default position ProgressivePromise < Location /* [1] */ , LocatingException , LocatingProgress > locateSelfPromise = navigationManager . locateSelf (); //Use parameter for positioning LocatingOption locatingOption /* [2] */ = new LocatingOption . Builder () . setNearby ( location ) . setTimeout ( timeout ) . build (); ProgressivePromise < Location , LocatingException , LocatingProgress > locateSelfPromise = navigationManager . locateSelf ( locatingOption ); //Cancel the positioning locateSelfPromise . cancel (); [1] Location location data Attribute getter Descriptions position x-axis and y-axis coordinates are equal to the map origin z z-axis coordinates rotation Orientation angle, the range is (0 ~ 360), the angle is equal to the initial angle of the map. [2] LocationOption is built using LocationOption.Builder . The instructions are as follows: Methods Descriptions Default value Buidler.setTimeout(timeout) Timeout unit: ms -1 Buidler.setNearby(useNearby) Positioning near a specified coordinate It will not be used by default Get the current location Location currentLocation = navigationManager . getCurrentLocation (); Navigation Query navigation status boolean navigating = navigationManager . isNavigating (); Start the navigation NavigationOption navigationOption /* [1] */ = new NavigationOption . Builder ( location ). build (); ProgressivePromise < Void , NavigationException , NavigationProgress /* [2] */ > navigatePromise = navigationManager . navigate ( navigationOption ); [1] NavigationOption is built using NavigationOption.Builder . The instructions are as follows: Attributes Description Builder.constructor(location) Construction method, set the destination of navigation Builder.setMaxSpeed(maxSpeed) Set the maximum speed, m/s Builder.setRetryCount(retryCount) Set the number of retries Builder.setRetryInterval(retryInterval) Set the interval of retries, ms Builder.setTrackMode(trackMode) Set whether to navigate along the track (the track information is included in the map) [2] NavigationProgress gets the current location via getLocation() Stop the navigation navigatePromise . cancel (); Monitor the location Set location monitor LocationListener locationListener = new LocationListener () { @Override public void onLocationChanged ( Location location ) { //Change of location } }; navigationManager . registerListener ( locationListener ); Cancel the location monitoring navigationManager . unregisterListener ( locationListener );","title":"Navigation service"},{"location":"android/guide/system-service/navigation.html#navigation-service","text":"The navigation service provides the robot with the ability to move from its current position to its destination. NavigationManager provides corresponding APIs for map management, positioning, and movement. NavigationManager can be obtained through RobotContext . NavigationManager navigationManager = robotContext . getSystemService ( NavigationManager . SERVICE );","title":"Navigation service"},{"location":"android/guide/system-service/navigation.html#map-library","text":"The map data file is generated by a specific map scanning machine, which specifies the map's initial direction, map origin, distance unit, marker point, and tour route, etc. (This data is set when the map is generated and cannot be modified). After the map data file is obtained, the data above needs to be imported into the map library. Create a map The map consists of marker points, overlays, trajectories, and map data, which need to be added when creating a map. //Create a marker point Marker marker /* [1] */ = new Marker . Builder ( point /* [2] */ ). build (); //Create a overlay GroundOverlay groundOverlay /* [3] */ = new GroundOverlay . Builder ( width , height ). build (); //Create a route Polyline polyline /* [4] */ = new Polyline . Builder ( id ). build (); //Create a map NavMap navMap /* [5] */ = new NavMap . Builder ( scale ). build (); [1] Marker is used to describe each coordinate point, and is constructed by Marker.Builder . The instructions are as follows: Methods Descriptions Builder.constructor(point) The construction method is to set the coordinates of the marker point. Builder.setId(id) Set ID of the coordinate, which is the unique sign. Builder.setTitle(title) Set a title Builder.addTag(tag) Add a tag Builder.addTag(index,tag) Add a tag at the specified location Builder.addTagList(taglist) Add a tag list Builder.removeTag(index) Remove the tag at a specified location Builder.setDescription(description) Set description Builder.setExtension(extension) Set extended data, the content can be customized [2] Point, the coordinate is the position relative to the map origin (the unit of distance is the same as the map), and the origin is the starting point of the scan. Type Attributes Descriptions float x x-axis coordinates float y y-axis coordinates [3] GroundOverlay is overlaid on the map when displayed, it is built by GroundOverlay.Builder . The instructions are as follows: Methods Descriptions Builder.constructor(width, height) Construction method, width and height Builder.setName(name) Set the name of the overlay Builder.setType(type) Set the type of overlay Builder.setOriginInImage(originInImage) Set the origin coordinates of the overlay Builder.setImage(image) Set the URI of the local image Builder.setImageUrl(imageUrl) Set the URL of the online image [4] Polyline is composed of multiple [Location] (# Location) components and is built by Polyline.Builder . The instructions are as follows: Methods Descriptions Builder.constructor(id) Construction method, add a route id, which is the only sign Builder.setName(name) Set the name Builder.setDescription(description) Set description Builder.addLocation(location) Add a list of coordinate points included in the route Builder.addLocation(index,location) Add coordinates at the specified position Builder.addLocationList(locationList) Add a list of coordinates Builder.removeLocation(index) Remove the coordinates of the specified position Builder.setExtension(extension) Set extended data, the content can be customized [5] NavMap is built by NavMap.Builder , the instructions are as follows: Methods Descriptions Builder.constructor(scale) You can zoom on the map when building it. Builder.setId(id) Set map ID Builder.setName(name) Set map name Builder.setNavFile(navFileUri) Set the path of the map data file Builder.addGroundOverlay(groundOverlay) Add an overlay Builder.addGroundOverlay(index, groundOverlay) Add an overlay at a specified position in the list Builder.addGroundOverlayList(groundOverlayList) Add an overlay list Builder.removeGroundOverlay(index) Remove the overlay at the specified position in the list Builder.addMarker(marker) Add a marker Builder.addMarker(index, marker) Mark at the specified position in the list Builder.addMarkerList(markerList) Add a marker list Builder.removeMarker(index) Remove the marker at the specified position of the list Builder.addPolyline(polyline) Add navigation route Builder.addPolyline(index, polyline) Add a navigation route at a specified position in the list Builder.addPolylineList(polylineList) Add navigation route list Builder.removePolyline(index) Remove the navigation route at the specified position in the list Add a map Promise < NavMap , NavMapException > addNavMapPromise /* [1] */ = navigationManager . addNavMap ( navMap ); /* [2] */ [1]The method that the returned result is Promise is an asynchronous method. You can obtain the execution result by registering a callback, or you can execute the get () method to convert it to a synchronous method. For details, see async . [2]Adding a map may take a long time, you need to be careful when using the synchronization method. Get the map Promise < NavMap , NavMapException > getNavMapPromise = navigationManager . getNavMap ( navMapId ); Promise < List < NavMap > , NavMapException > getNavMapListPromise = navigationManager . getNavMapList (); Edit the map Promise < NavMap , NavMapException > modifyNavMapPromise = navigationManager . modifyNavMap ( navMap ); Remove the map Promise < NavMap , NavMapException > removeNavMapPromise = navigationManager . removeNavMap ( navMapId );","title":"Map library"},{"location":"android/guide/system-service/navigation.html#current-map","text":"You need to set a map before using position and navigation. Get the current map Promise < NavMap , NavMapException > currentNavMapPromise = navigationManager . getCurrentNavMap (); Set the current map Promise < NavMap , NavMapException > setCurrentNavMapPromise = navigationManager . setCurrentNavMap ( navMapId ); Remove the current map Promise < NavMap , NavMapException > unsetCurrentNavMapPromise = navigationManager . unsetCurrentNavMap ();","title":"Current map"},{"location":"android/guide/system-service/navigation.html#positioning","text":"The robot needs to determine its position on the map before navigation. Query the positioning status //If the positioning is correct boolean locatingSelf = navigationManager . isLocatingSelf (); //If the positioning is successful boolean selfLocated = navigationManager . isSelfLocated (); Positioning //Default position ProgressivePromise < Location /* [1] */ , LocatingException , LocatingProgress > locateSelfPromise = navigationManager . locateSelf (); //Use parameter for positioning LocatingOption locatingOption /* [2] */ = new LocatingOption . Builder () . setNearby ( location ) . setTimeout ( timeout ) . build (); ProgressivePromise < Location , LocatingException , LocatingProgress > locateSelfPromise = navigationManager . locateSelf ( locatingOption ); //Cancel the positioning locateSelfPromise . cancel (); [1] Location location data Attribute getter Descriptions position x-axis and y-axis coordinates are equal to the map origin z z-axis coordinates rotation Orientation angle, the range is (0 ~ 360), the angle is equal to the initial angle of the map. [2] LocationOption is built using LocationOption.Builder . The instructions are as follows: Methods Descriptions Default value Buidler.setTimeout(timeout) Timeout unit: ms -1 Buidler.setNearby(useNearby) Positioning near a specified coordinate It will not be used by default Get the current location Location currentLocation = navigationManager . getCurrentLocation ();","title":"Positioning"},{"location":"android/guide/system-service/navigation.html#navigation","text":"Query navigation status boolean navigating = navigationManager . isNavigating (); Start the navigation NavigationOption navigationOption /* [1] */ = new NavigationOption . Builder ( location ). build (); ProgressivePromise < Void , NavigationException , NavigationProgress /* [2] */ > navigatePromise = navigationManager . navigate ( navigationOption ); [1] NavigationOption is built using NavigationOption.Builder . The instructions are as follows: Attributes Description Builder.constructor(location) Construction method, set the destination of navigation Builder.setMaxSpeed(maxSpeed) Set the maximum speed, m/s Builder.setRetryCount(retryCount) Set the number of retries Builder.setRetryInterval(retryInterval) Set the interval of retries, ms Builder.setTrackMode(trackMode) Set whether to navigate along the track (the track information is included in the map) [2] NavigationProgress gets the current location via getLocation() Stop the navigation navigatePromise . cancel ();","title":"Navigation"},{"location":"android/guide/system-service/navigation.html#monitor-the-location","text":"Set location monitor LocationListener locationListener = new LocationListener () { @Override public void onLocationChanged ( Location location ) { //Change of location } }; navigationManager . registerListener ( locationListener ); Cancel the location monitoring navigationManager . unregisterListener ( locationListener );","title":"Monitor the location"},{"location":"android/guide/system-service/orchestration.html","text":"Dance service The dance service provides the ability to make the API call the device to dance. As the dance service access agent, OrchestrationManager provides the main API of the dance service, which can be obtained through RobotContext . OrchestrationManager orchestrationManager = aRobotContext . getSystemService ( OrchestrationManager . SERVICE ); dance When you need the device to dance like a human, you can achieve it with the following code: promise /* [2] */ = orchestrationManager . play ( Uri . parse ( \"orchestration://id/Naxi\" ) /* [1] */ ) . progress ( new ProgressCallback < PlayProgress > () { @Override public void onProgress ( PlayProgress playProgress /* [3] */ ) { // Callback the dance process } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback the dance completed } }) . fail ( new FailCallback < PlayException > () { @Override public void onFail ( PlayException e ) { // Callback the dance goes wrong } }); [1]The unique identification of the dance, see the appendix for details. [2]Return the asynchronous object that is waiting for the progress and result of the dance, through which you can wait for or monitor the progress and results, and cancel the dance. See Promise for specific usage. [3] The PlayProgress object of the asynchronous callback describes the progress information of the dance, including: Attribute getter Descriptions PlayProgress.progress Progress information began : dance started ended : dance ended playing : dancing By specifying options, the dancing action can be adjusted using the following code: PlayOption /* [1] */ playOption = new PlayOption . Builder ( new Orchestration . Builder (). build () /* [2] */ ). setOffsetTime ( 5000 ). build (); promise = orchestrationManager . play ( playOption ); [1] The PlayOption object is constructed through PlayOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(orchestration) During construction, the arranged object Orchestration need to be introduced. Builder.setOffsetTime(5000) The length of time the dance has lasted, unit: millisecond 0 [2] The Orchestration object is built using Orchestration.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor() Builder.addTrack(track) Add tracks, to create an object, see Track Builder.addTrackList(tracks) Add track collection Builder.setMainTrackIndex(0) Specify the main track 0 The Track object is built using Track.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor(type, segment) When constructing, specify the track type and fragment . Currently, the track types supported are \"motion\", \"locomotion\", \"audio\", and \"emotion\". Builder.setRootSegment(rootSegment) Specify the segment, to create an object, see Segment Builder.setDescription(description) Description Zero-length string The Segment object is built using Segment.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor() Builder.setName(name) Name of the segment Zero-length string Builder.setDescription(description) Description of the segment Zero-length string Builder.setLoops(loops) Number of loops, 0 : unlimited 0 Builder.setDuration(duration) Total execution time: unit: millisecond 0 Builder.setDurationLoopOnce(durationLoopOnce) Time required for one execution, unit: millisecond 0 Builder.setBlank(blank) wait, true: wait false Builder.setOption(option) Genericity, set the information unique to each track. For details of \"motion\", please see PerformingOption . For details of \"locomotion\", please see LocomotionOption . For details of \" emotion\", please see ExpressingOption If you want to build a collection of fragments, you can use the SegmentGroup object, which inherits Segment and is built through SegmentGroup.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor() Builder.addChildren(childrenSegment) Add a segment Builder.addChildren(childSegmentGroup) Add multiple segments Builder.addChildFirst(childrenSegment) Add a segment to the beginning Builder.removeChild(childSegment) Remove a segment Dance list Dance name English name ID seaweed Seaweed Seaweed Dura Dura Dura Faded Faded Faded Panama Panama Panama Curry Curry Curry Ievan Polkka IevanPolkka IevanPolkka Crayon Crayon Crayon Toca Toca TocaToca TocaToca Modern dance Rock ModernDance Flamenco Spain Flamenco Boom Boom BoomBoom BBoomBBoom Naxi Naxi Naxi Appendix public static final Uri NAXI = Uri . parse ( \"orchestration://id/Naxi\" ); public static final Uri BBOOM_BBOOM = Uri . parse ( \"orchestration://id/BBoomBBoom\" ); public static final Uri FLAMENCO = Uri . parse ( \"orchestration://id/Flamenco\" ); public static final Uri MODERN = Uri . parse ( \"orchestration://id/Modern Dance\" ); public static final Uri TOCA_TOCA = Uri . parse ( \"orchestration://id/TocaToca\" ); public static final Uri CRAYON = Uri . parse ( \"orchestration://id/Crayon\" ); public static final Uri SEAWEED = Uri . parse ( \"orchestration://id/Seaweed\" ); public static final Uri CURRY = Uri . parse ( \"orchestration://id/Curry\" ); public static final Uri IEVAN_POLKKA = Uri . parse ( \"orchestration://id/IevanPolkka\" ); public static final Uri PANAMA = Uri . parse ( \"orchestration://id/Panama\" ); public static final Uri FADED = Uri . parse ( \"orchestration://id/Faded\" ); public static final Uri DURA = Uri . parse ( \"orchestration://id/Dura\" );","title":"Dance service"},{"location":"android/guide/system-service/orchestration.html#dance-service","text":"The dance service provides the ability to make the API call the device to dance. As the dance service access agent, OrchestrationManager provides the main API of the dance service, which can be obtained through RobotContext . OrchestrationManager orchestrationManager = aRobotContext . getSystemService ( OrchestrationManager . SERVICE );","title":"Dance service"},{"location":"android/guide/system-service/orchestration.html#dance","text":"When you need the device to dance like a human, you can achieve it with the following code: promise /* [2] */ = orchestrationManager . play ( Uri . parse ( \"orchestration://id/Naxi\" ) /* [1] */ ) . progress ( new ProgressCallback < PlayProgress > () { @Override public void onProgress ( PlayProgress playProgress /* [3] */ ) { // Callback the dance process } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback the dance completed } }) . fail ( new FailCallback < PlayException > () { @Override public void onFail ( PlayException e ) { // Callback the dance goes wrong } }); [1]The unique identification of the dance, see the appendix for details. [2]Return the asynchronous object that is waiting for the progress and result of the dance, through which you can wait for or monitor the progress and results, and cancel the dance. See Promise for specific usage. [3] The PlayProgress object of the asynchronous callback describes the progress information of the dance, including: Attribute getter Descriptions PlayProgress.progress Progress information began : dance started ended : dance ended playing : dancing By specifying options, the dancing action can be adjusted using the following code: PlayOption /* [1] */ playOption = new PlayOption . Builder ( new Orchestration . Builder (). build () /* [2] */ ). setOffsetTime ( 5000 ). build (); promise = orchestrationManager . play ( playOption ); [1] The PlayOption object is constructed through PlayOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(orchestration) During construction, the arranged object Orchestration need to be introduced. Builder.setOffsetTime(5000) The length of time the dance has lasted, unit: millisecond 0 [2] The Orchestration object is built using Orchestration.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor() Builder.addTrack(track) Add tracks, to create an object, see Track Builder.addTrackList(tracks) Add track collection Builder.setMainTrackIndex(0) Specify the main track 0 The Track object is built using Track.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor(type, segment) When constructing, specify the track type and fragment . Currently, the track types supported are \"motion\", \"locomotion\", \"audio\", and \"emotion\". Builder.setRootSegment(rootSegment) Specify the segment, to create an object, see Segment Builder.setDescription(description) Description Zero-length string The Segment object is built using Segment.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor() Builder.setName(name) Name of the segment Zero-length string Builder.setDescription(description) Description of the segment Zero-length string Builder.setLoops(loops) Number of loops, 0 : unlimited 0 Builder.setDuration(duration) Total execution time: unit: millisecond 0 Builder.setDurationLoopOnce(durationLoopOnce) Time required for one execution, unit: millisecond 0 Builder.setBlank(blank) wait, true: wait false Builder.setOption(option) Genericity, set the information unique to each track. For details of \"motion\", please see PerformingOption . For details of \"locomotion\", please see LocomotionOption . For details of \" emotion\", please see ExpressingOption If you want to build a collection of fragments, you can use the SegmentGroup object, which inherits Segment and is built through SegmentGroup.Builder . The instructions are as follows: Methods Descriptions Default value Builder.constructor() Builder.addChildren(childrenSegment) Add a segment Builder.addChildren(childSegmentGroup) Add multiple segments Builder.addChildFirst(childrenSegment) Add a segment to the beginning Builder.removeChild(childSegment) Remove a segment","title":"dance"},{"location":"android/guide/system-service/orchestration.html#dance-list","text":"Dance name English name ID seaweed Seaweed Seaweed Dura Dura Dura Faded Faded Faded Panama Panama Panama Curry Curry Curry Ievan Polkka IevanPolkka IevanPolkka Crayon Crayon Crayon Toca Toca TocaToca TocaToca Modern dance Rock ModernDance Flamenco Spain Flamenco Boom Boom BoomBoom BBoomBBoom Naxi Naxi Naxi","title":"Dance list"},{"location":"android/guide/system-service/orchestration.html#appendix","text":"public static final Uri NAXI = Uri . parse ( \"orchestration://id/Naxi\" ); public static final Uri BBOOM_BBOOM = Uri . parse ( \"orchestration://id/BBoomBBoom\" ); public static final Uri FLAMENCO = Uri . parse ( \"orchestration://id/Flamenco\" ); public static final Uri MODERN = Uri . parse ( \"orchestration://id/Modern Dance\" ); public static final Uri TOCA_TOCA = Uri . parse ( \"orchestration://id/TocaToca\" ); public static final Uri CRAYON = Uri . parse ( \"orchestration://id/Crayon\" ); public static final Uri SEAWEED = Uri . parse ( \"orchestration://id/Seaweed\" ); public static final Uri CURRY = Uri . parse ( \"orchestration://id/Curry\" ); public static final Uri IEVAN_POLKKA = Uri . parse ( \"orchestration://id/IevanPolkka\" ); public static final Uri PANAMA = Uri . parse ( \"orchestration://id/Panama\" ); public static final Uri FADED = Uri . parse ( \"orchestration://id/Faded\" ); public static final Uri DURA = Uri . parse ( \"orchestration://id/Dura\" );","title":"Appendix"},{"location":"android/guide/system-service/overview.html","text":"System service System service is a program where Cruzr implements the basic functions and provides APIs for robotic applications. Divided by hardware components or software functions, Cruzr provides various system services. Robot applications can call the APIs of these system services to implement their complex functions. Here are all the system services Cruzr currently offers: Service name Service function SpeechManager Speech service wake up, speech recognition, text-to-speech, and natural language processing ServoManager Obtain servo equipment and control its rotation, etc. MotionManager Perform motions and get poses LocomotionManager Turn, move in straight lines or around corners EmotionManager Use animation to express the robot's emotions LightManager Obtain lighting equipment, control the switch on lighting equipment, change color, play lighting effects OrchestrationManager Use locomotion, movement, lights, emotions, etc. to choreograph dances and make the robot dance SensorManager Obtain sensor device data and listen sensor environmental data NavigationManager Manage maps, positioning and navigation DiagnosisManager Diagnose and listen faults of various components PowerManager Listen power on and off, sleep and wake up, power and battery status RechargingManager Automatic charging These system services can be obtained in the following ways: FooManager fooManager = robotContext . getSystemService ( FooManager . SERVICE ); Take the speech service as an example: SpeechManager speechManager = robotContext . getSystemService ( SpeechManager . SERVICE );","title":"System-service"},{"location":"android/guide/system-service/overview.html#system-service","text":"System service is a program where Cruzr implements the basic functions and provides APIs for robotic applications. Divided by hardware components or software functions, Cruzr provides various system services. Robot applications can call the APIs of these system services to implement their complex functions. Here are all the system services Cruzr currently offers: Service name Service function SpeechManager Speech service wake up, speech recognition, text-to-speech, and natural language processing ServoManager Obtain servo equipment and control its rotation, etc. MotionManager Perform motions and get poses LocomotionManager Turn, move in straight lines or around corners EmotionManager Use animation to express the robot's emotions LightManager Obtain lighting equipment, control the switch on lighting equipment, change color, play lighting effects OrchestrationManager Use locomotion, movement, lights, emotions, etc. to choreograph dances and make the robot dance SensorManager Obtain sensor device data and listen sensor environmental data NavigationManager Manage maps, positioning and navigation DiagnosisManager Diagnose and listen faults of various components PowerManager Listen power on and off, sleep and wake up, power and battery status RechargingManager Automatic charging These system services can be obtained in the following ways: FooManager fooManager = robotContext . getSystemService ( FooManager . SERVICE ); Take the speech service as an example: SpeechManager speechManager = robotContext . getSystemService ( SpeechManager . SERVICE );","title":"System service"},{"location":"android/guide/system-service/power.html","text":"Power service The power service provides the function of robot power management. Through PowerManager , you can control the robot to sleep, wake up, query power status, shutdown and make it turn on at a set time. Get PowerManager through the [ RobotContext[ object. PowerManager powerManager = robotContext . getSystemService ( PowerManager . SERVICE ); Turn on at a set time. Set a time to turn on java Promise<Void, PowerException> scheduleStartupPromise = powerManager.scheduleStartup(waitSeconds);/* [1] */ [1]The method that returns Promise is an asynchronous method. You can obtain the execution result by registering a callback, or you can execute the get () method to convert it to a synchronous method. For details, see async . Cancel the scheduled turn on java Promise<Void, PowerException> cancelStartupSchedulePromise = powerManager.cancelStartupSchedule(); Turn off When not in use, the robot can be actively turned off. Promise < Void , PowerException > shutdownPromise = powerManager . shutdown (); Sleep and wake up sleep In sleep mode, power consumption will be reduced. java Promise<Void, PowerException> sleepPromise = powerManager.sleep(); wake up In sleep mode, you can wake up the robot by calling WakeUp () . java Promise<Void, PowerException> wakeUpPromise = powerManager.wakeUp(); Get the sleeping status java boolean isSleeping = powerManager.isSleeping(); Power status Get the power status BatteryProperties batteryProperties = powerManager . getBatteryProperties (); /* [1] */ [1] BatteryProperties power status Type Attributes Descriptions int level power boolean chargerAcOnline Whether it is charging with AC power boolean chargingStationOnline Whether it is charging at the charging station int chargingVoltage Charging voltage boolean full Whether the battery is fully charged int temperature Power temperature Monitor power status Register the monitoring java BatteryListener batteryListener=new BatteryListener() { @Override public void onBatteryChanged(BatteryProperties batteryProperties) { //the power status has changed } }; powerManager.registerBatteryListener(batteryListener); Cancel monitoring java powerManager.unregisterBatteryListener(batteryListener);","title":"Power service"},{"location":"android/guide/system-service/power.html#power-service","text":"The power service provides the function of robot power management. Through PowerManager , you can control the robot to sleep, wake up, query power status, shutdown and make it turn on at a set time. Get PowerManager through the [ RobotContext[ object. PowerManager powerManager = robotContext . getSystemService ( PowerManager . SERVICE );","title":"Power service"},{"location":"android/guide/system-service/power.html#turn-on-at-a-set-time","text":"Set a time to turn on java Promise<Void, PowerException> scheduleStartupPromise = powerManager.scheduleStartup(waitSeconds);/* [1] */ [1]The method that returns Promise is an asynchronous method. You can obtain the execution result by registering a callback, or you can execute the get () method to convert it to a synchronous method. For details, see async . Cancel the scheduled turn on java Promise<Void, PowerException> cancelStartupSchedulePromise = powerManager.cancelStartupSchedule();","title":"Turn on at a set time."},{"location":"android/guide/system-service/power.html#turn-off","text":"When not in use, the robot can be actively turned off. Promise < Void , PowerException > shutdownPromise = powerManager . shutdown ();","title":"Turn off"},{"location":"android/guide/system-service/power.html#sleep-and-wake-up","text":"sleep In sleep mode, power consumption will be reduced. java Promise<Void, PowerException> sleepPromise = powerManager.sleep(); wake up In sleep mode, you can wake up the robot by calling WakeUp () . java Promise<Void, PowerException> wakeUpPromise = powerManager.wakeUp(); Get the sleeping status java boolean isSleeping = powerManager.isSleeping();","title":"Sleep and wake up"},{"location":"android/guide/system-service/power.html#power-status","text":"Get the power status BatteryProperties batteryProperties = powerManager . getBatteryProperties (); /* [1] */ [1] BatteryProperties power status Type Attributes Descriptions int level power boolean chargerAcOnline Whether it is charging with AC power boolean chargingStationOnline Whether it is charging at the charging station int chargingVoltage Charging voltage boolean full Whether the battery is fully charged int temperature Power temperature","title":"Power status"},{"location":"android/guide/system-service/power.html#monitor-power-status","text":"Register the monitoring java BatteryListener batteryListener=new BatteryListener() { @Override public void onBatteryChanged(BatteryProperties batteryProperties) { //the power status has changed } }; powerManager.registerBatteryListener(batteryListener); Cancel monitoring java powerManager.unregisterBatteryListener(batteryListener);","title":"Monitor power status"},{"location":"android/guide/system-service/recharging.html","text":"Recharging service For devices with a charging station, the recharging service provides the ability for the API to call the device to connect and disconnect to the charging station. As a recharging service access agent, RechargingManager provides the main API of the recharging service, which can be obtained through the RobotContext object. RechargingManager rechargingManager = robotContext . getSystemService ( RechargingManager . SERVICE ); Connect to the charging station When the device is connected to the charging station, the charging function is automatically started, which can be achieved using the following methods: promise /* [1] */ = rechargingManager . connectToStation () . progress ( new ProgressCallback < ConnectingProgress > () { @Override public void onProgress ( ConnectingProgress connectingProgress /* [2] */ ) { // Callback the progress of the calling } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { //Callback if the calling is completed } }) . fail ( new FailCallback < RechargingException > () { @Override public void onFail ( RechargingException e ) { // Callback if the calling has failed } }); [1]Return the asynchronous object that is waiting for the progress of the connection. This object can wait for or monitor the progress, and cancel the charging process. For concrete usage, please refer to [ Promise[ . [2] TheConnectingProgress object of the asynchronous callback describes the progress information for charging station connection, including: Attribute getter Descriptions progress Connection progress type Constants Descriptions PROGRESS_BEGAN Start to connect to the charging station PROGRESS_ENDED Stop connecting to the charging station By specifying parameters, you can set the timeout duration for the charging station connection: rechargingManager . connectToStation ( new ConnectingOption . Builder (). setTimeout ( 10 ). build () /* [1] */ ); [1] The ConnectingOption object is constructed through ConnectingOption.Builder , and the instructions are as follows: Descriptions Default value Constructor function Set timeout duration, unit: second -1 (do not time out) To know whether the device is charging, use the following code: boolean isCharging = rechargingManager . isConnectedToStation (); Disconnect from the charging station If you need to disconnect from the charging station during charging, you can use the following code: rechargingManager . disconnectFromStation (); Note: This interface is an asynchronous interface, it also has progress and exception notifications. The connection is similar to the operation of connecting to a charging station.","title":"Recharging service"},{"location":"android/guide/system-service/recharging.html#recharging-service","text":"For devices with a charging station, the recharging service provides the ability for the API to call the device to connect and disconnect to the charging station. As a recharging service access agent, RechargingManager provides the main API of the recharging service, which can be obtained through the RobotContext object. RechargingManager rechargingManager = robotContext . getSystemService ( RechargingManager . SERVICE );","title":"Recharging service"},{"location":"android/guide/system-service/recharging.html#connect-to-the-charging-station","text":"When the device is connected to the charging station, the charging function is automatically started, which can be achieved using the following methods: promise /* [1] */ = rechargingManager . connectToStation () . progress ( new ProgressCallback < ConnectingProgress > () { @Override public void onProgress ( ConnectingProgress connectingProgress /* [2] */ ) { // Callback the progress of the calling } }) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { //Callback if the calling is completed } }) . fail ( new FailCallback < RechargingException > () { @Override public void onFail ( RechargingException e ) { // Callback if the calling has failed } }); [1]Return the asynchronous object that is waiting for the progress of the connection. This object can wait for or monitor the progress, and cancel the charging process. For concrete usage, please refer to [ Promise[ . [2] TheConnectingProgress object of the asynchronous callback describes the progress information for charging station connection, including: Attribute getter Descriptions progress Connection progress type Constants Descriptions PROGRESS_BEGAN Start to connect to the charging station PROGRESS_ENDED Stop connecting to the charging station By specifying parameters, you can set the timeout duration for the charging station connection: rechargingManager . connectToStation ( new ConnectingOption . Builder (). setTimeout ( 10 ). build () /* [1] */ ); [1] The ConnectingOption object is constructed through ConnectingOption.Builder , and the instructions are as follows: Descriptions Default value Constructor function Set timeout duration, unit: second -1 (do not time out) To know whether the device is charging, use the following code: boolean isCharging = rechargingManager . isConnectedToStation ();","title":"Connect to the charging station"},{"location":"android/guide/system-service/recharging.html#disconnect-from-the-charging-station","text":"If you need to disconnect from the charging station during charging, you can use the following code: rechargingManager . disconnectFromStation (); Note: This interface is an asynchronous interface, it also has progress and exception notifications. The connection is similar to the operation of connecting to a charging station.","title":"Disconnect from the charging station"},{"location":"android/guide/system-service/sensor.html","text":"Sensor service A sensor is a device that can detect one or more kinds of information from the outside world and can be transformed into electrical signals or other required forms of information output according to certain rules. The robot can sense the outside environment through the sensor. Because of the sensor, robots have human-like perception and response capabilities. The sensor service provides a series of interfaces for controlling the robot's sensors. We can obtain SensorManager through [ RobotContext[ to control the sensors. SensorManager sensorManager = robotContext . getSystemService ( SensorManager . SERVICE ); Obtain the sensor Before using the sensor, you can get the list of sensors supported by the robot and view the sensor information. Obtain the sensor of a specified ID java SensorDevice sensorDevice = sensorManager.getDevice(sensorId);/* [1] */ [1] The SensorDevice object describes the information from the sensor as follows:`` Type Attributes Descriptions String id Sensor ID String type Sensor type String name Sensor name String description Description List supportedCommandList The list of instructions supported Map commandOptionClassMap Special instruction parameters Obtain the list of sensors java List<SensorDevice> sensorDeviceList = sensorManager.getDeviceList(); Obtain the list of sensors of a specified type java List<SensorDevice> sensorDeviceList = sensorManager.getDeviceList(type);/* [2] */ [2] The sensor type is defined by the implementation of specific sensor services and varies according to product design. Turn the sensor on/off You can turn the sensor on/off through ID Turn the sensor on Promise < Void , SensorException > result = sensorManager . enable ( sensorId ); /* [1] */ [1]The method that the returned result is Promise is an asynchronous method. You can obtain the execution result by registering a callback, or you can execute the get () method to convert it to a synchronous method. For details, see async . Turn the sensor off Promise < Void , SensorException > result = ssensorManager . disable ( sensorId ); Obtain the status of the sensor You can query the status of the sensor by ID boolean sensorStatus = sensorManager . isEnabled ( sensorId ); monitor Register a sensor monitor SensorListener sensorListener = new SensorListener () { @Override public void onSensorChanged ( SensorDevice sensorDevice , SensorEvent sensorEvent /* [1] */ ) { //view the event of sensor changes here } }; sensorManager . registerListener ( sensorId , sensorListener ); [1] The sensor value's changed values are stored in SensorEvent object. The specific information is as follows: Type Attribute Description String id Sensor ID long timestamp Time stamp float[] values Changed sensor value Remove sensor monitor sensorManager . unregisterListener ( sensorListener ); Sensor list Name Id ultrasonic1 ultrasonic1 ultrasonic2 ultrasonic2 ultrasonic3 ultrasonic3 ultrasonic4 ultrasonic4 ultrasonic5 ultrasonic5 ultrasonic6 ultrasonic6 ultrasonic7 ultrasonic7 ultrasonic8 ultrasonic8 rgbd rgbd1 wallIR1 wallIR1 wallIR2 wallIR2 wallIR3 wallIR3 wallIR4 wallIR4 wallIR5 wallIR5 wallIR6 wallIR6 groundIR1 groundIR1 groundIR2 groundIR2 groundIR3 groundIR3 groundIR4 groundIR4 groundIR5 groundIR5 groundIR6 groundIR6 lidar lidar1 obstacle_detect obstacle_detect human_detect human_detect cliff_detect cliff_detect humiture humiture pyroelectric pyroelectric TOF_infrared TOF_infrared geomagnetism_detect geomagnetism_detect LPalm_skin LPalm_skin LPalm_switch LPalm_switch LFArm_skin LFArm_skin LBArm_skin LBArm_skin RPalm_skin RPalm_skin RPalm_switch RPalm_switch RFArm_skin RFArm_skin RBArm_skin RBArm_skin emergency_stop emergency_stop totalSwitch totalSwitch","title":"Sensor service"},{"location":"android/guide/system-service/sensor.html#sensor-service","text":"A sensor is a device that can detect one or more kinds of information from the outside world and can be transformed into electrical signals or other required forms of information output according to certain rules. The robot can sense the outside environment through the sensor. Because of the sensor, robots have human-like perception and response capabilities. The sensor service provides a series of interfaces for controlling the robot's sensors. We can obtain SensorManager through [ RobotContext[ to control the sensors. SensorManager sensorManager = robotContext . getSystemService ( SensorManager . SERVICE );","title":"Sensor service"},{"location":"android/guide/system-service/sensor.html#obtain-the-sensor","text":"Before using the sensor, you can get the list of sensors supported by the robot and view the sensor information. Obtain the sensor of a specified ID java SensorDevice sensorDevice = sensorManager.getDevice(sensorId);/* [1] */ [1] The SensorDevice object describes the information from the sensor as follows:`` Type Attributes Descriptions String id Sensor ID String type Sensor type String name Sensor name String description Description List supportedCommandList The list of instructions supported Map commandOptionClassMap Special instruction parameters Obtain the list of sensors java List<SensorDevice> sensorDeviceList = sensorManager.getDeviceList(); Obtain the list of sensors of a specified type java List<SensorDevice> sensorDeviceList = sensorManager.getDeviceList(type);/* [2] */ [2] The sensor type is defined by the implementation of specific sensor services and varies according to product design.","title":"Obtain the sensor"},{"location":"android/guide/system-service/sensor.html#turn-the-sensor-onoff","text":"You can turn the sensor on/off through ID Turn the sensor on Promise < Void , SensorException > result = sensorManager . enable ( sensorId ); /* [1] */ [1]The method that the returned result is Promise is an asynchronous method. You can obtain the execution result by registering a callback, or you can execute the get () method to convert it to a synchronous method. For details, see async . Turn the sensor off Promise < Void , SensorException > result = ssensorManager . disable ( sensorId );","title":"Turn the sensor on/off"},{"location":"android/guide/system-service/sensor.html#obtain-the-status-of-the-sensor","text":"You can query the status of the sensor by ID boolean sensorStatus = sensorManager . isEnabled ( sensorId );","title":"Obtain the status of the sensor"},{"location":"android/guide/system-service/sensor.html#monitor","text":"Register a sensor monitor SensorListener sensorListener = new SensorListener () { @Override public void onSensorChanged ( SensorDevice sensorDevice , SensorEvent sensorEvent /* [1] */ ) { //view the event of sensor changes here } }; sensorManager . registerListener ( sensorId , sensorListener ); [1] The sensor value's changed values are stored in SensorEvent object. The specific information is as follows: Type Attribute Description String id Sensor ID long timestamp Time stamp float[] values Changed sensor value Remove sensor monitor sensorManager . unregisterListener ( sensorListener );","title":"monitor"},{"location":"android/guide/system-service/sensor.html#sensor-list","text":"Name Id ultrasonic1 ultrasonic1 ultrasonic2 ultrasonic2 ultrasonic3 ultrasonic3 ultrasonic4 ultrasonic4 ultrasonic5 ultrasonic5 ultrasonic6 ultrasonic6 ultrasonic7 ultrasonic7 ultrasonic8 ultrasonic8 rgbd rgbd1 wallIR1 wallIR1 wallIR2 wallIR2 wallIR3 wallIR3 wallIR4 wallIR4 wallIR5 wallIR5 wallIR6 wallIR6 groundIR1 groundIR1 groundIR2 groundIR2 groundIR3 groundIR3 groundIR4 groundIR4 groundIR5 groundIR5 groundIR6 groundIR6 lidar lidar1 obstacle_detect obstacle_detect human_detect human_detect cliff_detect cliff_detect humiture humiture pyroelectric pyroelectric TOF_infrared TOF_infrared geomagnetism_detect geomagnetism_detect LPalm_skin LPalm_skin LPalm_switch LPalm_switch LFArm_skin LFArm_skin LBArm_skin LBArm_skin RPalm_skin RPalm_skin RPalm_switch RPalm_switch RFArm_skin RFArm_skin RBArm_skin RBArm_skin emergency_stop emergency_stop totalSwitch totalSwitch","title":"Sensor list"},{"location":"android/guide/system-service/servo.html","text":"Servo service The servo service provides the API's ability to call the device's \"joints\". As the servo service access agent, ServoManager provides the main API of the servo service, which can be obtained through the [ RobotContext[ object. ServoManager servoManager = aRobotContext . getSystemService ( ServoManager . SERVICE ); Obtain servo list To obtain the number of servos and the configuration parameters, you can use the following code: List < ServoDevice > /* [1] */ servoDevices = servoManager . getDeviceList (); [1]Servo set, ServoDevice is the detailed configuration parameters of the servo, including: Attribute getter Descriptions ServoDevice.id Servo id ServoDevice.name Servo name ServoDevice.description Servo description ServoDevice.minAngle The minimum angle it can rotate ServoDevice.maxAngle The maximum angle it can rotate ServoDevice.minSpeed Minimum speed ServoDevice.maxSpeed Maximum speed ServoDevice.defaultSpeed Default speed Turn the servo When the device is required to act like a human, it can be achieved by turning the servo. To rotate the servo, you can use the relative angle or absolute angle. The rotation can be a single task or a serial task, which will be introduced one by one below. When the servo is rotated using the relative angle, you can specify the angle of relative rotation only, or you can also specify the angle, speed, and duration of the relative rotation. To only specify the angle of relative rotation, you can use the following code: servo no. 1001 rotates 90 degrees. promise /* [1] */ = servoManager . rotateBy ( \"1001\" , 90 f ) . progress ( new ProgressCallback < RotationProgress > () { @Override public void onProgress ( RotationProgress rotationProgress /* [2] */ ) { // Callback the servo's rotation process } }). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback the servo rotation accomplishment } }). fail ( new FailCallback < ServoException > () { @Override public void onFail ( ServoException e ) { // Callback the servo rotation wrong } }); [1]Return the asynchronous object that is waiting for the rotation progress and results, through which you can wait for or monitor the progress and results, and cancel the rotation process. See Promise for specific usage. [2] TheRotationProgress object of the asynchronous callback describes the progress information of the servo rotation, including: Attribute getter Descriptions RotationProgress.sessionId Servo id RotationProgress.angle The current angle of rotation Specify the angle and speed of relative rotation using the following code: servo no.1001 rotates 90 degrees at a speed of 10 degrees per second. promise = servoManager . rotateBy ( \"1001\" , 90 f , 10 f ); Specify the angle and duration of relative rotation through the following code: servo no.1001 rotates 90 degrees within 10 seconds. promise = servoManager . rotateBy ( \"1001\" , 90 f , 10000 ); When the servo uses absolute angle rotation, you can just specify the angle of absolute rotation, or also the angle, speed, and duration of absolute rotation. To only specify the angle of relative rotation, you can use the following code: servo no.1001 rotates to the position of 90 degrees. promise = servoManager . rotateTo ( \"1001\" , 90 f ); Specify the angle and speed of absolute rotation through the following code: servo no.1001 rotates 90 degrees at a speed of 10 degrees per second. promise = servoManager . rotateTo ( \"1001\" , 90 f , 10 f ); Specify the angle and duration of absolute rotation through the following code: servo no.1001 rotates 90 degrees within 10 seconds. promise = servoManager . rotateTo ( \"1001\" , 90 f , 10000 ); Specify a rotation option to adjust the action of the servo executing a single task. For example: servo no.1001 will rotate to 90 degrees after 10 seconds. RotationOption /* [1] */ option = new RotationOption . Builder ( \"1001\" ) . setAngleAbsolute ( true ). setAngle ( 90 f ). setDuration ( 10000 ). build (); promise = servoManager . rotate ( option ); [1] The RotationOption object is constructed by RotationOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(servoId) Specify the servo id when constructing Builder.setServoId(servoId) Servo id Zero-length string Builder.setAngle(angle) angle 0 Builder.setAngleAbsolute(angleAbsolute) Identifier of the rotation method Relative angle rotation Builder.setDuration(duration) Duration of rotation, unit: millisecond 0 Builder.setSpeed(speed) Speed, unit: degrees per second If there are both duration and speed, the priority will be given to speed 0 Specify multiple rotation options to adjust the action of the servo's execution of serial tasks. The following two implementation methods are available. For example: servo no.1001 will rotate to the position of 90 degree after 10 seconds, and it will take another 10 seconds to rotate 360 degrees. // Method 1 RotationOption option1 = new RotationOption . Builder ( \"1001\" ) . setAngleAbsolute ( true ). setAngle ( 90 f ). setDuration ( 10000 ). build (); RotationOption option2 = new RotationOption . Builder ( \"1001\" ) . setAngle ( 360 f ). setDuration ( 10000 ). build (); promise = servoManager . rotateSerially ( option1 , option2 ); // Method 2 option1 = new RotationOption . Builder ( \"1001\" ) . setAngleAbsolute ( true ). setAngle ( 90 f ). setDuration ( 10000 ). build (); option2 = new RotationOption . Builder ( \"1001\" ). setAngle ( 360 f ). setDuration ( 10000 ). build (); List < RotationOption > optionList = new ArrayList <> (); optionList . add ( option1 ); optionList . add ( option2 ); promise = servoManager . rotateSerially ( optionList ); If you want to know if the servo is rotating currently, use the following code. boolean isRotating = servoManager . servoManager ( \"1001\" ); If true is returned, it means that servo no.1001 is currently rotating. If you want to obtain the current angle of the servo, use the following code. float angle = servoManager . getAngle ( \"1001\" ); Release servo When you need to calibrate the angle of the servo, you need to call the API that releases the servo first, so that the upper layer can no longer access the agent object ServoManager to control the servo during the process of calibration. When releasing, you can specify one servo or multiple servos, which will be introduced one by one below. Release one: release servo no.1001. promise /* [1] */ = servoManager . release ( \"1001\" ). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback when servo no.1001 is released, you can start calibratiing } }). fail ( new FailCallback < ServoException > () { @Override public void onFail ( ServoException e ) { // Callback the releasing goes wrong } }); [1] Return the asynchronous object that is waiting for the result of releasing, through which you can wait for the result and cancel the release process. See [ Promise ](../other/async.md) for specific usage. To release multiple servos at the same time, it can be achieved by the following two methods. promise = servoManager . release ( \"1001\" , \"1002\" ); List < String > servoIds = new ArrayList <> (); servoIds . add ( \"1001\" ); servoIds . add ( \"1002\" ); promise = servoManager . release ( servoIds ); If you want to know if the servo is released, use the following code. boolean isReleased = servoManager . isReleased ( \"1001\" ); If true is returned, it means that servo no.1001 has been released.`` Servo monitoring If you want to perceive the rotation of the servo, you can register a servo monitor, which is achieved using the following code. RotationListener listener = new RotationListener () { @Override public void onRotationBegan ( List < ServoDevice > list /* [1] */ ) { // Callback the servo starts rotating } @Override public void onRotating ( Map < ServoDevice , RotationProgress > map /* [2] */ ) { // Callback the servo's rotation process } @Override public void onRotationEnded ( List < ServoDevice > list /* [3] */ ) { // Callback the servo completes rotation } }; servoManager . registerRotationListener ( listener , \"1001\" , \"1002\" /* [4] */ ); [1]The set of servos that started rotation. See [ServoDevice[ for details of the servos. [2] The set of servos and their corresponding progress. For details about the progress, see [RotationProgress . [3] The set of servos that completed rotation [4] Both 1001 and 1002 indicate the number of servos, which can also be written as a set here, as follows: List < String > servoIds = new ArrayList <> (); servoIds . add ( \"1001\" ); servoIds . add ( \"1002\" ); servoManager . registerRotationListener ( listener , servoIds ); If you don't want to perceive the rotation of the servo, use the following code: servoManager . unregisterRotationListener ( listener ); Angle of the head, it provides a suggested value in comparison to the height At present, only the head servo (id: 0x0e) can be controlled, and the angle range of the head is 180 to 270 degrees. The following table shows the experience value of the angle of the head servo when users of different heights are about 50cm in front of the robot. Height (cm) Angle (\u00b0) 180+ 180 165-180 180-195 155-165 195-210 145-155 210-225 135-145 225-240 125-135 240-255 115-125 255-270 <115 270 Servo images Note: The robot 1s has no waist servo 0x0d Robot servo chart: ID NAME Note 0x01 LShoulderPith The first servo on the left arm 0x02 LShoulderRoll The second servo on the left arm 0x03 LShoulderYaw The third servo on the left arm 0x04 LElbowRoll The fourth servo on the left arm 0x05 LElbowYaw The fifth servo on the left arm 0x07 RShoulderPith The first servo on the right arm 0x08 RShoulderRoll The second servo on the right arm 0x09 RShoulderYaw The third servo on the right arm 0x10 RHand Right hand servo 0x0a RElbowRol The fourth servo on the right arm 0x0b RElbowYaw The fifth servo on the right arm 0x0d HeadYaw Waist servo 0x0e HeadPitch Head servo 0x0f LHand Left hand servo","title":"Servo service"},{"location":"android/guide/system-service/servo.html#servo-service","text":"The servo service provides the API's ability to call the device's \"joints\". As the servo service access agent, ServoManager provides the main API of the servo service, which can be obtained through the [ RobotContext[ object. ServoManager servoManager = aRobotContext . getSystemService ( ServoManager . SERVICE );","title":"Servo service"},{"location":"android/guide/system-service/servo.html#obtain-servo-list","text":"To obtain the number of servos and the configuration parameters, you can use the following code: List < ServoDevice > /* [1] */ servoDevices = servoManager . getDeviceList (); [1]Servo set, ServoDevice is the detailed configuration parameters of the servo, including: Attribute getter Descriptions ServoDevice.id Servo id ServoDevice.name Servo name ServoDevice.description Servo description ServoDevice.minAngle The minimum angle it can rotate ServoDevice.maxAngle The maximum angle it can rotate ServoDevice.minSpeed Minimum speed ServoDevice.maxSpeed Maximum speed ServoDevice.defaultSpeed Default speed","title":"Obtain servo list"},{"location":"android/guide/system-service/servo.html#turn-the-servo","text":"When the device is required to act like a human, it can be achieved by turning the servo. To rotate the servo, you can use the relative angle or absolute angle. The rotation can be a single task or a serial task, which will be introduced one by one below. When the servo is rotated using the relative angle, you can specify the angle of relative rotation only, or you can also specify the angle, speed, and duration of the relative rotation. To only specify the angle of relative rotation, you can use the following code: servo no. 1001 rotates 90 degrees. promise /* [1] */ = servoManager . rotateBy ( \"1001\" , 90 f ) . progress ( new ProgressCallback < RotationProgress > () { @Override public void onProgress ( RotationProgress rotationProgress /* [2] */ ) { // Callback the servo's rotation process } }). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback the servo rotation accomplishment } }). fail ( new FailCallback < ServoException > () { @Override public void onFail ( ServoException e ) { // Callback the servo rotation wrong } }); [1]Return the asynchronous object that is waiting for the rotation progress and results, through which you can wait for or monitor the progress and results, and cancel the rotation process. See Promise for specific usage. [2] TheRotationProgress object of the asynchronous callback describes the progress information of the servo rotation, including: Attribute getter Descriptions RotationProgress.sessionId Servo id RotationProgress.angle The current angle of rotation Specify the angle and speed of relative rotation using the following code: servo no.1001 rotates 90 degrees at a speed of 10 degrees per second. promise = servoManager . rotateBy ( \"1001\" , 90 f , 10 f ); Specify the angle and duration of relative rotation through the following code: servo no.1001 rotates 90 degrees within 10 seconds. promise = servoManager . rotateBy ( \"1001\" , 90 f , 10000 ); When the servo uses absolute angle rotation, you can just specify the angle of absolute rotation, or also the angle, speed, and duration of absolute rotation. To only specify the angle of relative rotation, you can use the following code: servo no.1001 rotates to the position of 90 degrees. promise = servoManager . rotateTo ( \"1001\" , 90 f ); Specify the angle and speed of absolute rotation through the following code: servo no.1001 rotates 90 degrees at a speed of 10 degrees per second. promise = servoManager . rotateTo ( \"1001\" , 90 f , 10 f ); Specify the angle and duration of absolute rotation through the following code: servo no.1001 rotates 90 degrees within 10 seconds. promise = servoManager . rotateTo ( \"1001\" , 90 f , 10000 ); Specify a rotation option to adjust the action of the servo executing a single task. For example: servo no.1001 will rotate to 90 degrees after 10 seconds. RotationOption /* [1] */ option = new RotationOption . Builder ( \"1001\" ) . setAngleAbsolute ( true ). setAngle ( 90 f ). setDuration ( 10000 ). build (); promise = servoManager . rotate ( option ); [1] The RotationOption object is constructed by RotationOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(servoId) Specify the servo id when constructing Builder.setServoId(servoId) Servo id Zero-length string Builder.setAngle(angle) angle 0 Builder.setAngleAbsolute(angleAbsolute) Identifier of the rotation method Relative angle rotation Builder.setDuration(duration) Duration of rotation, unit: millisecond 0 Builder.setSpeed(speed) Speed, unit: degrees per second If there are both duration and speed, the priority will be given to speed 0 Specify multiple rotation options to adjust the action of the servo's execution of serial tasks. The following two implementation methods are available. For example: servo no.1001 will rotate to the position of 90 degree after 10 seconds, and it will take another 10 seconds to rotate 360 degrees. // Method 1 RotationOption option1 = new RotationOption . Builder ( \"1001\" ) . setAngleAbsolute ( true ). setAngle ( 90 f ). setDuration ( 10000 ). build (); RotationOption option2 = new RotationOption . Builder ( \"1001\" ) . setAngle ( 360 f ). setDuration ( 10000 ). build (); promise = servoManager . rotateSerially ( option1 , option2 ); // Method 2 option1 = new RotationOption . Builder ( \"1001\" ) . setAngleAbsolute ( true ). setAngle ( 90 f ). setDuration ( 10000 ). build (); option2 = new RotationOption . Builder ( \"1001\" ). setAngle ( 360 f ). setDuration ( 10000 ). build (); List < RotationOption > optionList = new ArrayList <> (); optionList . add ( option1 ); optionList . add ( option2 ); promise = servoManager . rotateSerially ( optionList ); If you want to know if the servo is rotating currently, use the following code. boolean isRotating = servoManager . servoManager ( \"1001\" ); If true is returned, it means that servo no.1001 is currently rotating. If you want to obtain the current angle of the servo, use the following code. float angle = servoManager . getAngle ( \"1001\" );","title":"Turn the servo"},{"location":"android/guide/system-service/servo.html#release-servo","text":"When you need to calibrate the angle of the servo, you need to call the API that releases the servo first, so that the upper layer can no longer access the agent object ServoManager to control the servo during the process of calibration. When releasing, you can specify one servo or multiple servos, which will be introduced one by one below. Release one: release servo no.1001. promise /* [1] */ = servoManager . release ( \"1001\" ). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // Callback when servo no.1001 is released, you can start calibratiing } }). fail ( new FailCallback < ServoException > () { @Override public void onFail ( ServoException e ) { // Callback the releasing goes wrong } }); [1] Return the asynchronous object that is waiting for the result of releasing, through which you can wait for the result and cancel the release process. See [ Promise ](../other/async.md) for specific usage. To release multiple servos at the same time, it can be achieved by the following two methods. promise = servoManager . release ( \"1001\" , \"1002\" ); List < String > servoIds = new ArrayList <> (); servoIds . add ( \"1001\" ); servoIds . add ( \"1002\" ); promise = servoManager . release ( servoIds ); If you want to know if the servo is released, use the following code. boolean isReleased = servoManager . isReleased ( \"1001\" ); If true is returned, it means that servo no.1001 has been released.``","title":"Release servo"},{"location":"android/guide/system-service/servo.html#servo-monitoring","text":"If you want to perceive the rotation of the servo, you can register a servo monitor, which is achieved using the following code. RotationListener listener = new RotationListener () { @Override public void onRotationBegan ( List < ServoDevice > list /* [1] */ ) { // Callback the servo starts rotating } @Override public void onRotating ( Map < ServoDevice , RotationProgress > map /* [2] */ ) { // Callback the servo's rotation process } @Override public void onRotationEnded ( List < ServoDevice > list /* [3] */ ) { // Callback the servo completes rotation } }; servoManager . registerRotationListener ( listener , \"1001\" , \"1002\" /* [4] */ ); [1]The set of servos that started rotation. See [ServoDevice[ for details of the servos. [2] The set of servos and their corresponding progress. For details about the progress, see [RotationProgress . [3] The set of servos that completed rotation [4] Both 1001 and 1002 indicate the number of servos, which can also be written as a set here, as follows: List < String > servoIds = new ArrayList <> (); servoIds . add ( \"1001\" ); servoIds . add ( \"1002\" ); servoManager . registerRotationListener ( listener , servoIds ); If you don't want to perceive the rotation of the servo, use the following code: servoManager . unregisterRotationListener ( listener );","title":"Servo monitoring"},{"location":"android/guide/system-service/servo.html#angle-of-the-head-it-provides-a-suggested-value-in-comparison-to-the-height","text":"At present, only the head servo (id: 0x0e) can be controlled, and the angle range of the head is 180 to 270 degrees. The following table shows the experience value of the angle of the head servo when users of different heights are about 50cm in front of the robot. Height (cm) Angle (\u00b0) 180+ 180 165-180 180-195 155-165 195-210 145-155 210-225 135-145 225-240 125-135 240-255 115-125 255-270 <115 270","title":"Angle of the head, it provides a suggested value in comparison to the height"},{"location":"android/guide/system-service/servo.html#servo-images","text":"Note: The robot 1s has no waist servo 0x0d Robot servo chart: ID NAME Note 0x01 LShoulderPith The first servo on the left arm 0x02 LShoulderRoll The second servo on the left arm 0x03 LShoulderYaw The third servo on the left arm 0x04 LElbowRoll The fourth servo on the left arm 0x05 LElbowYaw The fifth servo on the left arm 0x07 RShoulderPith The first servo on the right arm 0x08 RShoulderRoll The second servo on the right arm 0x09 RShoulderYaw The third servo on the right arm 0x10 RHand Right hand servo 0x0a RElbowRol The fourth servo on the right arm 0x0b RElbowYaw The fifth servo on the right arm 0x0d HeadYaw Waist servo 0x0e HeadPitch Head servo 0x0f LHand Left hand servo","title":"Servo images"},{"location":"android/guide/system-service/skillmanager.html","text":"Skill manager The skill manager service provides APIs to handle operations related to skills , including controlling skill status, monitoring skill status and changes, and distributing skill instructions. As the skill management service access agent, SkillManager provides the main API of the skill management service, which can be obtained through RobotContext . SkillManager skillManager = robotContext . getSystemService ( SkillManager . SERVICE ); Control skill status In the Cruzr system, in most cases, the status of skills is managed through system decision-making. For developers who need to actively control the status of skills, skill management also provides interfaces for processing. Pause the skill promise /* [1] */ = skillManager . pauseSkill ( \"com.xxx.pkg\" , \"music\" /* [2] */ ) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // if the callback is successful, it will be executed here } }) . fail ( new FailCallback < SkillOperationException > () { @Override public void onFail ( SkillOperationException e ) { // if the callback has failed, it will be executed here } }); [1] The asynchronous object that returns the results of the pause skill. For specific usage, refer to Promise [2] To pause the skill, the packagename and `skill name need to be provided. skill Stop the skill promise = skillManager . stopSkill ( \"com.xxx.pkg\" , \"music\" ); The operation of stopping skills is similar to that of pausing skills, which will not be described in detail here. Listen the changes of skill status When the skill status changes, the Cruzr system will send event notifications, and skill management provides an interface to monitor the status changes of skills in the system. skillManager . registerSkillLifecycleCallbacks ( new SkillLifecycleCallbacks () { @Override public void onSkillStarted ( SkillInfo skillInfo /* [1] */ ) { // If the skill is started, it will be noticed here } @Override public void onSkillPaused ( SkillInfo skillInfo , SkillPauseCause skillPauseCause /* [2] */ ) { // If the skill is paused, it will be noticed here } @Override public void onSkillStopped ( SkillInfo skillInfo , SkillStopCause skillStopCause /* [3] */ ) { // If the skill is stopped, it will be noticed here } }); [1] SkillInfo contains the detailed information of skills, the parameters are as follows: Attribute getter Descriptions SkillInfo.name Skill name, refer to Configure Skills SkillInfo.className Skill name, Java class name SkillInfo.packageName The package name of the process where the skill is located SkillInfo.isSystemPackage Whether the process where the skill is located is a system process SkillInfo.directiveList A list of skill instructions. For details, refer to Configuration Instructions. [2] SkillPauseCause The cause for pausing the skill. For details, refer to Skill Status. [3] SkillStopCause The cause for stopping the skill. For details, refer to Skill Status. Inter-process instruction distribution Cruzr developers often send instructions to other process skills. Skill management provides interfaces to distribute inter-process instructions. For in-process instruction distribution, see Context JSONObject jsonObject = new JSONObject (); jsonObject . put ( \"test\" , \"test\" ); Music music = new Music ( \"xxx/xxx.pm3\" , \"singer\" ); Directive directive /* [1] */ = Directive . Builder . fromAction ( Directive . SOURCE_INTER_PROCESS , \"music/play\" ) /* [2] */ . setSourceExtra ( JsonObjectString . from ( jsonObject )) /* [3] */ . setParam ( music , ContentTypes . PARCELABLE ) /* [4] */ . build (); skillManager . dispatchDirective ( directive ) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // if the distribution is successful, it will be executed here } }) . fail ( new FailCallback < DispatchException > () { @Override public void onFail ( DispatchException e ) { // if the distribution has failed, it will be executed here } }); [1] Directive uses an instruction object that sends instructions. For details, refer to the instruction object. [2] Specifying the instruction source is an inter-process, the instruction action is \"music / play\" [3] Customized information carried by the instruction source [4] The instruction contains parameters, and the specified type is ContentTypes.PARCELABLE , where music must be a parcelable type. For other types, refer to serialization type. [5] To pause the skill, the packagename and `skill name need to be provided. skill","title":"Skill manager"},{"location":"android/guide/system-service/skillmanager.html#skill-manager","text":"The skill manager service provides APIs to handle operations related to skills , including controlling skill status, monitoring skill status and changes, and distributing skill instructions. As the skill management service access agent, SkillManager provides the main API of the skill management service, which can be obtained through RobotContext . SkillManager skillManager = robotContext . getSystemService ( SkillManager . SERVICE );","title":"Skill manager"},{"location":"android/guide/system-service/skillmanager.html#control-skill-status","text":"In the Cruzr system, in most cases, the status of skills is managed through system decision-making. For developers who need to actively control the status of skills, skill management also provides interfaces for processing.","title":"Control skill status"},{"location":"android/guide/system-service/skillmanager.html#pause-the-skill","text":"promise /* [1] */ = skillManager . pauseSkill ( \"com.xxx.pkg\" , \"music\" /* [2] */ ) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // if the callback is successful, it will be executed here } }) . fail ( new FailCallback < SkillOperationException > () { @Override public void onFail ( SkillOperationException e ) { // if the callback has failed, it will be executed here } }); [1] The asynchronous object that returns the results of the pause skill. For specific usage, refer to Promise [2] To pause the skill, the packagename and `skill name need to be provided. skill","title":"Pause the skill"},{"location":"android/guide/system-service/skillmanager.html#stop-the-skill","text":"promise = skillManager . stopSkill ( \"com.xxx.pkg\" , \"music\" ); The operation of stopping skills is similar to that of pausing skills, which will not be described in detail here.","title":"Stop the skill"},{"location":"android/guide/system-service/skillmanager.html#listen-the-changes-of-skill-status","text":"When the skill status changes, the Cruzr system will send event notifications, and skill management provides an interface to monitor the status changes of skills in the system. skillManager . registerSkillLifecycleCallbacks ( new SkillLifecycleCallbacks () { @Override public void onSkillStarted ( SkillInfo skillInfo /* [1] */ ) { // If the skill is started, it will be noticed here } @Override public void onSkillPaused ( SkillInfo skillInfo , SkillPauseCause skillPauseCause /* [2] */ ) { // If the skill is paused, it will be noticed here } @Override public void onSkillStopped ( SkillInfo skillInfo , SkillStopCause skillStopCause /* [3] */ ) { // If the skill is stopped, it will be noticed here } }); [1] SkillInfo contains the detailed information of skills, the parameters are as follows: Attribute getter Descriptions SkillInfo.name Skill name, refer to Configure Skills SkillInfo.className Skill name, Java class name SkillInfo.packageName The package name of the process where the skill is located SkillInfo.isSystemPackage Whether the process where the skill is located is a system process SkillInfo.directiveList A list of skill instructions. For details, refer to Configuration Instructions. [2] SkillPauseCause The cause for pausing the skill. For details, refer to Skill Status. [3] SkillStopCause The cause for stopping the skill. For details, refer to Skill Status.","title":"Listen the changes of skill status"},{"location":"android/guide/system-service/skillmanager.html#inter-process-instruction-distribution","text":"Cruzr developers often send instructions to other process skills. Skill management provides interfaces to distribute inter-process instructions. For in-process instruction distribution, see Context JSONObject jsonObject = new JSONObject (); jsonObject . put ( \"test\" , \"test\" ); Music music = new Music ( \"xxx/xxx.pm3\" , \"singer\" ); Directive directive /* [1] */ = Directive . Builder . fromAction ( Directive . SOURCE_INTER_PROCESS , \"music/play\" ) /* [2] */ . setSourceExtra ( JsonObjectString . from ( jsonObject )) /* [3] */ . setParam ( music , ContentTypes . PARCELABLE ) /* [4] */ . build (); skillManager . dispatchDirective ( directive ) . done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // if the distribution is successful, it will be executed here } }) . fail ( new FailCallback < DispatchException > () { @Override public void onFail ( DispatchException e ) { // if the distribution has failed, it will be executed here } }); [1] Directive uses an instruction object that sends instructions. For details, refer to the instruction object. [2] Specifying the instruction source is an inter-process, the instruction action is \"music / play\" [3] Customized information carried by the instruction source [4] The instruction contains parameters, and the specified type is ContentTypes.PARCELABLE , where music must be a parcelable type. For other types, refer to serialization type. [5] To pause the skill, the packagename and `skill name need to be provided. skill","title":"Inter-process instruction distribution"},{"location":"android/guide/system-service/speech.html","text":"Speech service The speech service provides API with the ability to call the device\u2019s speech related capabilities, such as \"speak\", \"listen\", and \u201cunderstand\u201d. As the speech service access agent, SpeechManager provides the main API of the speech service and can be obtained through RobotContext . SpeechManager speechManager = aRobotContext . getSystemService ( SpeechManager . SERVICE ); In the field of speech technology, there are terms such as Keyword Spotting (KWS), Text to Speech (TTS), Automatic Speech Recognition (ASR), Natural Language Understanding (NLU), and Natural Language Processing (NLP). These terms refer to the subdivided technologies of the field of speech technology, such as speech service wake-up, text-to-speech, speech recognition, natural language understanding and processing. The speech service is realized using these technologies, and calling the SpeechManager API is actually a call to these technical capabilities. The following chapters will introduce them one by one. Speech service wake-up When the user speaks a wake-up word to the device, using the speech wake-up technology, the device can perceive that the user is \"calling it\", and then it can start interacting with the user. This process is called \"speech service wake-up\". If you need to perceive speech wake-up, use the following code. WakeUpListener wakeUpListener = new WakeUpListener () { @Override public void onWakingUp ( WakeUp wakeUp /* [1] */ ) { // if it is perceived that the user has said the wake-up word, it will run here } }; speechManager . registerWakeUpListener ( wakeUpListener ); [1] The WakeUp object describes the environmental information at the time of speech service wake-up, including: Attribute getter Descriptions WakeUp.when The timestamp of the wake-up time, unit: ms. WakeUp.voiceDirection The sound source of the wake-up time If you no longer need to perceive speech service wake-up, use the following code. speechManager . unregisterWakeUpListener ( wakeUpListener ); Text-to-speech When you need the device to speak like a human, using the text-to-speech technology, text can be synthesized into human voices and played. This process is realized using the following code. promise /* [2] */ = speechManager . synthesize ( \"Hello, what can I do for you?\" /* [1] */ ) . progress ( new ProgressCallback < SynthesisProgress > () { @Override public void onProgress ( SynthesisProgress progress /* [3] */ ) { // The synthetic process will be run here multiple times } }). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // \u201cHello, what can I do for you?\u201d After saying that, it will be run here } }). fail ( new FailCallback < SynthesisException > () { @Override public void onFail ( SynthesisException e ) { // If the synthetic process goes wrong, it will be run here } }); [1] Enter the text sentences to be synthesized, it supports multiple sentences with punctuation. [2] The asynchronous object that is waiting for the progress and result of the synthesis is returned, through which you can wait for or monitor the progress and results, and cancel the synthesis process. See Promise for specific usage. [3] The SynthesisProgress object of the asynchronous callback describes the progress information of speech synthesis, including: Attribute getter Descriptions SynthesisProgress.remainingTimeMillis Remaining playback time SynthesisProgress.playProgress Playback progress SynthesisProgress.audioBytes Play the resource, some platforms may not provide this resource. By specifying options, you can adjust the behavior of speech synthesis. SynthesisOption /*[1]*/ option = new SynthesisOption . Builder ( \"Hello, what can I do for you?\" ). [1] The SynthesisOption object is constructed through SynthesisOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(inputText) When constructing, you must import synthetic texts, NULL objects or strings whose lengths are 0 are not allowed. Builder.setSpeakingSpeed(speakingSpeed) Set the speaking speed 0 Builder.setSpeakingVolume(speakingVolume) Set the speaking volume 0 Builder.setSpeakingVoiceId(speakingVoiceId) Set the speaker, if you want to know the speakers that the system has, please refer to the speech settings Zero-length string Constants Descriptions SynthesisOption.SPEAKING_VOLUME_MAX The maximum volume available SynthesisOption.SPEAKING_VOLUME_MIN The minimum volume available SynthesisOption.SPEAKING_SPEED_FAST Fast speech speed SynthesisOption.SPEAKING_SPEED_NORMAL Normal speech speed SynthesisOption.SPEAKING_SPEED_SLOW Slow speech speed If you want to know the current synthesis status, use the following code. boolean isSynthesizing = speechManager . isSynthesizing (); If true is returned, it means that the synthesis is in progress. Speech recognition When the user speaks to the device, the speech recognition technology can be used to convert speech into text or instructions. This process is realized using the following code. RecognitionOption /* [1] */ option = new RecognitionOption . Builder ( RecognitionOption . MODE_SINGLE ). build (); promise /* [2] */ = speechManager . recognize ( option ) . progress ( new ProgressCallback < RecognitionProgress > () { @Override public void onProgress ( RecognitionProgress recognitionProgress /* [3] */ ) { // the recognition progress will be run here multiple times } }). done ( new DoneCallback < RecognitionResult > () { @Override public void onDone ( RecognitionResult recognitionResult /* [4] */ ) { // if the recognition progress is completed, the final recognition result will be returned } }). fail ( new FailCallback < RecognitionException > () { @Override public void onFail ( RecognitionException e ) { // if the recognition progress goes wrong, it will be run here. } }); [1] Speech recognition options, which can be used to adjust the speech recognition behavior. It is built with RecognitionOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(mode) You must specify a recognition mode when constructing. Builder.setDistanceRange(distanceRange) Recognition scenarios Near-field recognition Builder.setTimeoutMillis(timeoutMillis) Recognize the timeout length No timeout Builder.setUnderstandingOption(understandingOption) Natural language processing options NULL Builder.setExtension(extension) Customized field NULL Constants Descriptions RecognitionOption.MODE_SINGLE Single recognition: It is generally used to start speech recognition once after waking up. The recognition progress will be stopped after getting the result, and it will wait for the next wake up. RecognitionOption.MODE_CONTINUOUS Continuous recognition: Uninterrupted recognition after wake-up is started, the data is returned after recognition, and the recognition will be continued. RecognitionOption.DISTANCE_RANGE_NEAR_FIELD Near-field recognition RecognitionOption.DISTANCE_RANGE_FAR_FIELD Far-field recognition RecognitionOption.DISTANCE_RANGE_CLOSE_TALK Super near field recognition [2] The asynchronous object that is waiting for the recognition progress and results is returned, through which you can wait for or monitor the progress and results, and cancel the synthesis process. See Promise for specific usage. [3] The RecognitionProgress object of the asynchronous callback describes the progress information of speech recognition, including: Attribute getter Descriptions RecognitionProgress.decibel Decibels recognized RecognitionProgress.textResult Text converted from the recognized speech RecognitionProgress.understandingResult The result of the recognized speech after natural language processing. For details, see Natural Language Processing. RecognitionProgress.progress Recognition progress Constants Descriptions RecognitionResult.began Start of recognition RecognitionResult.ended End of recognition RecognitionProgress.PROGRESS_RECOGNIZING Recognizing RecognitionProgress.PROGRESS_RECOGNITION_TEXT_RESULT Text recognized RecognitionProgress.PROGRESS_UNDERSTANDING_RESULT In natural language processing [4] The RecognitionResult object of the asynchronous callback describes the final result of speech recognition, including: Attribute getter Descriptions RecognitionResult.text The text recognized RecognitionResult.withUnderstandingResult The recognition result is processed by natural language. RecognitionResult.understandingResult The processing results by natural language For single recognition, if you don't want to build recognition options, you can also use an interface without parameters. The specific process is as follows: promise = speechManager . recognize (); If you want to know the current recognition status, use the following code. boolean isRecognizing = speechManager . isRecognizing (); If true is returned, it means that it is recognizing. Natural language processing When you need the device to have the ability to \"listen\" like a human, using natural language processing technology, the device can sense that the user is talking to it and then start interacting with the user. This process is realized by the following code. promise /* [2] */ = speechManager . understand ( \"How's the weather today?\" /* [1] */ ) . done ( new DoneCallback < UnderstandingResult > () { @Override public void onDone ( UnderstandingResult understandingResult /* [3] */ ) { // After the natural language processing is completed, it will return here. At this time, you can call the speech synthesis interface to talk with the user. } }). fail ( new FailCallback < UnderstandingException > () { @Override public void onFail ( UnderstandingException e ) { // if the natural language processing goes wrong, it will be run here } }); [1] Enter the text sentences for natural language processing, it supports multiple sentences with punctuations. [2] The asynchronous object of natural language processing is returned, through which the result can be obtained and the processing can be cancelled. See Promise for specific usage. [3] Returns the results of natural language processing, including: Attribute getter Descriptions UnderstandingResult.sessionId Session id, it will remain unchanged during multiple rounds of conversation. UnderstandingResult.source Service providers of natural language processing UnderstandingResult.inputText Texts that need to be understood UnderstandingResult.language System language UnderstandingResult.version The version of the library of the natural language processing service provider UnderstandingResult.sessionIncomplete false: the conversation is completed UnderstandingResult.contextList Context parameter UnderstandingResult.intent Intent, see SpeechIntent for details SpeechIntent.action Intent SpeechIntent.parameters Word slot SpeechIntent.score Matching degree UnderstandingResult.speechFulfillment The information broadcast after natural language processing, see SpeechFulfillment for details SpeechFulfillment.type Source of the recognition result SpeechFulfillment.text type = TYPE_TEXT SpeechFulfillment.uri type = TYPE_URI SpeechFulfillment.audio type = TYPE_AUDIO SpeechFulfillment.audioType audio format UnderstandingResult.fulfillmentList Detailed results for natural language processing By specifying options, you can adjust the behavior of natural language processing. UnderstandingOption /* [1] */ option = new UnderstandingOption . Builder ( \u201c How ' s the weather today ? \" ). [1] The UnderstandingOption object is constructed by UnderstandingOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor() Construction without parameters The natural language processing of text whose length is 0. Builder.constructor(inputText) During construction, you must specify the text information to perform natural language processing Builder.setSessionId(sessionId) Session id Zero-length string Builder.setVersion(version) The version of the library of the natural language processing service provider Zero-length string Builder.setParameters(parameters) Customized parameters JsonObjectString.EMPTY_OBJECT Builder.setContextList(contextList) Context parameter JsonArrayString.EMPTY_ARRAY Builder.setTimeoutMillis(timeoutMillis) Understanding the timeout length 0: no timeout Speech settings If you need to get or set the device's speech configuration, you can use the SpeechSettings object. SpeechSettings speechSettings = speechManager . speechSettings (); Methods Descriptions SpeechSettings.getConfiguration() Obtain language configuration information SpeechSettings.setConfiguration(configutation) Set language configuration information SpeechSettings.registerListener(listener) Register a speech configuration change monitor SpeechSettings.unregisterListener(listener) Unregister a speech configuration change monitor To set the speech-related configuration information, you can use the following code. SpeechConfiguration /* [1] */ configutation = new SpeechConfiguration . Builder (). build (); speechSettings . setConfiguration ( configutation ); [1] The speech configuration information is constructed by SpeechConfiguration.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor() A structure without parameters, the recognition mode will not be specified. Builder.setRecognitionMode(recognitionMode) Recognition mode 0: unknown recognition mode Builder.setSpeakingVoiceId(speakingVoiceId) speaker NULL Builder.setSpeakingSpeed(speakingSpeed) Speaking speed 0 Builder.setSpeakingVolume(speakingVolume) volume 0 Constants Descriptions SpeechConfiguration.RECOGNITION_MODE_UNKNOWN Unknown recognition mode SpeechConfiguration.RECOGNITION_MODE_SINGLE Single recognition mode SpeechConfiguration.RECOGNITION_MODE_CONTINUOUS Continuous recognition mode SpeechConfiguration.SPEAKING_VOLUME_MAX The maximum volume available SpeechConfiguration.SPEAKING_VOLUME_MIN The minimum volume available SpeechConfiguration.SPEAKING_SPEED_FAST Fast speech speed SpeechConfiguration.SPEAKING_SPEED_NORMAL Normal speech speed SpeechConfiguration.SPEAKING_SPEED_SLOW Slow speech speed To obtain the speech configuration information, use the following code: SpeechConfiguration configutation = speechSettings . getConfiguration (); If you need to perceive the speech configuration changes, use the following code: SpeechConfigurationListener listener = new SpeechConfigurationListener () { @Override public void onConfigurationChanged ( SpeechConfiguration speechConfiguration ) { // when the device's speech configuration changes, it will be run here } }; speechSettings . registerListener ( listener ); If you no longer need to perceive the speech configuration changes, use the following code: speechSettings . unregisterListener ( listener );","title":"Speech service"},{"location":"android/guide/system-service/speech.html#speech-service","text":"The speech service provides API with the ability to call the device\u2019s speech related capabilities, such as \"speak\", \"listen\", and \u201cunderstand\u201d. As the speech service access agent, SpeechManager provides the main API of the speech service and can be obtained through RobotContext . SpeechManager speechManager = aRobotContext . getSystemService ( SpeechManager . SERVICE ); In the field of speech technology, there are terms such as Keyword Spotting (KWS), Text to Speech (TTS), Automatic Speech Recognition (ASR), Natural Language Understanding (NLU), and Natural Language Processing (NLP). These terms refer to the subdivided technologies of the field of speech technology, such as speech service wake-up, text-to-speech, speech recognition, natural language understanding and processing. The speech service is realized using these technologies, and calling the SpeechManager API is actually a call to these technical capabilities. The following chapters will introduce them one by one.","title":"Speech service"},{"location":"android/guide/system-service/speech.html#speech-service-wake-up","text":"When the user speaks a wake-up word to the device, using the speech wake-up technology, the device can perceive that the user is \"calling it\", and then it can start interacting with the user. This process is called \"speech service wake-up\". If you need to perceive speech wake-up, use the following code. WakeUpListener wakeUpListener = new WakeUpListener () { @Override public void onWakingUp ( WakeUp wakeUp /* [1] */ ) { // if it is perceived that the user has said the wake-up word, it will run here } }; speechManager . registerWakeUpListener ( wakeUpListener ); [1] The WakeUp object describes the environmental information at the time of speech service wake-up, including: Attribute getter Descriptions WakeUp.when The timestamp of the wake-up time, unit: ms. WakeUp.voiceDirection The sound source of the wake-up time If you no longer need to perceive speech service wake-up, use the following code. speechManager . unregisterWakeUpListener ( wakeUpListener );","title":"Speech service wake-up "},{"location":"android/guide/system-service/speech.html#text-to-speech","text":"When you need the device to speak like a human, using the text-to-speech technology, text can be synthesized into human voices and played. This process is realized using the following code. promise /* [2] */ = speechManager . synthesize ( \"Hello, what can I do for you?\" /* [1] */ ) . progress ( new ProgressCallback < SynthesisProgress > () { @Override public void onProgress ( SynthesisProgress progress /* [3] */ ) { // The synthetic process will be run here multiple times } }). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // \u201cHello, what can I do for you?\u201d After saying that, it will be run here } }). fail ( new FailCallback < SynthesisException > () { @Override public void onFail ( SynthesisException e ) { // If the synthetic process goes wrong, it will be run here } }); [1] Enter the text sentences to be synthesized, it supports multiple sentences with punctuation. [2] The asynchronous object that is waiting for the progress and result of the synthesis is returned, through which you can wait for or monitor the progress and results, and cancel the synthesis process. See Promise for specific usage. [3] The SynthesisProgress object of the asynchronous callback describes the progress information of speech synthesis, including: Attribute getter Descriptions SynthesisProgress.remainingTimeMillis Remaining playback time SynthesisProgress.playProgress Playback progress SynthesisProgress.audioBytes Play the resource, some platforms may not provide this resource. By specifying options, you can adjust the behavior of speech synthesis. SynthesisOption /*[1]*/ option = new SynthesisOption . Builder ( \"Hello, what can I do for you?\" ). [1] The SynthesisOption object is constructed through SynthesisOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(inputText) When constructing, you must import synthetic texts, NULL objects or strings whose lengths are 0 are not allowed. Builder.setSpeakingSpeed(speakingSpeed) Set the speaking speed 0 Builder.setSpeakingVolume(speakingVolume) Set the speaking volume 0 Builder.setSpeakingVoiceId(speakingVoiceId) Set the speaker, if you want to know the speakers that the system has, please refer to the speech settings Zero-length string Constants Descriptions SynthesisOption.SPEAKING_VOLUME_MAX The maximum volume available SynthesisOption.SPEAKING_VOLUME_MIN The minimum volume available SynthesisOption.SPEAKING_SPEED_FAST Fast speech speed SynthesisOption.SPEAKING_SPEED_NORMAL Normal speech speed SynthesisOption.SPEAKING_SPEED_SLOW Slow speech speed If you want to know the current synthesis status, use the following code. boolean isSynthesizing = speechManager . isSynthesizing (); If true is returned, it means that the synthesis is in progress.","title":"Text-to-speech"},{"location":"android/guide/system-service/speech.html#speech-recognition","text":"When the user speaks to the device, the speech recognition technology can be used to convert speech into text or instructions. This process is realized using the following code. RecognitionOption /* [1] */ option = new RecognitionOption . Builder ( RecognitionOption . MODE_SINGLE ). build (); promise /* [2] */ = speechManager . recognize ( option ) . progress ( new ProgressCallback < RecognitionProgress > () { @Override public void onProgress ( RecognitionProgress recognitionProgress /* [3] */ ) { // the recognition progress will be run here multiple times } }). done ( new DoneCallback < RecognitionResult > () { @Override public void onDone ( RecognitionResult recognitionResult /* [4] */ ) { // if the recognition progress is completed, the final recognition result will be returned } }). fail ( new FailCallback < RecognitionException > () { @Override public void onFail ( RecognitionException e ) { // if the recognition progress goes wrong, it will be run here. } }); [1] Speech recognition options, which can be used to adjust the speech recognition behavior. It is built with RecognitionOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor(mode) You must specify a recognition mode when constructing. Builder.setDistanceRange(distanceRange) Recognition scenarios Near-field recognition Builder.setTimeoutMillis(timeoutMillis) Recognize the timeout length No timeout Builder.setUnderstandingOption(understandingOption) Natural language processing options NULL Builder.setExtension(extension) Customized field NULL Constants Descriptions RecognitionOption.MODE_SINGLE Single recognition: It is generally used to start speech recognition once after waking up. The recognition progress will be stopped after getting the result, and it will wait for the next wake up. RecognitionOption.MODE_CONTINUOUS Continuous recognition: Uninterrupted recognition after wake-up is started, the data is returned after recognition, and the recognition will be continued. RecognitionOption.DISTANCE_RANGE_NEAR_FIELD Near-field recognition RecognitionOption.DISTANCE_RANGE_FAR_FIELD Far-field recognition RecognitionOption.DISTANCE_RANGE_CLOSE_TALK Super near field recognition [2] The asynchronous object that is waiting for the recognition progress and results is returned, through which you can wait for or monitor the progress and results, and cancel the synthesis process. See Promise for specific usage. [3] The RecognitionProgress object of the asynchronous callback describes the progress information of speech recognition, including: Attribute getter Descriptions RecognitionProgress.decibel Decibels recognized RecognitionProgress.textResult Text converted from the recognized speech RecognitionProgress.understandingResult The result of the recognized speech after natural language processing. For details, see Natural Language Processing. RecognitionProgress.progress Recognition progress Constants Descriptions RecognitionResult.began Start of recognition RecognitionResult.ended End of recognition RecognitionProgress.PROGRESS_RECOGNIZING Recognizing RecognitionProgress.PROGRESS_RECOGNITION_TEXT_RESULT Text recognized RecognitionProgress.PROGRESS_UNDERSTANDING_RESULT In natural language processing [4] The RecognitionResult object of the asynchronous callback describes the final result of speech recognition, including: Attribute getter Descriptions RecognitionResult.text The text recognized RecognitionResult.withUnderstandingResult The recognition result is processed by natural language. RecognitionResult.understandingResult The processing results by natural language For single recognition, if you don't want to build recognition options, you can also use an interface without parameters. The specific process is as follows: promise = speechManager . recognize (); If you want to know the current recognition status, use the following code. boolean isRecognizing = speechManager . isRecognizing (); If true is returned, it means that it is recognizing.","title":"Speech recognition "},{"location":"android/guide/system-service/speech.html#natural-language-processing","text":"When you need the device to have the ability to \"listen\" like a human, using natural language processing technology, the device can sense that the user is talking to it and then start interacting with the user. This process is realized by the following code. promise /* [2] */ = speechManager . understand ( \"How's the weather today?\" /* [1] */ ) . done ( new DoneCallback < UnderstandingResult > () { @Override public void onDone ( UnderstandingResult understandingResult /* [3] */ ) { // After the natural language processing is completed, it will return here. At this time, you can call the speech synthesis interface to talk with the user. } }). fail ( new FailCallback < UnderstandingException > () { @Override public void onFail ( UnderstandingException e ) { // if the natural language processing goes wrong, it will be run here } }); [1] Enter the text sentences for natural language processing, it supports multiple sentences with punctuations. [2] The asynchronous object of natural language processing is returned, through which the result can be obtained and the processing can be cancelled. See Promise for specific usage. [3] Returns the results of natural language processing, including: Attribute getter Descriptions UnderstandingResult.sessionId Session id, it will remain unchanged during multiple rounds of conversation. UnderstandingResult.source Service providers of natural language processing UnderstandingResult.inputText Texts that need to be understood UnderstandingResult.language System language UnderstandingResult.version The version of the library of the natural language processing service provider UnderstandingResult.sessionIncomplete false: the conversation is completed UnderstandingResult.contextList Context parameter UnderstandingResult.intent Intent, see SpeechIntent for details SpeechIntent.action Intent SpeechIntent.parameters Word slot SpeechIntent.score Matching degree UnderstandingResult.speechFulfillment The information broadcast after natural language processing, see SpeechFulfillment for details SpeechFulfillment.type Source of the recognition result SpeechFulfillment.text type = TYPE_TEXT SpeechFulfillment.uri type = TYPE_URI SpeechFulfillment.audio type = TYPE_AUDIO SpeechFulfillment.audioType audio format UnderstandingResult.fulfillmentList Detailed results for natural language processing By specifying options, you can adjust the behavior of natural language processing. UnderstandingOption /* [1] */ option = new UnderstandingOption . Builder ( \u201c How ' s the weather today ? \" ). [1] The UnderstandingOption object is constructed by UnderstandingOption.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor() Construction without parameters The natural language processing of text whose length is 0. Builder.constructor(inputText) During construction, you must specify the text information to perform natural language processing Builder.setSessionId(sessionId) Session id Zero-length string Builder.setVersion(version) The version of the library of the natural language processing service provider Zero-length string Builder.setParameters(parameters) Customized parameters JsonObjectString.EMPTY_OBJECT Builder.setContextList(contextList) Context parameter JsonArrayString.EMPTY_ARRAY Builder.setTimeoutMillis(timeoutMillis) Understanding the timeout length 0: no timeout","title":"Natural language processing "},{"location":"android/guide/system-service/speech.html#speech-settings","text":"If you need to get or set the device's speech configuration, you can use the SpeechSettings object. SpeechSettings speechSettings = speechManager . speechSettings (); Methods Descriptions SpeechSettings.getConfiguration() Obtain language configuration information SpeechSettings.setConfiguration(configutation) Set language configuration information SpeechSettings.registerListener(listener) Register a speech configuration change monitor SpeechSettings.unregisterListener(listener) Unregister a speech configuration change monitor To set the speech-related configuration information, you can use the following code. SpeechConfiguration /* [1] */ configutation = new SpeechConfiguration . Builder (). build (); speechSettings . setConfiguration ( configutation ); [1] The speech configuration information is constructed by SpeechConfiguration.Builder , and the instructions are as follows: Methods Descriptions Default value Builder.constructor() A structure without parameters, the recognition mode will not be specified. Builder.setRecognitionMode(recognitionMode) Recognition mode 0: unknown recognition mode Builder.setSpeakingVoiceId(speakingVoiceId) speaker NULL Builder.setSpeakingSpeed(speakingSpeed) Speaking speed 0 Builder.setSpeakingVolume(speakingVolume) volume 0 Constants Descriptions SpeechConfiguration.RECOGNITION_MODE_UNKNOWN Unknown recognition mode SpeechConfiguration.RECOGNITION_MODE_SINGLE Single recognition mode SpeechConfiguration.RECOGNITION_MODE_CONTINUOUS Continuous recognition mode SpeechConfiguration.SPEAKING_VOLUME_MAX The maximum volume available SpeechConfiguration.SPEAKING_VOLUME_MIN The minimum volume available SpeechConfiguration.SPEAKING_SPEED_FAST Fast speech speed SpeechConfiguration.SPEAKING_SPEED_NORMAL Normal speech speed SpeechConfiguration.SPEAKING_SPEED_SLOW Slow speech speed To obtain the speech configuration information, use the following code: SpeechConfiguration configutation = speechSettings . getConfiguration (); If you need to perceive the speech configuration changes, use the following code: SpeechConfigurationListener listener = new SpeechConfigurationListener () { @Override public void onConfigurationChanged ( SpeechConfiguration speechConfiguration ) { // when the device's speech configuration changes, it will be run here } }; speechSettings . registerListener ( listener ); If you no longer need to perceive the speech configuration changes, use the following code: speechSettings . unregisterListener ( listener );","title":"Speech settings"},{"location":"android/tutorial/build your first app.html","text":"Build your first robot application Prepare the development environment Developing robot applications based on Cruzr is the same as developing ordinary Android applications. They both use Android Studio IDE for development. Please go to Android's official website to download and install Android Studio. Create a project Open the installed Android Studio, and refer to Android's official guide to create an Android application project. Add library dependencies Add a declaration of the shared library. Open the file app/src/main/AndroidManifest.xml and add a declaration of using the Cruzr shared library inside the <application> tag. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <application> <uses-library android:name= \"com.ubtrobot.rosa\" android:required= \"true\" /> </application> </manifest> Introduce the SDK package Get the Cruzr SDK (cruzr.jar), and creat the cruzr-libs directory in the app directory ,put the jar package in the cruzr-libs directory. Open file app/build.gradle and add the Cruzr library dependencies in the dependencies . allprojects { repositories { flatDir { dirs 'cruzr-libs' } } } dependencies { compileOnly files ( 'cruzr-libs/cruzr.jar' ) } Find the Sync Project with Gradle Files button on the Android Studio tool bar and click to perform a sync operation. After synchronizing, you can use the API in the Cruzr library. Note: The latest version of the Cruzr SDK is built into the system. Developers only use the downloaded SDK for compilation to ensure stable compatibility. Configure permissions To develop a robot application based on Cruzr, you need to declare the necessary permissions and Android application components in AndroidManifest. Open the file app/src/main/AndroidManifest.xml and add the permission to call the Cruzr API in the <manifest> tag. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <uses-permission android:name= \"com.ubtrobot.permission.ROBOT\" /> </manifest> SDK initialization Create a subclass of Application and set the class name to the [android: name[ attribute of the [ [tag of [AndroidManifest[ <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> ... <application android:name= \".YourApp\" > ... </application> </manifest> The initialization method to call Cruzr inside onCreate , which is the subclass of Application . public class YourApp extends Application { @Override public void onCreate () { super . onCreate (); Robot . initialize ( this ); } } Make the robot say \"Hello\" Cruzr provides the basic functions of multiple modules in the form of system services. Let's try to call the speech synthesis API of the voice service to make the robot say \"Hello\". Open the pre-created file app/src/main/java/.../MainActivity.java in the project and add the following code to the onCreate method: public class MainActivity extends AppCompatActivity { @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); SpeechManager speechManager = Robot . globalContext () . getSystemService ( SpeechManager . SERVICE ); speechManager . synthesize ( \"Hello\" ); } } To install and run the application, refer to the \"Run on a real device\" chapter of the [\"Run your app\"[ document in the official Android guide, and perform the corresponding operations to get your first robot application running on the robot. If everything runs smoothly, you will hear the robot say \"Hello\". Next step In the example above, when the robot application is started, the robot carries out the action of saying \"Hello\". In fact, it is the user's instructions that make the robot carry out the action. Skill is the component that Cruzr uses to receive user instructions and perform the corresponding actions. You should first be proficient in implementing a skill. Please refer to the create your first skill chapter.","title":"Build your first app"},{"location":"android/tutorial/build your first app.html#build-your-first-robot-application","text":"","title":"Build your first robot application"},{"location":"android/tutorial/build your first app.html#prepare-the-development-environment","text":"Developing robot applications based on Cruzr is the same as developing ordinary Android applications. They both use Android Studio IDE for development. Please go to Android's official website to download and install Android Studio.","title":"Prepare the development environment"},{"location":"android/tutorial/build your first app.html#create-a-project","text":"Open the installed Android Studio, and refer to Android's official guide to create an Android application project.","title":"Create a project"},{"location":"android/tutorial/build your first app.html#add-library-dependencies","text":"","title":"Add library dependencies"},{"location":"android/tutorial/build your first app.html#add-a-declaration-of-the-shared-library","text":"Open the file app/src/main/AndroidManifest.xml and add a declaration of using the Cruzr shared library inside the <application> tag. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <application> <uses-library android:name= \"com.ubtrobot.rosa\" android:required= \"true\" /> </application> </manifest>","title":"Add a declaration of the shared library."},{"location":"android/tutorial/build your first app.html#introduce-the-sdk-package","text":"Get the Cruzr SDK (cruzr.jar), and creat the cruzr-libs directory in the app directory ,put the jar package in the cruzr-libs directory. Open file app/build.gradle and add the Cruzr library dependencies in the dependencies . allprojects { repositories { flatDir { dirs 'cruzr-libs' } } } dependencies { compileOnly files ( 'cruzr-libs/cruzr.jar' ) } Find the Sync Project with Gradle Files button on the Android Studio tool bar and click to perform a sync operation. After synchronizing, you can use the API in the Cruzr library. Note: The latest version of the Cruzr SDK is built into the system. Developers only use the downloaded SDK for compilation to ensure stable compatibility.","title":"Introduce the SDK package"},{"location":"android/tutorial/build your first app.html#configure-permissions","text":"To develop a robot application based on Cruzr, you need to declare the necessary permissions and Android application components in AndroidManifest. Open the file app/src/main/AndroidManifest.xml and add the permission to call the Cruzr API in the <manifest> tag. <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <uses-permission android:name= \"com.ubtrobot.permission.ROBOT\" /> </manifest>","title":"Configure permissions"},{"location":"android/tutorial/build your first app.html#sdk-initialization","text":"Create a subclass of Application and set the class name to the [android: name[ attribute of the [ [tag of [AndroidManifest[ <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> ... <application android:name= \".YourApp\" > ... </application> </manifest> The initialization method to call Cruzr inside onCreate , which is the subclass of Application . public class YourApp extends Application { @Override public void onCreate () { super . onCreate (); Robot . initialize ( this ); } }","title":"SDK initialization"},{"location":"android/tutorial/build your first app.html#make-the-robot-say-hello","text":"Cruzr provides the basic functions of multiple modules in the form of system services. Let's try to call the speech synthesis API of the voice service to make the robot say \"Hello\". Open the pre-created file app/src/main/java/.../MainActivity.java in the project and add the following code to the onCreate method: public class MainActivity extends AppCompatActivity { @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); SpeechManager speechManager = Robot . globalContext () . getSystemService ( SpeechManager . SERVICE ); speechManager . synthesize ( \"Hello\" ); } } To install and run the application, refer to the \"Run on a real device\" chapter of the [\"Run your app\"[ document in the official Android guide, and perform the corresponding operations to get your first robot application running on the robot. If everything runs smoothly, you will hear the robot say \"Hello\".","title":"Make the robot say \"Hello\""},{"location":"android/tutorial/build your first app.html#next-step","text":"In the example above, when the robot application is started, the robot carries out the action of saying \"Hello\". In fact, it is the user's instructions that make the robot carry out the action. Skill is the component that Cruzr uses to receive user instructions and perform the corresponding actions. You should first be proficient in implementing a skill. Please refer to the create your first skill chapter.","title":"Next step"},{"location":"android/tutorial/create your first skill.html","text":"Create your first skill After you build your first app , you can start building skills. You can use the various system services of Cruzr in skills to customize the functions of the robot. The following example will implement a simple time-telling skill. The function of the skill is to broadcast the current time when triggered. Configure services In addition to adding library dependencies and configuration permissions (see build your first app ) to develop skills, you also need to configure skill services separately for the system startup skill. Open the file app/src/main/AndroidManifest.xml and add Bootstrap service in the <application> tag. <application> <service android:name= \"com.ubtrobot.app.Bootstrap\" android:exported= \"true\" /> </application> Create skills Create a TimerSkill class in the directory of the module package of the previously created robot application, and extends RobotSkill. public class TimerSkill extends RobotSkill { } At the same time, create a robot_manifest.xml file in the app/src/main/res/xml/ folder and add the following content. <manifest> <skill class= \"com.ubtrobot.demo.TimerSkill\" name= \"timer\" > </skill> </manifest> A skill named timer is configured in the above code, and its path is com.ubtrobot.demo.TimerSkill. Add command receiving method After creating a skill, you need to add command receiving method in the skill class to receive and process the commands. Add the timeIndicate method here, use the @OnDirective annotation, and configure an action attribute for it public class TimerSkill extends RobotSkill { @OnDirective ( action = \"timer.indicator\" ) public void timeIndicate ( Directive directive ) { } } Add the <directive /> command tag in the <skill /> tag of robot_manifest.xml , where the action needs to be the same as the action in the code above. <manifest> <skill class= \"com.ubtrobot.demo.TimerSkill\" name= \"timer\" > <directive action= \"timer.indicator\" /> </skill> </manifest> Implement the time-telling function Get the current time using the timeIndicate method and use the voice service to broadcast the time. public class TimerSkill extends RobotSkill { @OnDirective ( action = \"timer.indicator\" ) public void timeIndicate ( Directive directive ) { //Get the current time SimpleDateFormat simpleDateFormat = new SimpleDateFormat ( \"yyyyMMdd HHmm\" ); String timeStr = \"Now is\uff1a\" + simpleDateFormat . format ( new Date ()); //use the voice service to broadcast the time SpeechManager speechManager = getSystemService ( SpeechManager . SERVICE ); speechManager . synthesize ( timeStr ); } } Simulate command sending The skill has been completed, and the robot is now able to tell the time. So how do you use this function? Next we will simulate an command and send it to skill to start it. Add the following code in MainActivity : public class MainActivity extends AppCompatActivity { @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); //send command Robot . globalContext (). dispatchDirective ( \"timer.indicator\" ); } } Install and run the app and you will hear the robot broadcast the time for you. The above process demonstrated how to implement a simple skill. Skill still has many other functions. For details, please refer to skill . More services Above, we used the voice service to broadcast the time. We can also add other effects for time-telling, such as: Add lighting effects using lighting services LightManager lightManager = getSystemService ( LightManager . SERVICE ); lightManager . displayEffect ( lightId , effectUri ); Use the emotion service to display emojis EmotionManager emotionManager = getSystemService ( EmotionManager . SERVICE ); emotionManager . express ( emotionUri ); Use the servo service to add actions ServoManager servoManager = getSystemService ( ServoManager . SERVICE ); servoManager . rotate ( rotationOption ); In order to implement a robot skill, a lot of basic functions of the robot need to be called. Cruzr has many system services. You can use these services to implement various special function. For details, please see the System Services chapter.","title":"Create your first skill"},{"location":"android/tutorial/create your first skill.html#create-your-first-skill","text":"After you build your first app , you can start building skills. You can use the various system services of Cruzr in skills to customize the functions of the robot. The following example will implement a simple time-telling skill. The function of the skill is to broadcast the current time when triggered.","title":"Create your first skill"},{"location":"android/tutorial/create your first skill.html#configure-services","text":"In addition to adding library dependencies and configuration permissions (see build your first app ) to develop skills, you also need to configure skill services separately for the system startup skill. Open the file app/src/main/AndroidManifest.xml and add Bootstrap service in the <application> tag. <application> <service android:name= \"com.ubtrobot.app.Bootstrap\" android:exported= \"true\" /> </application>","title":"Configure services"},{"location":"android/tutorial/create your first skill.html#create-skills","text":"Create a TimerSkill class in the directory of the module package of the previously created robot application, and extends RobotSkill. public class TimerSkill extends RobotSkill { } At the same time, create a robot_manifest.xml file in the app/src/main/res/xml/ folder and add the following content. <manifest> <skill class= \"com.ubtrobot.demo.TimerSkill\" name= \"timer\" > </skill> </manifest> A skill named timer is configured in the above code, and its path is com.ubtrobot.demo.TimerSkill.","title":"Create skills"},{"location":"android/tutorial/create your first skill.html#add-command-receiving-method","text":"After creating a skill, you need to add command receiving method in the skill class to receive and process the commands. Add the timeIndicate method here, use the @OnDirective annotation, and configure an action attribute for it public class TimerSkill extends RobotSkill { @OnDirective ( action = \"timer.indicator\" ) public void timeIndicate ( Directive directive ) { } } Add the <directive /> command tag in the <skill /> tag of robot_manifest.xml , where the action needs to be the same as the action in the code above. <manifest> <skill class= \"com.ubtrobot.demo.TimerSkill\" name= \"timer\" > <directive action= \"timer.indicator\" /> </skill> </manifest>","title":"Add command receiving method"},{"location":"android/tutorial/create your first skill.html#implement-the-time-telling-function","text":"Get the current time using the timeIndicate method and use the voice service to broadcast the time. public class TimerSkill extends RobotSkill { @OnDirective ( action = \"timer.indicator\" ) public void timeIndicate ( Directive directive ) { //Get the current time SimpleDateFormat simpleDateFormat = new SimpleDateFormat ( \"yyyyMMdd HHmm\" ); String timeStr = \"Now is\uff1a\" + simpleDateFormat . format ( new Date ()); //use the voice service to broadcast the time SpeechManager speechManager = getSystemService ( SpeechManager . SERVICE ); speechManager . synthesize ( timeStr ); } }","title":"Implement the time-telling function"},{"location":"android/tutorial/create your first skill.html#simulate-command-sending","text":"The skill has been completed, and the robot is now able to tell the time. So how do you use this function? Next we will simulate an command and send it to skill to start it. Add the following code in MainActivity : public class MainActivity extends AppCompatActivity { @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); //send command Robot . globalContext (). dispatchDirective ( \"timer.indicator\" ); } } Install and run the app and you will hear the robot broadcast the time for you. The above process demonstrated how to implement a simple skill. Skill still has many other functions. For details, please refer to skill .","title":"Simulate command sending"},{"location":"android/tutorial/create your first skill.html#more-services","text":"Above, we used the voice service to broadcast the time. We can also add other effects for time-telling, such as: Add lighting effects using lighting services LightManager lightManager = getSystemService ( LightManager . SERVICE ); lightManager . displayEffect ( lightId , effectUri ); Use the emotion service to display emojis EmotionManager emotionManager = getSystemService ( EmotionManager . SERVICE ); emotionManager . express ( emotionUri ); Use the servo service to add actions ServoManager servoManager = getSystemService ( ServoManager . SERVICE ); servoManager . rotate ( rotationOption ); In order to implement a robot skill, a lot of basic functions of the robot need to be called. Cruzr has many system services. You can use these services to implement various special function. For details, please see the System Services chapter.","title":"More services"},{"location":"android/tutorial/overriding-speech-service.html","text":"Replace speech services Cruzr provides various system services, including speech services. When the speech service function provided by Cruzr cannot meet your requirements, developers can use the services they have implemented to replace the original services by overriding it. The speech services include three services: speech recognition, natural language processing, and speech synthesis. Each module can be overridden separately. Add library dependencies Introduce the Cruzr SDK To download and introduce the Cruzr SDK, please refer to build your first app Introduce Cruzr_speech_override.aar Replacing speech services requires an additional SDK (Cruzr_speech_override.aar). Put the downloaded dependency package into the app/libs/ directory and add the following dependencies in app/build.gradle : dependencies { implementation ( name: 'cruzr_speech_override' , ext: aar ' ) //Speech service override SDK } Finally, find the Sync Project with Gradle Files button on the Android Studio tool bar, and click it to perform the synchronization operation. Note: Overriding speech services requires both Cruzr.jar and Cruzr_speech_override.aar. There is a difference between the two dependency methods. Configure overriding rules The replaced voice service supports automatic switching in the specified languages. Create a decision_list.xml file in the app/src/main/res/xml directory and add the overriding configuration. When the system language is switched to the specified language, the specified module of the system's speech service is overridden, and the speech service module developed by the developer will be used. <?xml version=\"1.0\" encoding=\"utf-8\"?> <decision-list> <decision> <if> <!-- choose the languages to replace the speech service --> <language name= \"en-rUS\" /> </if> <then> <route-service> <!-- Overriding speech synthesis --> <parameter-group package= \"name of your application package\" service= \"speech.synthesis\" /> <!-- Overriding speech recognition --> <parameter-group package= \"name of your application package\" service= \"speech.recognition\" /> <!-- Overriding speech understanding --> <parameter-group package= \"name of your application package\" service= \"speech.understanding\" /> </route-service> </then> <if> <!-- choose the languages to replace the speech service --> <language name= \"zh-rCN\" /> </if> <then> <route-service> <!-- Overriding speech synthesis --> <parameter-group package= \"name of your application package\" service= \"speech.synthesis\" /> </route-service> </then> </decision> </decision-list> Note: The three modules of the speech service can be overridden independently or in multiples. In the example, the English environment overrode four modules at the same time, and the Chinese environment only overrode the speech synthesis module. The language configuration must be configured according to the Android resource's standard language format. Develop customized speech service Implement the service Since you want to replace the original speech service, it is necessary to implement a customized speech service. After configuring the service replacing rules, the services in the configuration need to be implemented concretely. The following code takes Text-to-sound(TTS) service as an example: public class YourSynthesisServices extends AbstractSynthesizer { private GoogleSpeaker googleSpeaker ; public YourSynthesisServices () { googleSpeaker = new GoogleSpeaker (); } @Override protected void startSynthesizing ( SynthesisOption synthesisOption ) { // the new speech broadcast is realized googleSpeaker . speak ( synthesisOption . getInputText (), new TtsCallBack () { @Override public void onSpeakStart () { // the broadcast is started, the progress change will be notified reportSynthesizingProgress ( new SynthesisProgress . Builder ( SynthesisProgress . PROGRESS_BEGAN ). build ()); } @Override public void onSpeaking () { // broadcasting, the broadcast is started, the progress change will be notified reportSynthesizingProgress ( new SynthesisProgress . Builder ( SynthesisProgress . PROGRESS_PLAYING ) // broadcast status . setRemainingTimeMillis ( 3000 ) // time remaining of the broadcast . setPlayProgress ( 50 ) // current progress of the broadcast . build ()); } @Override public void onSpeakError ( Exception e ) { // broadcast error rejectSynthesizing ( new SynthesisException ( SynthesisException . CODE_INTERNAL_ERROR )); } @Override public void onSpeakCompleted () { // broadcast completed reportSynthesizingProgress ( new SynthesisProgress . Builder ( SynthesisProgress . PROGRESS_ENDED ). build ()); resolveSynthesizing (); } }); } @Override protected void stopSynthesizing () { // cancel the broadcast googleSpeaker . stop (); } @Override public List < SpeakingVoice > getSpeakingVoiceList () { // get the speaker's information SpeakingVoice speaker = new SpeakingVoice . Builder ( \"google\" ) . setLanguage ( \"en\" ) . build (); return Collections . singletonList ( speaker ); } } Declare service After implementing the speech service, we need one more step, which is declarative Services. Declare the service we implemented in the onCreate() method of Application class of the project. public class FooApplication extends Application { @Override public void onCreate () { //TODO Initialization service ServiceModules . initialize ( this ); //TODO Declare speech recognition service ServiceModules . declare ( Synthesizer . class , new ModuleCreator < Synthesizer > () { @Override public void createModule ( Class < Synthesizer > aClass , ModuleCreatedNotifier < Synthesizer > notifier ) { notifier . notifyModuleCreated ( new FooServices ()); //Provide an example of the service } } ); } } Finally, restart the system to use the overriding speech service.","title":"Overriding-speech-service"},{"location":"android/tutorial/overriding-speech-service.html#replace-speech-services","text":"Cruzr provides various system services, including speech services. When the speech service function provided by Cruzr cannot meet your requirements, developers can use the services they have implemented to replace the original services by overriding it. The speech services include three services: speech recognition, natural language processing, and speech synthesis. Each module can be overridden separately.","title":"Replace speech services"},{"location":"android/tutorial/overriding-speech-service.html#add-library-dependencies","text":"","title":"Add library dependencies"},{"location":"android/tutorial/overriding-speech-service.html#introduce-the-cruzr-sdk","text":"To download and introduce the Cruzr SDK, please refer to build your first app","title":"Introduce the Cruzr SDK"},{"location":"android/tutorial/overriding-speech-service.html#introduce-cruzr_speech_overrideaar","text":"Replacing speech services requires an additional SDK (Cruzr_speech_override.aar). Put the downloaded dependency package into the app/libs/ directory and add the following dependencies in app/build.gradle : dependencies { implementation ( name: 'cruzr_speech_override' , ext: aar ' ) //Speech service override SDK } Finally, find the Sync Project with Gradle Files button on the Android Studio tool bar, and click it to perform the synchronization operation. Note: Overriding speech services requires both Cruzr.jar and Cruzr_speech_override.aar. There is a difference between the two dependency methods.","title":"Introduce Cruzr_speech_override.aar"},{"location":"android/tutorial/overriding-speech-service.html#configure-overriding-rules","text":"The replaced voice service supports automatic switching in the specified languages. Create a decision_list.xml file in the app/src/main/res/xml directory and add the overriding configuration. When the system language is switched to the specified language, the specified module of the system's speech service is overridden, and the speech service module developed by the developer will be used. <?xml version=\"1.0\" encoding=\"utf-8\"?> <decision-list> <decision> <if> <!-- choose the languages to replace the speech service --> <language name= \"en-rUS\" /> </if> <then> <route-service> <!-- Overriding speech synthesis --> <parameter-group package= \"name of your application package\" service= \"speech.synthesis\" /> <!-- Overriding speech recognition --> <parameter-group package= \"name of your application package\" service= \"speech.recognition\" /> <!-- Overriding speech understanding --> <parameter-group package= \"name of your application package\" service= \"speech.understanding\" /> </route-service> </then> <if> <!-- choose the languages to replace the speech service --> <language name= \"zh-rCN\" /> </if> <then> <route-service> <!-- Overriding speech synthesis --> <parameter-group package= \"name of your application package\" service= \"speech.synthesis\" /> </route-service> </then> </decision> </decision-list> Note: The three modules of the speech service can be overridden independently or in multiples. In the example, the English environment overrode four modules at the same time, and the Chinese environment only overrode the speech synthesis module. The language configuration must be configured according to the Android resource's standard language format.","title":"Configure overriding rules"},{"location":"android/tutorial/overriding-speech-service.html#develop-customized-speech-service","text":"","title":"Develop customized speech service"},{"location":"android/tutorial/overriding-speech-service.html#implement-the-service","text":"Since you want to replace the original speech service, it is necessary to implement a customized speech service. After configuring the service replacing rules, the services in the configuration need to be implemented concretely. The following code takes Text-to-sound(TTS) service as an example: public class YourSynthesisServices extends AbstractSynthesizer { private GoogleSpeaker googleSpeaker ; public YourSynthesisServices () { googleSpeaker = new GoogleSpeaker (); } @Override protected void startSynthesizing ( SynthesisOption synthesisOption ) { // the new speech broadcast is realized googleSpeaker . speak ( synthesisOption . getInputText (), new TtsCallBack () { @Override public void onSpeakStart () { // the broadcast is started, the progress change will be notified reportSynthesizingProgress ( new SynthesisProgress . Builder ( SynthesisProgress . PROGRESS_BEGAN ). build ()); } @Override public void onSpeaking () { // broadcasting, the broadcast is started, the progress change will be notified reportSynthesizingProgress ( new SynthesisProgress . Builder ( SynthesisProgress . PROGRESS_PLAYING ) // broadcast status . setRemainingTimeMillis ( 3000 ) // time remaining of the broadcast . setPlayProgress ( 50 ) // current progress of the broadcast . build ()); } @Override public void onSpeakError ( Exception e ) { // broadcast error rejectSynthesizing ( new SynthesisException ( SynthesisException . CODE_INTERNAL_ERROR )); } @Override public void onSpeakCompleted () { // broadcast completed reportSynthesizingProgress ( new SynthesisProgress . Builder ( SynthesisProgress . PROGRESS_ENDED ). build ()); resolveSynthesizing (); } }); } @Override protected void stopSynthesizing () { // cancel the broadcast googleSpeaker . stop (); } @Override public List < SpeakingVoice > getSpeakingVoiceList () { // get the speaker's information SpeakingVoice speaker = new SpeakingVoice . Builder ( \"google\" ) . setLanguage ( \"en\" ) . build (); return Collections . singletonList ( speaker ); } }","title":"Implement the service"},{"location":"android/tutorial/overriding-speech-service.html#declare-service","text":"After implementing the speech service, we need one more step, which is declarative Services. Declare the service we implemented in the onCreate() method of Application class of the project. public class FooApplication extends Application { @Override public void onCreate () { //TODO Initialization service ServiceModules . initialize ( this ); //TODO Declare speech recognition service ServiceModules . declare ( Synthesizer . class , new ModuleCreator < Synthesizer > () { @Override public void createModule ( Class < Synthesizer > aClass , ModuleCreatedNotifier < Synthesizer > notifier ) { notifier . notifyModuleCreated ( new FooServices ()); //Provide an example of the service } } ); } } Finally, restart the system to use the overriding speech service.","title":"Declare service"},{"location":"android/tutorial/recognize understand and distribute voice instructions.html","text":"Recognize, understand and distribute voice Commands The robot has skills such as \"dancing\", \"singing\", and \"navigating\". The user can trigger the corresponding skills by speaking to the robot (sending voice commands). For example, if the user says \"Dance for me\", the robot will start dancing. From the user sending an instruction to the robot carrying out the instruction, the robot's working steps are as follows: Recognition: The robot notices that a user is talking to it and recognizes what the user is saying Understand: To understand the meaning of the user\u2019s words and translate them into the corresponding instructions Distributing instructions: To distribute instructions through a certain mechanism. Execute the instruction. The above steps are implemented through the following code: Preparations Preparations include setting up a development environment, creating a project, adding library dependencies, etc. For details, see build your first app . Recognition To enable the robot's \"listening\" skills, you can use the speech recognition or speech wake-up technology. To use speech recognition or speech wake-up technology, you need to access the API provided by the SpeechManager proxy object through the speech service. For creating the SpeechManager object, see speech service . The API used in the identification process comes from the speech service. The detailed information such as parameters, interfaces, and proprietary names that appear in this implementation process can be found through the speech service . To use speech recognition technology to enable the robot's \"listening\", you can use the following code. speechManager . recognize ( new RecognitionOption . Builder ( RecognitionOption . MODE_CONTINUOUS ). build ()) . progress ( new ProgressCallback < RecognitionProgress > () { @Override public void onProgress ( RecognitionProgress recognitionProgress ) { if ( recognitionProgress . getProgress () . equals ( RecognitionProgress . PROGRESS_RECOGNITION_TEXT_RESULT )) { String asr = recognitionProgress . getTextResult (); understand ( asr ); // execute the understanding operation } } }) . fail ( new FailCallback < RecognitionException > () { @Override public void onFail ( RecognitionException e ) { // if the listening goes wrong, it will run here } }); To use speech wake-up technology to enable the robot's \"listening\" skill, you can use the following code. speechManager . registerWakeUpListener ( new WakeUpListener () { @Override public void onWakingUp ( WakeUp wakeUp ) { // After the robot perceives to be awakened, it can perform the following tasks: adjust posture (facing the user), enable recognition // enable identification speechManager . recognize ( new RecognitionOption . Builder ( RecognitionOption . MODE_SINGLE ). build ()) . done ( new DoneCallback < RecognitionResult > () { @Override public void onDone ( RecognitionResult recognitionResult ) { if ( ! recognitionResult . withUnderstandingResult ()) { String inputText = recognitionResult . getText (); understand ( inputText ); // execute the understanding operation } } }) . fail ( new FailCallback < RecognitionException > () { @Override public void onFail ( RecognitionException e ) { // if the listening goes wrong, it will run here } }); } }); If the robot's \"listening\" skill is turned off, the user needs to use a non-speech method (click the button) to trigger it. If using wake-up technology, the user can turn on the listening skill through wake-up words. Understanding If the robot wants to understand what the user says, it can use natural language processing technology. The code is presented below. public Promise < UnderstandingResult , UnderstandingException > understand ( String inputText ) { return speechManager . understand ( inputText ) . done ( new DoneCallback < UnderstandingResult > () { @Override public void onDone ( UnderstandingResult understandingResult ) { String action = understandingResult . getIntent (). getAction (); dispatchDirective ( action ); // execute the operation of distributing orders } }). fail ( new FailCallback < UnderstandingException > () { @Override public void onFail ( UnderstandingException e ) { // if the listening goes wrong, it will run here } }); } Distribute instructions public Promise < Void , DispatchException > dispatchDirective ( String action ) { return Robot . globalContext (). dispatchDirective ( action ). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // if the instruction distribution is completed, it will run here } }). fail ( new FailCallback < DispatchException > () { @Override public void onFail ( DispatchException e ) { // if the instruction distribution goes wrong, it will run here } }); } Execute instructions The application that executes an instruction is called a skill application. It is mainly used to receive an instruction. After receiving the instruction, it can be executed by calling the corresponding system service . The skill application and the instruction distribution application can be the same application or different applications. If they are different applications, you need to create them. To create applications, please refer to build your first app . To realize the skills, please refer to skills .","title":"Recognize understand and distribute voice instructions"},{"location":"android/tutorial/recognize understand and distribute voice instructions.html#recognize-understand-and-distribute-voice-commands","text":"The robot has skills such as \"dancing\", \"singing\", and \"navigating\". The user can trigger the corresponding skills by speaking to the robot (sending voice commands). For example, if the user says \"Dance for me\", the robot will start dancing. From the user sending an instruction to the robot carrying out the instruction, the robot's working steps are as follows: Recognition: The robot notices that a user is talking to it and recognizes what the user is saying Understand: To understand the meaning of the user\u2019s words and translate them into the corresponding instructions Distributing instructions: To distribute instructions through a certain mechanism. Execute the instruction. The above steps are implemented through the following code:","title":"Recognize, understand and distribute voice Commands"},{"location":"android/tutorial/recognize understand and distribute voice instructions.html#preparations","text":"Preparations include setting up a development environment, creating a project, adding library dependencies, etc. For details, see build your first app .","title":"Preparations"},{"location":"android/tutorial/recognize understand and distribute voice instructions.html#recognition","text":"To enable the robot's \"listening\" skills, you can use the speech recognition or speech wake-up technology. To use speech recognition or speech wake-up technology, you need to access the API provided by the SpeechManager proxy object through the speech service. For creating the SpeechManager object, see speech service . The API used in the identification process comes from the speech service. The detailed information such as parameters, interfaces, and proprietary names that appear in this implementation process can be found through the speech service . To use speech recognition technology to enable the robot's \"listening\", you can use the following code. speechManager . recognize ( new RecognitionOption . Builder ( RecognitionOption . MODE_CONTINUOUS ). build ()) . progress ( new ProgressCallback < RecognitionProgress > () { @Override public void onProgress ( RecognitionProgress recognitionProgress ) { if ( recognitionProgress . getProgress () . equals ( RecognitionProgress . PROGRESS_RECOGNITION_TEXT_RESULT )) { String asr = recognitionProgress . getTextResult (); understand ( asr ); // execute the understanding operation } } }) . fail ( new FailCallback < RecognitionException > () { @Override public void onFail ( RecognitionException e ) { // if the listening goes wrong, it will run here } }); To use speech wake-up technology to enable the robot's \"listening\" skill, you can use the following code. speechManager . registerWakeUpListener ( new WakeUpListener () { @Override public void onWakingUp ( WakeUp wakeUp ) { // After the robot perceives to be awakened, it can perform the following tasks: adjust posture (facing the user), enable recognition // enable identification speechManager . recognize ( new RecognitionOption . Builder ( RecognitionOption . MODE_SINGLE ). build ()) . done ( new DoneCallback < RecognitionResult > () { @Override public void onDone ( RecognitionResult recognitionResult ) { if ( ! recognitionResult . withUnderstandingResult ()) { String inputText = recognitionResult . getText (); understand ( inputText ); // execute the understanding operation } } }) . fail ( new FailCallback < RecognitionException > () { @Override public void onFail ( RecognitionException e ) { // if the listening goes wrong, it will run here } }); } }); If the robot's \"listening\" skill is turned off, the user needs to use a non-speech method (click the button) to trigger it. If using wake-up technology, the user can turn on the listening skill through wake-up words.","title":"Recognition"},{"location":"android/tutorial/recognize understand and distribute voice instructions.html#understanding","text":"If the robot wants to understand what the user says, it can use natural language processing technology. The code is presented below. public Promise < UnderstandingResult , UnderstandingException > understand ( String inputText ) { return speechManager . understand ( inputText ) . done ( new DoneCallback < UnderstandingResult > () { @Override public void onDone ( UnderstandingResult understandingResult ) { String action = understandingResult . getIntent (). getAction (); dispatchDirective ( action ); // execute the operation of distributing orders } }). fail ( new FailCallback < UnderstandingException > () { @Override public void onFail ( UnderstandingException e ) { // if the listening goes wrong, it will run here } }); }","title":"Understanding"},{"location":"android/tutorial/recognize understand and distribute voice instructions.html#distribute-instructions","text":"public Promise < Void , DispatchException > dispatchDirective ( String action ) { return Robot . globalContext (). dispatchDirective ( action ). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { // if the instruction distribution is completed, it will run here } }). fail ( new FailCallback < DispatchException > () { @Override public void onFail ( DispatchException e ) { // if the instruction distribution goes wrong, it will run here } }); }","title":"Distribute instructions"},{"location":"android/tutorial/recognize understand and distribute voice instructions.html#execute-instructions","text":"The application that executes an instruction is called a skill application. It is mainly used to receive an instruction. After receiving the instruction, it can be executed by calling the corresponding system service . The skill application and the instruction distribution application can be the same application or different applications. If they are different applications, you need to create them. To create applications, please refer to build your first app . To realize the skills, please refer to skills .","title":"Execute instructions"},{"location":"cruzr/leisure.html","text":"Leisure Service\uff08idle service\uff09 Generally speaking, if no one interacts with the robot for a long time, the robot will enter the leisure state. If you want the robot to perform some specific tasks in \"leisure\" time, such as greeting, cruising or playing advertisements, the leisure service can help you implement and manage the relationship between these tasks. Leisure\uff08Idle\uff09 state\uff1a We define the situation that the robot has no skill running state and there is no object within 1.3 meters ahead as \"leisure state\" . Leisure \uff08Idle\uff09 task\uff1a We define the situation that the robot starts task such as greeting\uff0ccruising or playing advertisements in leisure state as \"leisure task\" . Management Rules Set tasks with short leisure time will execute first. The high priority task is executed first, while the leisure time is the same . Tasks with the same leisure time and priority will be executed at the same time. As shown in the figure above, A, B, C, D and E represent all leisure tasks registered by the robot. T represents the time of each task entering, P represents the priority level. Leisure task management arranges these tasks into a new task list. Leisure Task|Break leisure rule Break leisure tasks: If the current task is \u2018break leisure task\u2019 , it will wake up the robot, reschedule, and will not enter the next task . Non breaking leisure tasks :If the current task is \u2018non breaking leisure task\u2019, T\u3001the task list will be executed in turn until the wake-up event interrupts the leisure state. As shown in the figure above, after the break leisure task is executed, the leisure state recalculates the time, and the subsequent tasks can not be executed, as shown in the red part of the figure above . Leisure Task|Creating Configure leisure tasks and create XML folders in the app directory . Create the robot_leisure.xml file under the XML folder. The contents of the file are as follows, the key , action and skillname can not be empty, they are case sensitive. The leisure service will automatically scan all configured applications and manage them uniformly. xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <leisures> <task action=\"com/leisure/test1\" key=\"task1\" open=\"true\" priority=\"5\" skillName=\"test1\" time=\"5\" wakeup=\"false\" > </task> </leisures> </manifest> API Interface of Leisure Service Class LeisureManager java.lang.Object com.ubtechinc.cruzr.sys.cruzrleisure.leisure.LeisureManager Methods Description enableLeisure(String key, boolean enable) key:Key value of leisure task enable: true\uff1aopen\uff0cfalse\uff1aclose Open or closed leisure tasks getLeisure(String key) key:key value of leisure task Get the Key's leisure tasks getLeisures() Get all currently registered leisure tasks init(Context context) Initialize the leisure service API. Be sure to initialize it before using the API init(Context context, IinitListener listener) context:context listener:Callback initialization status Initialize the leisure service API. Be sure to initialize it before using the API isConnected() Determine whether the current service is connected. prohibitedLeisure() It is forbidden for the robot to enter the leisure time. After calling the prohibitedleisme() method, the robot will not calculate the leisure time until it calls unProhibitedLeisure method to unlock. unProhibitedLeisure() Unprohibit leisure state of robot \uff0cthe robot recalculates the leisure time after being unprohibted. update(LeisureOptions option) option:Please see table2 for parameters Update the leisure task attribute. For details, please refer to the description above. wakeup() Interrupt the leisure time, and recalculate the leisure time after calling. Table 1 Class LeisureOptions java.lang.Object com.ubtechinc.cruzr.sys.cruzrleisure.entity.LeisureOptions Type Parameter Parameter Description Must Fill boolean isOpen true: current leisure task enabled false: current leisure task disabled. no boolean isWakeup When the current leisure task is started, it will be in the wake-up state. The robot will recalculate the leisure time and will not enter the next leisure task . false: The current task will not break leisure state, the next leisure task can continue to execute. yes String key Unique identification of leisure tasks. An app can create multiple leisure tasks. no int priority The priority of leisure tasks. If the leisure time is the same, only the leisure tasks with higher priority will be started. If the time and priority are the same, they will be started at the same time. yes long time Length of time to enter leisure task, unit:s. no Table2 Interface Instruction Initialize with onCreate method of Appcation when using interface . LeisureManager . get (). init ( this , new IinitListener () { @Override public void onInit () { Log . i ( \"leisure\" , \"successfully initialize\" ); } }); } The core interface of leisure service is update(LeisureOptions options)\uff0cIt can update the properties of leisure tasks through this interface \uff0cSuch as starting or stopping tasks, changing the time and priority of entering tasks, etc . For example, set the entry time of \"task1\" leisure task to 20s, and close the task . As follows: LeisureOptions options1 = new LeisureOptions . Builder (). key ( \"task1\" ). time ( 20 ). open ( false ). build (); LeisureManager . get (). update ( options1 ). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { Log . i ( \"leisure\" , \"update success\" ); } }). fail ( new FailCallback < Exception > () { @Override public void onFail ( Exception e ) { Log . e ( \"leisure\" , \"update failed-> \" + e . getMessage ()); } });","title":"Leisure service"},{"location":"cruzr/leisure.html#leisure-serviceidle-service","text":"Generally speaking, if no one interacts with the robot for a long time, the robot will enter the leisure state. If you want the robot to perform some specific tasks in \"leisure\" time, such as greeting, cruising or playing advertisements, the leisure service can help you implement and manage the relationship between these tasks. Leisure\uff08Idle\uff09 state\uff1a We define the situation that the robot has no skill running state and there is no object within 1.3 meters ahead as \"leisure state\" . Leisure \uff08Idle\uff09 task\uff1a We define the situation that the robot starts task such as greeting\uff0ccruising or playing advertisements in leisure state as \"leisure task\" .","title":"Leisure  Service\uff08idle service\uff09"},{"location":"cruzr/leisure.html#management-rules","text":"Set tasks with short leisure time will execute first. The high priority task is executed first, while the leisure time is the same . Tasks with the same leisure time and priority will be executed at the same time. As shown in the figure above, A, B, C, D and E represent all leisure tasks registered by the robot. T represents the time of each task entering, P represents the priority level. Leisure task management arranges these tasks into a new task list.","title":"Management Rules"},{"location":"cruzr/leisure.html#leisure-taskbreak-leisure-rule","text":"Break leisure tasks: If the current task is \u2018break leisure task\u2019 , it will wake up the robot, reschedule, and will not enter the next task . Non breaking leisure tasks :If the current task is \u2018non breaking leisure task\u2019, T\u3001the task list will be executed in turn until the wake-up event interrupts the leisure state. As shown in the figure above, after the break leisure task is executed, the leisure state recalculates the time, and the subsequent tasks can not be executed, as shown in the red part of the figure above .","title":"Leisure Task|Break leisure rule"},{"location":"cruzr/leisure.html#leisure-taskcreating","text":"Configure leisure tasks and create XML folders in the app directory . Create the robot_leisure.xml file under the XML folder. The contents of the file are as follows, the key , action and skillname can not be empty, they are case sensitive. The leisure service will automatically scan all configured applications and manage them uniformly. xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest> <leisures> <task action=\"com/leisure/test1\" key=\"task1\" open=\"true\" priority=\"5\" skillName=\"test1\" time=\"5\" wakeup=\"false\" > </task> </leisures> </manifest>","title":"Leisure Task|Creating"},{"location":"cruzr/leisure.html#api-interface-of-leisure-service","text":"Class LeisureManager java.lang.Object com.ubtechinc.cruzr.sys.cruzrleisure.leisure.LeisureManager Methods Description enableLeisure(String key, boolean enable) key:Key value of leisure task enable: true\uff1aopen\uff0cfalse\uff1aclose Open or closed leisure tasks getLeisure(String key) key:key value of leisure task Get the Key's leisure tasks getLeisures() Get all currently registered leisure tasks init(Context context) Initialize the leisure service API. Be sure to initialize it before using the API init(Context context, IinitListener listener) context:context listener:Callback initialization status Initialize the leisure service API. Be sure to initialize it before using the API isConnected() Determine whether the current service is connected. prohibitedLeisure() It is forbidden for the robot to enter the leisure time. After calling the prohibitedleisme() method, the robot will not calculate the leisure time until it calls unProhibitedLeisure method to unlock. unProhibitedLeisure() Unprohibit leisure state of robot \uff0cthe robot recalculates the leisure time after being unprohibted. update(LeisureOptions option) option:Please see table2 for parameters Update the leisure task attribute. For details, please refer to the description above. wakeup() Interrupt the leisure time, and recalculate the leisure time after calling. Table 1 Class LeisureOptions java.lang.Object com.ubtechinc.cruzr.sys.cruzrleisure.entity.LeisureOptions Type Parameter Parameter Description Must Fill boolean isOpen true: current leisure task enabled false: current leisure task disabled. no boolean isWakeup When the current leisure task is started, it will be in the wake-up state. The robot will recalculate the leisure time and will not enter the next leisure task . false: The current task will not break leisure state, the next leisure task can continue to execute. yes String key Unique identification of leisure tasks. An app can create multiple leisure tasks. no int priority The priority of leisure tasks. If the leisure time is the same, only the leisure tasks with higher priority will be started. If the time and priority are the same, they will be started at the same time. yes long time Length of time to enter leisure task, unit:s. no Table2","title":"API Interface of  Leisure Service"},{"location":"cruzr/leisure.html#interface-instruction","text":"Initialize with onCreate method of Appcation when using interface . LeisureManager . get (). init ( this , new IinitListener () { @Override public void onInit () { Log . i ( \"leisure\" , \"successfully initialize\" ); } }); } The core interface of leisure service is update(LeisureOptions options)\uff0cIt can update the properties of leisure tasks through this interface \uff0cSuch as starting or stopping tasks, changing the time and priority of entering tasks, etc . For example, set the entry time of \"task1\" leisure task to 20s, and close the task . As follows: LeisureOptions options1 = new LeisureOptions . Builder (). key ( \"task1\" ). time ( 20 ). open ( false ). build (); LeisureManager . get (). update ( options1 ). done ( new DoneCallback < Void > () { @Override public void onDone ( Void aVoid ) { Log . i ( \"leisure\" , \"update success\" ); } }). fail ( new FailCallback < Exception > () { @Override public void onFail ( Exception e ) { Log . e ( \"leisure\" , \"update failed-> \" + e . getMessage ()); } });","title":"Interface Instruction"},{"location":"cruzr/multimedia.html","text":"Multimedia Service Developers can easily call the system player to play local audio and video through multimedia services. Supported Audio formats\uff1amp3 Supported Video formats\uff1amp4\uff0cwmv\uff0cmov\uff0cavi Interface Description Methods Explanation Parameters (all required) init(Context context) Initialize before using other interfaces. context: Application context playMusic(String musicPath, boolean isShowControlView, String comeFrom) Play local music musicPath: Local music path isShowControlView: Show controls or not comeFrom: Call source playVideo(String videoPath, boolean isShowControlView, String comeFrom) Play local video videoPath: Local video path isShowControlView\uff1aShow controls or not comeFrom \uff1aCall source stopVideo() Stop playing video stopMusic() Stop playing music addVideoStatusListener(VideoStatusListener videoStatusListener) Add video playing status listener VideoStatusListener: removeVideoStatusListener(VideoStatusListener videoStatusListener) Remove video status listener VideoStatusListener addMusicStatusListener(MusicStatusListener musicStatusListener) Add music playing status listener MusicStatusListener removeMusicStatusListener(MusicStatusListener musicStatusListener) Remove music status listener MusicStatusListener Sample MediaPlayManager . getInstance (). init ( getApplicationContext ()); MediaPlayManager . getInstance (). playMusic ( \"/storage/emulated/0/Music/hao.mp3\" , false , \"danceApp\" ); MediaPlayManager . getInstance (). playVideo ( \"/storage/emulated/0/Movies/1.mp4\" , false , \"danceApp\" ); MediaPlayManager . getInstance (). stopVideo (); MediaPlayManager . getInstance (). addMusicStatusListener ( new MusicStatusListener () { @Override public void onMusicStart () { } @Override public void onMusicComplete () { } @Override public void onMusicStop () { } } );","title":"Multimedia service"},{"location":"cruzr/multimedia.html#multimedia-service","text":"Developers can easily call the system player to play local audio and video through multimedia services. Supported Audio formats\uff1amp3 Supported Video formats\uff1amp4\uff0cwmv\uff0cmov\uff0cavi","title":"Multimedia Service"},{"location":"cruzr/multimedia.html#interface-description","text":"Methods Explanation Parameters (all required) init(Context context) Initialize before using other interfaces. context: Application context playMusic(String musicPath, boolean isShowControlView, String comeFrom) Play local music musicPath: Local music path isShowControlView: Show controls or not comeFrom: Call source playVideo(String videoPath, boolean isShowControlView, String comeFrom) Play local video videoPath: Local video path isShowControlView\uff1aShow controls or not comeFrom \uff1aCall source stopVideo() Stop playing video stopMusic() Stop playing music addVideoStatusListener(VideoStatusListener videoStatusListener) Add video playing status listener VideoStatusListener: removeVideoStatusListener(VideoStatusListener videoStatusListener) Remove video status listener VideoStatusListener addMusicStatusListener(MusicStatusListener musicStatusListener) Add music playing status listener MusicStatusListener removeMusicStatusListener(MusicStatusListener musicStatusListener) Remove music status listener MusicStatusListener","title":"Interface Description"},{"location":"cruzr/multimedia.html#sample","text":"MediaPlayManager . getInstance (). init ( getApplicationContext ()); MediaPlayManager . getInstance (). playMusic ( \"/storage/emulated/0/Music/hao.mp3\" , false , \"danceApp\" ); MediaPlayManager . getInstance (). playVideo ( \"/storage/emulated/0/Movies/1.mp4\" , false , \"danceApp\" ); MediaPlayManager . getInstance (). stopVideo (); MediaPlayManager . getInstance (). addMusicStatusListener ( new MusicStatusListener () { @Override public void onMusicStart () { } @Override public void onMusicComplete () { } @Override public void onMusicStop () { } } );","title":"Sample"},{"location":"cruzr/setting.html","text":"System settings The Cruzr robot has some system setting parameters, which are often used by users, such as robot serial number, enterprise number, etc. Now, the methods will be provided. Get Setting Data The following methods are called to get the setting state data of the robot . String value = SettingValueFetcher.getStringValue(context,key,defaultValue); The parameters are described as follows : Type Parameter Description Context mContext\uff08required\uff09 context String key \uff08required\uff09 The field value of the data you want to get String defaultValue\uff08required\uff09 Default data returned if the query fails or there is no data in the database Modify Setting Data Modify setting data boolean insertSuccess = SettingValueFetcher.setStringValue(context,key,value) The parameters are described as follows: Type Parameter Discription\uff08Judge whether the operation is successful according to the boolean type of the return value\uff09 Context mContext(required\uff09 context String key \uff08required\uff09 The field value of the data you want to modify String value\uff08required\uff09 The data you want to modify Modifiable settings are as follows: key value Description \uff08R:read\uff0cW\uff1awrite\uff09 SerialText \" * *\" Unique identification of the robot\uff08R\uff09 cruiser_chassis_motion_state true/false Control the chassis on or off\uff08RW\uff09 cruiser_hand_motion_state true/false Control the arms on or off\uff08RW\uff09 visual_wakeup_state true/false Control visual wake-up on or off\uff08RW\uff09 key_sound_localization true/false Control sound source positioning on or off\uff08RW\uff09 language zh_CN/en_US Set the language of the robot\uff08W\uff09","title":"System settings"},{"location":"cruzr/setting.html#system-settings","text":"The Cruzr robot has some system setting parameters, which are often used by users, such as robot serial number, enterprise number, etc. Now, the methods will be provided.","title":"System settings"},{"location":"cruzr/setting.html#get-setting-data","text":"The following methods are called to get the setting state data of the robot . String value = SettingValueFetcher.getStringValue(context,key,defaultValue); The parameters are described as follows : Type Parameter Description Context mContext\uff08required\uff09 context String key \uff08required\uff09 The field value of the data you want to get String defaultValue\uff08required\uff09 Default data returned if the query fails or there is no data in the database","title":"Get Setting  Data"},{"location":"cruzr/setting.html#modify-setting-data","text":"Modify setting data boolean insertSuccess = SettingValueFetcher.setStringValue(context,key,value) The parameters are described as follows: Type Parameter Discription\uff08Judge whether the operation is successful according to the boolean type of the return value\uff09 Context mContext(required\uff09 context String key \uff08required\uff09 The field value of the data you want to modify String value\uff08required\uff09 The data you want to modify Modifiable settings are as follows: key value Description \uff08R:read\uff0cW\uff1awrite\uff09 SerialText \" * *\" Unique identification of the robot\uff08R\uff09 cruiser_chassis_motion_state true/false Control the chassis on or off\uff08RW\uff09 cruiser_hand_motion_state true/false Control the arms on or off\uff08RW\uff09 visual_wakeup_state true/false Control visual wake-up on or off\uff08RW\uff09 key_sound_localization true/false Control sound source positioning on or off\uff08RW\uff09 language zh_CN/en_US Set the language of the robot\uff08W\uff09","title":"Modify Setting Data"},{"location":"cruzr/user management.html","text":"User Management User management includes online user management and local user management. Online user data distinguishes different groups with enterprise number. Users in each group do not interfere with each other, and the groupID is the unique identification ID. Offline User Data is stored in the robot body and can be used when online users are not turned on and there is no network. It will actively obtain online or offline status when user management is started for the first time. Either online or offline must be selected, and the data is not interconnected. When you create an enterprise, online user management is enabled by default, and the groupID of the enterprise is generated. If you need to switch to offline user management, please contact the UBT technical support engineer. Online user management and offline user management query user information method\uff1a In order to query the information of online users, it is necessary to provide face pictures or face feature values for face recognition through visual services. After recognition, the information of online users can be returned. For details, please refer to visual services face recognition interface . To query the offline user information, you can call the following interfaces getusers and getuserbyfaceid to return the user information. When getuserbyfaceid is called to query single user information, faceID is required. The face recognition interface of visual services must be called before querying information to get faceID, then call the getuserbyfaceid method to query individual user information. Initialization Before using user management accesses the agent object,please initialize the usermanager object as follows: UserManager userManager = UserManager . getInstance ( this ); Get User Information Get all local users information : userManager . getUsers ( new UserListenrAbstract /*[1]*/ (){ @Override public void getUsers ( List < User > users /*[2]*/ ) { } }); [1] Userlistenrabstract is an abstract class that implements the usermanagerlistener interface. It implements callback listening by inheriting the userlistenrabstract class and overriding its method. The interface call method of UserManagerListenr is as follows\uff1a public interface UserManagerListenr { //Get all local users information void getUsers ( List < User > users ); //Get a local user information according to faceID void getUserByFaceId ( User user ); //Query user management status void getStatus ( UserStatus status ); } [2] Users are the information collection of all local users queried. The user entity attributes are as follows: public class User { private Long id ; //Userid private String uuid ; //User UUID private String faceId ; //user faceId\uff0cused to recognization private String name ; //User name private String title ; //Title private int gender ; //User gender 0:female 1\uff1amale 2\uff1asecrecy private List < Face > faces ; //Face informaton [A person can have multiple face photos, and the corresponding information of each photo ] } The properties of Face are as follows: public class Face { private Long id ; //id private String faceId ; //User faceId private String path ; //Picture path } Note: the image path is a file encrypted by image, which can only be used after decryption. Please refer to the following file decryption method. Query individual user information according to faceid: userManager . getUserByFaceId /*[1]*/ ( mFaceId , new UserListenrAbstract (){ @Override public void getUserByFaceId ( User user /*[2]*/ ) { super . getUserByFaceId ( user ); Log . d ( \"MainActivity\" , \"user:\" + user ); } }); [1]Parameter description of getUserByFaceId(String mFaceId,UserManagerListenr listenr) method : Parameter Type Explanation mFaceId String User faceId(Each user has only one faceid, which is generated by visual services \uff09 when adding user information. Therefore, when using this interface, you should first call the face recognition interface of visual services to get the faceid) listenr UserManagerListenr To callback listening\uff0cPlease refer to get all local user information description. [2] Quering a local user information by the condition, for user entity property , please refer to: get all local user information entity description. Get User Management Status It is used to query whether user management is online or offline. If it is online user management, the corresponding groupid under the enterprise number of the robot will be returned. The groupid is used to distinguish the personnel information under different enterprise numbers. People in the user library of the same groupid are not allowed to be duplicate.User libraries of different groupids do not affect each other . userManager . getStatus ( new UserListenrAbstract (){ @Override public void getStatus ( UserStatus status /*[1]*/ ) { } }); [1] Status refers to the status of user management (online / offline) and online groupid . The userstatus entity properties are as follows: public class UserStatus { private Boolean open ; //Whether online user management open true:online user management open false:online user management is not open private String groupId ; //return groupid when oline user management open,otherwise,return null } File Decryption InputStream inputStream /*[1]*/ = userManager . decryptInputStream ( String encryptFilePath /*[2]*/ ); [1] [inputStream] is the input stream after the encrypted file is decrypted. You can take the stream to generate file. If it is the encrypted image path, the stream can directly transfer to bitmap to load the image: InputStream decryptInputStream = userManager . decryptInputStream ( encryptFilePath ); Bitmap bitmap = BitmapFactory . decodeStream ( decryptInputStream ); ImageView imageView = findViewById ( R . id . img ); imageView . setImageBitmap ( bitmap ); [2] EncryptFilePath is the path of the encrypted file\uff0cFor example, the path property in the above Face entity User registration When a user registers a user through this interface, in order to maintain data consistency, the user data state flow is consistent with the state of user management. When user management is enabled for online user management, all users registered through this interface are online users. On the contrary, offline users are entered, and you can query the status through the above interface of Get User Management Status java userManager.registerUser(user /*[1]*/,UserListenrAbstract(){ @Override public void register(int code /*[2]*/, String msg/*[3]*/) { } }); [1] user Consistent with the above User content, the structure is as follows\uff1a java User user = new User.UserBuilder(\"Cruzr\" /*Name*/, \"/storage/emulated/0/Pictures/test2.png\" /*The path of the picture*/) //Name and picture path are required .gender(0) .title(\"title\") .build(); [2] code\uff1a java SUCCESS = 0; //The storage is successful or the user information is found FAILED_CODE = -1; //Visual service disconnected INSERT_FAILED_CODE = -2; //Database storage failed INSERT_VISUAL_FAILED_CODE = -3; //Visual service feature value entry failed INTERRUPT_FAILED_CODE = -4; //Interrupted by next request [3] msg: When code is not equal to 0, msg means error information, when code=0, msg means success or json data of user information Note: For more vision-related codes, please check the corresponding codes in the vision service","title":"User management"},{"location":"cruzr/user management.html#user-management","text":"User management includes online user management and local user management. Online user data distinguishes different groups with enterprise number. Users in each group do not interfere with each other, and the groupID is the unique identification ID. Offline User Data is stored in the robot body and can be used when online users are not turned on and there is no network. It will actively obtain online or offline status when user management is started for the first time. Either online or offline must be selected, and the data is not interconnected. When you create an enterprise, online user management is enabled by default, and the groupID of the enterprise is generated. If you need to switch to offline user management, please contact the UBT technical support engineer. Online user management and offline user management query user information method\uff1a In order to query the information of online users, it is necessary to provide face pictures or face feature values for face recognition through visual services. After recognition, the information of online users can be returned. For details, please refer to visual services face recognition interface . To query the offline user information, you can call the following interfaces getusers and getuserbyfaceid to return the user information. When getuserbyfaceid is called to query single user information, faceID is required. The face recognition interface of visual services must be called before querying information to get faceID, then call the getuserbyfaceid method to query individual user information.","title":"User Management"},{"location":"cruzr/user management.html#initialization","text":"Before using user management accesses the agent object,please initialize the usermanager object as follows: UserManager userManager = UserManager . getInstance ( this );","title":"Initialization"},{"location":"cruzr/user management.html#get-user-information","text":"Get all local users information : userManager . getUsers ( new UserListenrAbstract /*[1]*/ (){ @Override public void getUsers ( List < User > users /*[2]*/ ) { } }); [1] Userlistenrabstract is an abstract class that implements the usermanagerlistener interface. It implements callback listening by inheriting the userlistenrabstract class and overriding its method. The interface call method of UserManagerListenr is as follows\uff1a public interface UserManagerListenr { //Get all local users information void getUsers ( List < User > users ); //Get a local user information according to faceID void getUserByFaceId ( User user ); //Query user management status void getStatus ( UserStatus status ); } [2] Users are the information collection of all local users queried. The user entity attributes are as follows: public class User { private Long id ; //Userid private String uuid ; //User UUID private String faceId ; //user faceId\uff0cused to recognization private String name ; //User name private String title ; //Title private int gender ; //User gender 0:female 1\uff1amale 2\uff1asecrecy private List < Face > faces ; //Face informaton [A person can have multiple face photos, and the corresponding information of each photo ] } The properties of Face are as follows: public class Face { private Long id ; //id private String faceId ; //User faceId private String path ; //Picture path } Note: the image path is a file encrypted by image, which can only be used after decryption. Please refer to the following file decryption method. Query individual user information according to faceid: userManager . getUserByFaceId /*[1]*/ ( mFaceId , new UserListenrAbstract (){ @Override public void getUserByFaceId ( User user /*[2]*/ ) { super . getUserByFaceId ( user ); Log . d ( \"MainActivity\" , \"user:\" + user ); } }); [1]Parameter description of getUserByFaceId(String mFaceId,UserManagerListenr listenr) method : Parameter Type Explanation mFaceId String User faceId(Each user has only one faceid, which is generated by visual services \uff09 when adding user information. Therefore, when using this interface, you should first call the face recognition interface of visual services to get the faceid) listenr UserManagerListenr To callback listening\uff0cPlease refer to get all local user information description. [2] Quering a local user information by the condition, for user entity property , please refer to: get all local user information entity description.","title":"Get User Information"},{"location":"cruzr/user management.html#get-user-management-status","text":"It is used to query whether user management is online or offline. If it is online user management, the corresponding groupid under the enterprise number of the robot will be returned. The groupid is used to distinguish the personnel information under different enterprise numbers. People in the user library of the same groupid are not allowed to be duplicate.User libraries of different groupids do not affect each other . userManager . getStatus ( new UserListenrAbstract (){ @Override public void getStatus ( UserStatus status /*[1]*/ ) { } }); [1] Status refers to the status of user management (online / offline) and online groupid . The userstatus entity properties are as follows: public class UserStatus { private Boolean open ; //Whether online user management open true:online user management open false:online user management is not open private String groupId ; //return groupid when oline user management open,otherwise,return null }","title":"Get User Management Status"},{"location":"cruzr/user management.html#file-decryption","text":"InputStream inputStream /*[1]*/ = userManager . decryptInputStream ( String encryptFilePath /*[2]*/ ); [1] [inputStream] is the input stream after the encrypted file is decrypted. You can take the stream to generate file. If it is the encrypted image path, the stream can directly transfer to bitmap to load the image: InputStream decryptInputStream = userManager . decryptInputStream ( encryptFilePath ); Bitmap bitmap = BitmapFactory . decodeStream ( decryptInputStream ); ImageView imageView = findViewById ( R . id . img ); imageView . setImageBitmap ( bitmap ); [2] EncryptFilePath is the path of the encrypted file\uff0cFor example, the path property in the above Face entity","title":"File Decryption"},{"location":"cruzr/user management.html#user-registration","text":"","title":"User registration"},{"location":"cruzr/user management.html#when-a-user-registers-a-user-through-this-interface-in-order-to-maintain-data-consistency-the-user-data-state-flow-is-consistent-with-the-state-of-user-management-when-user-management-is-enabled-for-online-user-management-all-users-registered-through-this-interface-are-online-users-on-the-contrary-offline-users-are-entered-and-you-can-query-the-status-through-the-above-interface-of-get-user-management-status","text":"java userManager.registerUser(user /*[1]*/,UserListenrAbstract(){ @Override public void register(int code /*[2]*/, String msg/*[3]*/) { } }); [1] user Consistent with the above User content, the structure is as follows\uff1a java User user = new User.UserBuilder(\"Cruzr\" /*Name*/, \"/storage/emulated/0/Pictures/test2.png\" /*The path of the picture*/) //Name and picture path are required .gender(0) .title(\"title\") .build(); [2] code\uff1a java SUCCESS = 0; //The storage is successful or the user information is found FAILED_CODE = -1; //Visual service disconnected INSERT_FAILED_CODE = -2; //Database storage failed INSERT_VISUAL_FAILED_CODE = -3; //Visual service feature value entry failed INTERRUPT_FAILED_CODE = -4; //Interrupted by next request [3] msg: When code is not equal to 0, msg means error information, when code=0, msg means success or json data of user information Note: For more vision-related codes, please check the corresponding codes in the vision service","title":"When a user registers a user through this interface, in order to maintain data consistency, the user data state flow is consistent with the state of user management. When user management is enabled for online user management, all users registered through this interface are online users. On the contrary, offline users are entered, and you can query the status through the above interface of Get User Management Status"},{"location":"cruzr/visual.html","text":"Visual Service Visual service provides face detection, face recognition, face tracking and other services. Face detection \uff0cDetect whether there is a face in the camera image or target image, and if so, return the position, size and angle of the face. Face recognition \uff0cBy extracting human facial features, the similarity between two faces is calculated to determine whether they are the same person Face tracking \uff0cFace key point location and tracking technology can accurately locate and track the change of face position. Initialization As a visual service access agent, the VisualManager object provides the main API of visual service, which can be obtained as follows: VisualManager mVisualManager = VisualManager . getInstance (); mVisualManager . init ( getApplicationContext ()); // Initialization required Start a visual task mVisualManager . start ( VisualParam param ); // param is a visual parameter The construction method of visual parameters VisualParam param = new VisualParam . Builder () . requestId ( requestId ) // Set request ID . requestType ( requestType ) // Set request type . online ( online ) // Open online face recognition or not . showUi ( showUi ) // Display interface or not . timeOut ( timeOut ) // Set timeout . imageFile ( imageFile ) // The absolute path of the image passed in during image recognition . imagePath ( imagePath ) // Set the directory for saving photos when taking pictures of faces . groupId ( groupId ) // GroupId of online face recognition . build (); VisualParam parameter description type parameter parameter description required String requestId It is the unique ID of each request. This ID is returned in the information callback\uff0coptional parameter. UUID is generated by default no int requestType int TYPE_DEFAULT = 0; //face registration int TYPE_RECOGNITION = 1; //face recognition int TYPE_DETECTION = 2; //face tracking int TYPE_IMAGE= 3; //Picture recognition, screenshot interface int TYPE_IMAGE_BG= 4; //Picture recognition, no screenshot interface int TYPE_TAKE_PHOTO= 5; //Face photo \uff08In the face registration process, when the face is matched, the user information will be returned online, and the saved face ID will be returned offline; when the face is not matched, the newly generated face ID will be returned\uff09 yes boolean online Whether to use online face recognition? True is online face recognition and false is offline face recognition.\uff08groupid needs to be configured when online\uff09 yes boolean showUi Whether to display the interface? True is the display scanning interface, false is the non sensitive face recognition (Requesttype has no interface by default during face detecting. This parameter is ignored) yes long timeOut Timeout\uff0cIn milliseconds,\uff08No timeout when < = 0\uff09 yes String groupId When creating enterprise number, online user management mode is selected by default, and groupid is automatically generated. This item needs to be configured for online face recognition. If the enterprise number is changed, the groupid needs to be updated. (log in CBIS website and click user management to view the groupid or view getstatus method in [user management document][user management documen]. Required if online String imageFile The absolute path of the picture (the request type is picture recognition) Required when the request type is picture recognition String imagePath The saving path of the picture of face photo (request type is face photo) Required when the request type is face photo Note: When the requestType is TYPE_DETECTION = 2 (face tracking), local face detection is adopted, and there is no scan interface, the parameters online, showUi, groupId can be ignored. Face detection\uff0cface tracking\uff1a private void faceDetection () { VisualParam param = new VisualParam . Builder () . requestType ( VisualParam . TYPE_DETECTION ) . build (); mVisualManager . start ( param ); } Offline picture recognition\uff1a private void imageRecognizeLocal ( String imageFile ) { VisualParam param = new VisualParam . Builder () . requestType ( VisualParam . TYPE_IMAGE_BG ) . imageFile ( imageFile ) // Incoming photo path . online ( false ) // Offline . build (); mVisualManager . start ( param ); } Online picture recognition\uff1a private void imageRecognizeOnline ( String imageFile , String groupId ) { VisualParam param = new VisualParam . Builder () . requestType ( VisualParam . TYPE_IMAGE_BG ) . imageFile ( imageFile ) // Incoming image path . online ( true ) // online . groupId ( groupId ) // groupId . build (); mVisualManager . start ( param ); } Face photo\uff1a private void takeFacePhoto ( boolean showUi , String imagePath ) { VisualParam param = new VisualParam . Builder () . requestType ( VisualParam . TYPE_TAKE_PHOTO ) . showUi ( showUi ) // Whether to display the interface . imagePath ( imagePath ) // Set the directory for saving photos . build (); mVisualManager . start ( param ); } Stop visual task mVisualManager . stop (); //Stop last task mVisualManager . stop ( String requestId ); //Stop the specified task, requestId is the unique ID at the time of request Listener To obtain the information after the task is finished, you need to register a listening callback. Add a listener: mVisualManager . addListener ( VisualListener listener ); Remove a listener\uff1a mVisualManager . removeListener ( VisualListener listener ); Note\uff1aAdd and remove need to be paired . Listener\uff1a public class MyListener extends VisualListener { @Override public void onDone ( String message ) { // Analyze JSON according to business requirements Log . d ( TAG , \"onDone : \" + message ); try { Result result = new Gson (). fromJson ( message , Result . class ); Log . i ( TAG , \"result : \" + result ); } catch ( Exception e ) { Log . e ( TAG , \"onDone Exception\" , e ); } } @Override public void onFail ( int code , String error ) { // Code is the error code and error is the error message Log . d ( TAG , \"onFail code : \" + code + \", error : \" + error ); } @Override public void onProgress ( String message ) { Log . d ( TAG , \"onProgress : \" + message ); } } Result definition returned in onDone \uff1a type parameter parameter desciption String requestId The return value is the same as the requestId in the visual parameter VisualParam of the start task int code Face recognition return message code int DETECT_FAIL = 224; Face detection failed int RECOGNIZE_FAIL= 225; Face recognition failed int RECOGNIZE_INTERRUPT = 226; Face recognition interrupt int RECOGNIZE_TIME_OUT= 227; Face recognition timeout int CAMERA_OPEN_FAIL= 228; Camera open failed int CANCEL= 229; Cancel task actively int IMAGE_EXCEPTION= 230; Image exception int RECOGNIZE_FAIL_BROKEN= 8208; Face recognition blocking int PHOTO= 160; Take photo successfully int MATCH= 161; Match to face database, face recognition successful int MISMATCH= 162; Face recognition is not matched to face database int EXCEPTION= 163; Abnormal face recognition String message Message returned by face recognition \"fail\" Face recognition is not matched to face database \"success\" Successful face recognition, matching to face database \"exception\" Abnormal face recognition \"interrupt\" Face recognition interrupt \"time out;\" Face recognition timeout \"cancel\" Cancel task actively \"camera open fail\" Camera open failed \"image exception\" String faceId The face ID returned by offline face recognition. If a matching face is found, the saved ID will be returned. Otherwise, the UUID will be generated again. This value can be ignored in online face recognition. String imageFile Return to the image path location in face recognition PersonDetail personDetail Online face recognition: matching to face returns user details, including name, gender, title, remarks, grouping, face image address, etc., but not matching to face returns NULL; offline face recognition: this value returns null When the request parameter is face tracking (requesttype = 2), callback in onProgress function. type parameter parameter description float[] angles face angle\uff0c[0]pitch angle\u3001[1] left or right roll angle \u3001[2] inclination angle Rect rect The position of the face rectangle in the camera (Note: the resolution of the camera without scan frame is 800 * 600, and the resolution of the camera with scan frame is 1280 * 720) Free memory mVisualManager . onDestroy ();","title":"Visual service"},{"location":"cruzr/visual.html#visual-service","text":"Visual service provides face detection, face recognition, face tracking and other services. Face detection \uff0cDetect whether there is a face in the camera image or target image, and if so, return the position, size and angle of the face. Face recognition \uff0cBy extracting human facial features, the similarity between two faces is calculated to determine whether they are the same person Face tracking \uff0cFace key point location and tracking technology can accurately locate and track the change of face position.","title":"Visual Service"},{"location":"cruzr/visual.html#initialization","text":"As a visual service access agent, the VisualManager object provides the main API of visual service, which can be obtained as follows: VisualManager mVisualManager = VisualManager . getInstance (); mVisualManager . init ( getApplicationContext ()); // Initialization required","title":"Initialization"},{"location":"cruzr/visual.html#start-a-visual-task","text":"mVisualManager . start ( VisualParam param ); // param is a visual parameter The construction method of visual parameters VisualParam param = new VisualParam . Builder () . requestId ( requestId ) // Set request ID . requestType ( requestType ) // Set request type . online ( online ) // Open online face recognition or not . showUi ( showUi ) // Display interface or not . timeOut ( timeOut ) // Set timeout . imageFile ( imageFile ) // The absolute path of the image passed in during image recognition . imagePath ( imagePath ) // Set the directory for saving photos when taking pictures of faces . groupId ( groupId ) // GroupId of online face recognition . build (); VisualParam parameter description type parameter parameter description required String requestId It is the unique ID of each request. This ID is returned in the information callback\uff0coptional parameter. UUID is generated by default no int requestType int TYPE_DEFAULT = 0; //face registration int TYPE_RECOGNITION = 1; //face recognition int TYPE_DETECTION = 2; //face tracking int TYPE_IMAGE= 3; //Picture recognition, screenshot interface int TYPE_IMAGE_BG= 4; //Picture recognition, no screenshot interface int TYPE_TAKE_PHOTO= 5; //Face photo \uff08In the face registration process, when the face is matched, the user information will be returned online, and the saved face ID will be returned offline; when the face is not matched, the newly generated face ID will be returned\uff09 yes boolean online Whether to use online face recognition? True is online face recognition and false is offline face recognition.\uff08groupid needs to be configured when online\uff09 yes boolean showUi Whether to display the interface? True is the display scanning interface, false is the non sensitive face recognition (Requesttype has no interface by default during face detecting. This parameter is ignored) yes long timeOut Timeout\uff0cIn milliseconds,\uff08No timeout when < = 0\uff09 yes String groupId When creating enterprise number, online user management mode is selected by default, and groupid is automatically generated. This item needs to be configured for online face recognition. If the enterprise number is changed, the groupid needs to be updated. (log in CBIS website and click user management to view the groupid or view getstatus method in [user management document][user management documen]. Required if online String imageFile The absolute path of the picture (the request type is picture recognition) Required when the request type is picture recognition String imagePath The saving path of the picture of face photo (request type is face photo) Required when the request type is face photo Note: When the requestType is TYPE_DETECTION = 2 (face tracking), local face detection is adopted, and there is no scan interface, the parameters online, showUi, groupId can be ignored. Face detection\uff0cface tracking\uff1a private void faceDetection () { VisualParam param = new VisualParam . Builder () . requestType ( VisualParam . TYPE_DETECTION ) . build (); mVisualManager . start ( param ); } Offline picture recognition\uff1a private void imageRecognizeLocal ( String imageFile ) { VisualParam param = new VisualParam . Builder () . requestType ( VisualParam . TYPE_IMAGE_BG ) . imageFile ( imageFile ) // Incoming photo path . online ( false ) // Offline . build (); mVisualManager . start ( param ); } Online picture recognition\uff1a private void imageRecognizeOnline ( String imageFile , String groupId ) { VisualParam param = new VisualParam . Builder () . requestType ( VisualParam . TYPE_IMAGE_BG ) . imageFile ( imageFile ) // Incoming image path . online ( true ) // online . groupId ( groupId ) // groupId . build (); mVisualManager . start ( param ); } Face photo\uff1a private void takeFacePhoto ( boolean showUi , String imagePath ) { VisualParam param = new VisualParam . Builder () . requestType ( VisualParam . TYPE_TAKE_PHOTO ) . showUi ( showUi ) // Whether to display the interface . imagePath ( imagePath ) // Set the directory for saving photos . build (); mVisualManager . start ( param ); }","title":"Start a visual task"},{"location":"cruzr/visual.html#stop-visual-task","text":"mVisualManager . stop (); //Stop last task mVisualManager . stop ( String requestId ); //Stop the specified task, requestId is the unique ID at the time of request","title":"Stop visual task"},{"location":"cruzr/visual.html#listener","text":"To obtain the information after the task is finished, you need to register a listening callback. Add a listener: mVisualManager . addListener ( VisualListener listener ); Remove a listener\uff1a mVisualManager . removeListener ( VisualListener listener ); Note\uff1aAdd and remove need to be paired . Listener\uff1a public class MyListener extends VisualListener { @Override public void onDone ( String message ) { // Analyze JSON according to business requirements Log . d ( TAG , \"onDone : \" + message ); try { Result result = new Gson (). fromJson ( message , Result . class ); Log . i ( TAG , \"result : \" + result ); } catch ( Exception e ) { Log . e ( TAG , \"onDone Exception\" , e ); } } @Override public void onFail ( int code , String error ) { // Code is the error code and error is the error message Log . d ( TAG , \"onFail code : \" + code + \", error : \" + error ); } @Override public void onProgress ( String message ) { Log . d ( TAG , \"onProgress : \" + message ); } } Result definition returned in onDone \uff1a type parameter parameter desciption String requestId The return value is the same as the requestId in the visual parameter VisualParam of the start task int code Face recognition return message code int DETECT_FAIL = 224; Face detection failed int RECOGNIZE_FAIL= 225; Face recognition failed int RECOGNIZE_INTERRUPT = 226; Face recognition interrupt int RECOGNIZE_TIME_OUT= 227; Face recognition timeout int CAMERA_OPEN_FAIL= 228; Camera open failed int CANCEL= 229; Cancel task actively int IMAGE_EXCEPTION= 230; Image exception int RECOGNIZE_FAIL_BROKEN= 8208; Face recognition blocking int PHOTO= 160; Take photo successfully int MATCH= 161; Match to face database, face recognition successful int MISMATCH= 162; Face recognition is not matched to face database int EXCEPTION= 163; Abnormal face recognition String message Message returned by face recognition \"fail\" Face recognition is not matched to face database \"success\" Successful face recognition, matching to face database \"exception\" Abnormal face recognition \"interrupt\" Face recognition interrupt \"time out;\" Face recognition timeout \"cancel\" Cancel task actively \"camera open fail\" Camera open failed \"image exception\" String faceId The face ID returned by offline face recognition. If a matching face is found, the saved ID will be returned. Otherwise, the UUID will be generated again. This value can be ignored in online face recognition. String imageFile Return to the image path location in face recognition PersonDetail personDetail Online face recognition: matching to face returns user details, including name, gender, title, remarks, grouping, face image address, etc., but not matching to face returns NULL; offline face recognition: this value returns null When the request parameter is face tracking (requesttype = 2), callback in onProgress function. type parameter parameter description float[] angles face angle\uff0c[0]pitch angle\u3001[1] left or right roll angle \u3001[2] inclination angle Rect rect The position of the face rectangle in the camera (Note: the resolution of the camera without scan frame is 800 * 600, and the resolution of the camera with scan frame is 1280 * 720)","title":"Listener"},{"location":"cruzr/visual.html#free-memory","text":"mVisualManager . onDestroy ();","title":"Free memory"},{"location":"cruzr/voice-assistant.html","text":"Voice Assistant Voice assistant is a built-in application of robot, which displays the text content of dialogue with robot. The voice assistant includes: wake-up button, conversation flow and guidance components. Developers often want to be able to control the display and hiding of the whole or part of the components of the voice assistant, as well as the query of displaying an application. Here is the method. Control the display and hiding of voice assistant can be Implemented as follows Display voice assistant: AssistantManager.get(getContext()).showAssistant(); Hide voice assistant\uff1a AssistantManager.get(getContext()).hideAssistant(); Control the display and hiding methods of the guidance of an application When developers want to enter a third-party application, and only display the relevant guidance of the application, which can be implemented through this interface . Show specific application guidance \uff1a AssistantManager.get(getContext()).showSpecificPrompt(String packageName); Parameter Explanation packageName application package name Hide specific application guidance \uff1a AssistantManager.get(getContext()).hideSpecificPrompt(String packageName); Parameter Explanation packageName application package name Control voice assistant components to be displayed and hidden separately can be implemented as follows AssistantManager.get(getContext()).showOrHidePart(int type); Parameter Explanation type Control type TYPE_SHOW_PART_WAKEUP: display wake-up button TYPE_HIDE_PART_WAKEUP: hide wake-up button TYPE_SHOW_PART_MESSAGE: display dialog flow components TYPE_HIDE_PART_MESSAGE: hide dialog flow components Controlling the global display and hiding of the voice assistant can be achieved in the following ways AssistantManager.get(getContext()).switchAssistant(boolean bOnOff); Parameter Explanation bOnOff Control type TYPE_TURN_ON: Show voice assistant TYPE_TURN_OFF: Hide voice assistant","title":"Voice-assistant"},{"location":"cruzr/voice-assistant.html#voice-assistant","text":"Voice assistant is a built-in application of robot, which displays the text content of dialogue with robot. The voice assistant includes: wake-up button, conversation flow and guidance components. Developers often want to be able to control the display and hiding of the whole or part of the components of the voice assistant, as well as the query of displaying an application. Here is the method. Control the display and hiding of voice assistant can be Implemented as follows Display voice assistant: AssistantManager.get(getContext()).showAssistant(); Hide voice assistant\uff1a AssistantManager.get(getContext()).hideAssistant(); Control the display and hiding methods of the guidance of an application When developers want to enter a third-party application, and only display the relevant guidance of the application, which can be implemented through this interface . Show specific application guidance \uff1a AssistantManager.get(getContext()).showSpecificPrompt(String packageName); Parameter Explanation packageName application package name Hide specific application guidance \uff1a AssistantManager.get(getContext()).hideSpecificPrompt(String packageName); Parameter Explanation packageName application package name Control voice assistant components to be displayed and hidden separately can be implemented as follows AssistantManager.get(getContext()).showOrHidePart(int type); Parameter Explanation type Control type TYPE_SHOW_PART_WAKEUP: display wake-up button TYPE_HIDE_PART_WAKEUP: hide wake-up button TYPE_SHOW_PART_MESSAGE: display dialog flow components TYPE_HIDE_PART_MESSAGE: hide dialog flow components Controlling the global display and hiding of the voice assistant can be achieved in the following ways AssistantManager.get(getContext()).switchAssistant(boolean bOnOff); Parameter Explanation bOnOff Control type TYPE_TURN_ON: Show voice assistant TYPE_TURN_OFF: Hide voice assistant","title":"Voice Assistant"}]}